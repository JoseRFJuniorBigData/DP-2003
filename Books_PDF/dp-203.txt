Particionamento de dados horizontal, vertical e funcional


Em muitas soluções de grande escala, os dados são divididos em partições que podem ser gerenciadas e acessadas separadamente. O particionamento pode melhorar a escalabilidade, reduzir a contenção e otimizar o desempenho. Ele também pode fornecer um mecanismo para dividir dados por padrão de uso. Por exemplo, você pode arquivar dados mais antigos em um armazenamento de dados mais barato.


No entanto, a estratégia de particionamento deve ser escolhida com cuidado para maximizar os benefícios e minimizar os efeitos adversos.


 Observação


Neste artigo, o termo particionamento significa o processo de dividir fisicamente os dados em armazenamentos de dados separados. Não é o mesmo que particionamento de tabela do SQL Server.


Por que particionar dados?
Melhore a escalabilidade . Quando você escala um único sistema de banco de dados, ele eventualmente atingirá um limite de hardware físico. Se você dividir os dados em várias partições, cada uma hospedada em um servidor separado, poderá expandir o sistema quase indefinidamente.


Melhore o desempenho . As operações de acesso a dados em cada partição ocorrem em um volume menor de dados. Feito corretamente, o particionamento pode tornar seu sistema mais eficiente. As operações que afetam mais de uma partição podem ser executadas em paralelo.


Melhore a segurança . Em alguns casos, você pode separar dados confidenciais e não confidenciais em partições diferentes e aplicar controles de segurança diferentes aos dados confidenciais.


Fornecer flexibilidade operacional . O particionamento oferece muitas oportunidades para operações de ajuste fino, maximizando a eficiência administrativa e minimizando custos. Por exemplo, você pode definir diferentes estratégias para gerenciamento, monitoramento, backup e restauração e outras tarefas administrativas com base na importância dos dados em cada partição.


Combine o armazenamento de dados com o padrão de uso . O particionamento permite que cada partição seja implantada em um tipo diferente de armazenamento de dados, com base no custo e nos recursos internos que o armazenamento de dados oferece. Por exemplo, grandes dados binários podem ser armazenados no armazenamento de blobs, enquanto dados mais estruturados podem ser mantidos em um banco de dados de documentos. Consulte Escolha o armazenamento de dados correto .


Melhorar a disponibilidade . A separação de dados em vários servidores evita um único ponto de falha. Se uma instância falhar, apenas os dados dessa partição ficarão indisponíveis. As operações em outras partições podem continuar. Para armazenamentos de dados PaaS gerenciados, essa consideração é menos relevante, porque esses serviços são projetados com redundância integrada.


Projetando partições
Existem três estratégias típicas para particionar dados:


Particionamento horizontal (geralmente chamado de fragmentação ). Nesta estratégia, cada partição é um armazenamento de dados separado, mas todas as partições têm o mesmo esquema. Cada partição é conhecida como shard e contém um subconjunto específico de dados, como todos os pedidos de um conjunto específico de clientes.


Particionamento vertical . Nesta estratégia, cada partição contém um subconjunto dos campos para itens no armazenamento de dados. Os campos são divididos de acordo com seu padrão de uso. Por exemplo, os campos acessados com frequência podem ser colocados em uma partição vertical e os campos acessados com menos frequência em outra.


Particionamento funcional . Nesta estratégia, os dados são agregados de acordo com a forma como são utilizados por cada contexto limitado no sistema. Por exemplo, um sistema de comércio eletrônico pode armazenar dados de fatura em uma partição e dados de inventário de produtos em outra.


Essas estratégias podem ser combinadas e recomendamos que você as considere todas ao projetar um esquema de particionamento. Por exemplo, você pode dividir os dados em estilhaços e usar o particionamento vertical para subdividir ainda mais os dados em cada estilhaço.


Particionamento horizontal (fragmentação)
A Figura 1 mostra o particionamento ou fragmentação horizontal. Neste exemplo, os dados de inventário do produto são divididos em fragmentos com base na chave do produto. Cada fragmento contém os dados de um intervalo contíguo de chaves de fragmento (AG e HZ), organizados em ordem alfabética. A fragmentação distribui a carga por mais computadores, o que reduz a contenção e melhora o desempenho.






Figura 1 - Particionamento horizontal de dados (fragmentação) com base em uma chave de partição.


O fator mais importante é a escolha de uma chave de fragmentação. Pode ser difícil mudar a chave depois que o sistema estiver em operação. A chave deve garantir que os dados sejam particionados para distribuir a carga de trabalho da maneira mais uniforme possível entre os estilhaços.


Os fragmentos não precisam ser do mesmo tamanho. É mais importante equilibrar o número de solicitações. Alguns estilhaços podem ser muito grandes, mas cada item tem um número baixo de operações de acesso. Outros fragmentos podem ser menores, mas cada item é acessado com muito mais frequência. Também é importante garantir que um único estilhaço não exceda os limites de escala (em termos de capacidade e recursos de processamento) do armazenamento de dados.


Evite criar partições "quentes" que possam afetar o desempenho e a disponibilidade. Por exemplo, usar a primeira letra do nome de um cliente causa uma distribuição desequilibrada, porque algumas letras são mais comuns. Em vez disso, use um hash de um identificador de cliente para distribuir os dados de maneira mais uniforme entre as partições.


Escolha uma chave de fragmentação que minimize quaisquer requisitos futuros para dividir fragmentos grandes, unir fragmentos pequenos em partições maiores ou alterar o esquema. Essas operações podem consumir muito tempo e podem exigir que um ou mais estilhaços fiquem offline enquanto são executados.


Se os estilhaços forem replicados, talvez seja possível manter algumas das réplicas online enquanto outras são divididas, mescladas ou reconfiguradas. No entanto, o sistema pode precisar limitar as operações que podem ser executadas durante a reconfiguração. Por exemplo, os dados nas réplicas podem ser marcados como somente leitura para evitar inconsistências de dados.


Para obter mais informações sobre particionamento horizontal, consulte padrão de fragmentação .


Particionamento vertical
O uso mais comum do particionamento vertical é reduzir os custos de E/S e desempenho associados à busca de itens acessados com frequência. A Figura 2 mostra um exemplo de particionamento vertical. Neste exemplo, diferentes propriedades de um item são armazenadas em diferentes partições. Uma partição contém dados que são acessados com mais frequência, incluindo nome do produto, descrição e preço. Outra partição contém dados de estoque: a contagem de estoque e a data do último pedido.






Figura 2 - Particionamento vertical de dados por padrão de uso.


Neste exemplo, o aplicativo consulta regularmente o nome, a descrição e o preço do produto ao exibir os detalhes do produto aos clientes. A contagem de estoque e a data do último pedido são mantidas em uma partição separada porque esses dois itens são comumente usados juntos.


Outras vantagens do particionamento vertical:


Dados relativamente lentos (nome do produto, descrição e preço) podem ser separados dos dados mais dinâmicos (nível de estoque e data do último pedido). Dados em movimento lento são um bom candidato para um aplicativo armazenar em cache na memória.


Os dados confidenciais podem ser armazenados em uma partição separada com controles de segurança adicionais.


O particionamento vertical pode reduzir a quantidade de acesso simultâneo necessário.


O particionamento vertical opera no nível de entidade dentro de um armazenamento de dados, normalizando parcialmente uma entidade para dividi-la de um item amplo para um conjunto de itens estreitos . É ideal para armazenamentos de dados orientados a colunas, como HBase e Cassandra. Se for improvável que os dados em uma coleção de colunas sejam alterados, você também pode considerar o uso de repositórios de colunas no SQL Server.


Particionamento funcional
Quando é possível identificar um contexto limitado para cada área de negócios distinta em um aplicativo, o particionamento funcional é uma forma de melhorar o isolamento e o desempenho do acesso aos dados. Outro uso comum para particionamento funcional é separar dados de leitura/gravação de dados somente leitura. A Figura 3 mostra uma visão geral do particionamento funcional em que os dados de inventário são separados dos dados do cliente.






Figura 3 - Dados de particionamento funcional por contexto ou subdomínio limitado.


Essa estratégia de particionamento pode ajudar a reduzir a contenção de acesso a dados em diferentes partes de um sistema.


Projetando partições para escalabilidade
É vital considerar o tamanho e a carga de trabalho de cada partição e equilibrá-los para que os dados sejam distribuídos para alcançar a escalabilidade máxima. No entanto, você também deve particionar os dados para que eles não excedam os limites de dimensionamento de um único armazenamento de partição.


Siga estas etapas ao projetar partições para escalabilidade:


Analise o aplicativo para entender os padrões de acesso a dados, como o tamanho do conjunto de resultados retornado por cada consulta, a frequência de acesso, a latência inerente e os requisitos de processamento de computação do lado do servidor. Em muitos casos, algumas entidades importantes demandarão a maior parte dos recursos de processamento.
Use essa análise para determinar as metas de escalabilidade atuais e futuras, como tamanho dos dados e carga de trabalho. Em seguida, distribua os dados pelas partições para atender à meta de escalabilidade. Para particionamento horizontal, é importante escolher a chave de fragmentação correta para garantir que a distribuição seja uniforme. Para obter mais informações, consulte o padrão de fragmentação .
Certifique-se de que cada partição tenha recursos suficientes para lidar com os requisitos de escalabilidade, em termos de tamanho de dados e taxa de transferência. Dependendo do armazenamento de dados, pode haver um limite na quantidade de espaço de armazenamento, poder de processamento ou largura de banda de rede por partição. Se for provável que os requisitos excedam esses limites, talvez seja necessário refinar sua estratégia de particionamento ou dividir ainda mais os dados, possivelmente combinando duas ou mais estratégias.
Monitore o sistema para verificar se os dados são distribuídos conforme o esperado e se as partições podem lidar com a carga. O uso real nem sempre corresponde ao que uma análise prevê. Nesse caso, pode ser possível reequilibrar as partições ou redesenhar algumas partes do sistema para obter o equilíbrio necessário.
Alguns ambientes de nuvem alocam recursos em termos de limites de infraestrutura. Certifique-se de que os limites do limite selecionado forneçam espaço suficiente para qualquer crescimento previsto no volume de dados, em termos de armazenamento de dados, poder de processamento e largura de banda.


Por exemplo, se você usar o armazenamento de tabelas do Azure, há um limite para o volume de solicitações que podem ser tratadas por uma única partição em um determinado período de tempo. (Para obter mais informações, consulte escalabilidade de armazenamento do Azure e metas de desempenho .) Um estilhaço ocupado pode exigir mais recursos do que uma única partição pode manipular. Nesse caso, o estilhaço pode precisar ser reparticionado para distribuir a carga. Se o tamanho total ou a taxa de transferência dessas tabelas exceder a capacidade de uma conta de armazenamento, talvez seja necessário criar contas de armazenamento adicionais e distribuir as tabelas entre essas contas.


Projetando partições para desempenho de consulta
O desempenho da consulta geralmente pode ser aprimorado usando conjuntos de dados menores e executando consultas paralelas. Cada partição deve conter uma pequena proporção de todo o conjunto de dados. Essa redução no volume pode melhorar o desempenho das consultas. No entanto, o particionamento não é uma alternativa para projetar e configurar um banco de dados adequadamente. Por exemplo, verifique se você possui os índices necessários.


Siga estas etapas ao projetar partições para desempenho de consulta:


Examine os requisitos e o desempenho do aplicativo:


Use os requisitos de negócios para determinar as consultas críticas que devem sempre ser executadas rapidamente.
Monitore o sistema para identificar quaisquer consultas que sejam executadas lentamente.
Descubra quais consultas são realizadas com mais frequência. Mesmo que uma única consulta tenha um custo mínimo, o consumo cumulativo de recursos pode ser significativo.
Particione os dados que estão causando lentidão no desempenho:


Limite o tamanho de cada partição para que o tempo de resposta da consulta esteja dentro do destino.
Se você usar o particionamento horizontal, projete a chave de fragmentação para que o aplicativo possa selecionar facilmente a partição correta. Isso evita que a consulta tenha que varrer todas as partições.
Considere a localização de uma partição. Se possível, tente manter os dados em partições geograficamente próximas aos aplicativos e usuários que os acessam.
Se uma entidade tiver requisitos de rendimento e desempenho de consulta, use o particionamento funcional com base nessa entidade. Se isso ainda não atender aos requisitos, aplique também o particionamento horizontal. Na maioria dos casos, uma única estratégia de particionamento será suficiente, mas em alguns casos é mais eficiente combinar as duas estratégias.


Considere executar consultas em paralelo nas partições para melhorar o desempenho.


Projetando partições para disponibilidade
O particionamento de dados pode melhorar a disponibilidade de aplicativos, garantindo que todo o conjunto de dados não constitua um único ponto de falha e que subconjuntos individuais do conjunto de dados possam ser gerenciados de forma independente.


Considere os seguintes fatores que afetam a disponibilidade:


A importância dos dados para as operações de negócios . Identifique quais dados são informações comerciais críticas, como transações, e quais dados são dados operacionais menos críticos, como arquivos de log.


Considere armazenar dados críticos em partições altamente disponíveis com um plano de backup apropriado.


Estabeleça procedimentos separados de gerenciamento e monitoramento para os diferentes conjuntos de dados.


Coloque dados que tenham o mesmo nível de criticidade na mesma partição para que possa ser feito backup em uma frequência apropriada. Por exemplo, as partições que retêm dados de transação podem precisar de backup com mais frequência do que as partições que retêm informações de log ou rastreio.


Como as partições individuais podem ser gerenciadas . Projetar partições para suportar gerenciamento e manutenção independentes oferece várias vantagens. Por exemplo:


Se uma partição falhar, ela poderá ser recuperada independentemente, sem aplicativos que acessem dados em outras partições.


O particionamento de dados por área geográfica permite que as tarefas de manutenção programadas ocorram fora do horário de pico para cada local. Certifique-se de que as partições não sejam muito grandes para evitar que qualquer manutenção planejada seja concluída durante esse período.


Se deve replicar dados críticos entre partições . Essa estratégia pode melhorar a disponibilidade e o desempenho, mas também pode apresentar problemas de consistência. Leva tempo para sincronizar as alterações com cada réplica. Durante esse período, partições diferentes conterão valores de dados diferentes.


Considerações de design do aplicativo
O particionamento adiciona complexidade ao projeto e desenvolvimento do seu sistema. Considere o particionamento como uma parte fundamental do projeto do sistema, mesmo que inicialmente o sistema contenha apenas uma única partição. Se você abordar o particionamento como uma reflexão tardia, será mais desafiador porque você já tem um sistema ativo para manter:


A lógica de acesso a dados precisará ser modificada.
Pode ser necessário migrar grandes quantidades de dados existentes para distribuí-los entre partições.
Os usuários esperam poder continuar usando o sistema durante a migração.
Em alguns casos, o particionamento não é considerado importante porque o conjunto de dados inicial é pequeno e pode ser facilmente manipulado por um único servidor. Isso pode ser verdade para algumas cargas de trabalho, mas muitos sistemas comerciais precisam se expandir à medida que o número de usuários aumenta.


Além disso, não são apenas os grandes armazenamentos de dados que se beneficiam do particionamento. Por exemplo, um pequeno armazenamento de dados pode ser muito acessado por centenas de clientes simultâneos. Particionar os dados nessa situação pode ajudar a reduzir a contenção e melhorar o rendimento.


Considere os seguintes pontos ao projetar um esquema de particionamento de dados:


Minimize as operações de acesso a dados entre partições . Sempre que possível, mantenha os dados das operações de banco de dados mais comuns juntos em cada partição para minimizar as operações de acesso a dados entre partições. Consultar entre partições pode ser mais demorado do que consultar em uma única partição, mas otimizar partições para um conjunto de consultas pode afetar negativamente outros conjuntos de consultas. Se você precisar consultar entre partições, minimize o tempo de consulta executando consultas paralelas e agregando os resultados no aplicativo. (Essa abordagem pode não ser possível em alguns casos, como quando o resultado de uma consulta é usado na próxima consulta.)


Considere replicar dados de referência estática. Se as consultas usarem dados de referência relativamente estáticos, como tabelas de códigos postais ou listas de produtos, considere replicar esses dados em todas as partições para reduzir operações de pesquisa separadas em partições diferentes. Essa abordagem também pode reduzir a probabilidade de os dados de referência se tornarem um conjunto de dados "quente", com tráfego intenso em todo o sistema. No entanto, há um custo adicional associado à sincronização de quaisquer alterações nos dados de referência.


Minimize junções entre partições. Sempre que possível, minimize os requisitos de integridade referencial em partições verticais e funcionais. Nesses esquemas, o aplicativo é responsável por manter a integridade referencial entre as partições. As consultas que unem dados em várias partições são ineficientes porque o aplicativo normalmente precisa realizar consultas consecutivas com base em uma chave e depois em uma chave estrangeira. Em vez disso, considere replicar ou desnormalizar os dados relevantes. Se forem necessárias junções entre partições, execute consultas paralelas nas partições e una os dados no aplicativo.


Abrace a consistência eventual . Avalie se a consistência forte é realmente um requisito. Uma abordagem comum em sistemas distribuídos é implementar consistência eventual. Os dados em cada partição são atualizados separadamente e a lógica do aplicativo garante que todas as atualizações sejam concluídas com êxito. Ele também lida com as inconsistências que podem surgir da consulta de dados enquanto uma operação eventualmente consistente está em execução.


Considere como as consultas localizam a partição correta . Se uma consulta precisar varrer todas as partições para localizar os dados necessários, haverá um impacto significativo no desempenho, mesmo quando várias consultas paralelas estiverem em execução. Com o particionamento vertical e funcional, as consultas podem especificar naturalmente a partição. O particionamento horizontal, por outro lado, pode dificultar a localização de um item, pois todos os fragmentos têm o mesmo esquema. Uma solução típica para manter um mapa que é usado para pesquisar a localização do estilhaço para itens específicos. Esse mapa pode ser implementado na lógica de fragmentação do aplicativo ou mantido pelo armazenamento de dados se ele suportar fragmentação transparente.


Considere rebalancear os fragmentos periodicamente . Com o particionamento horizontal, o rebalanceamento de fragmentos pode ajudar a distribuir os dados uniformemente por tamanho e por carga de trabalho para minimizar pontos de acesso, maximizar o desempenho de consultas e contornar limitações de armazenamento físico. No entanto, essa é uma tarefa complexa que geralmente requer o uso de uma ferramenta ou processo personalizado.


Replicar partições. Se você replicar cada partição, ela fornecerá proteção adicional contra falhas. Se uma única réplica falhar, as consultas podem ser direcionadas para uma cópia de trabalho.


Se você atingir os limites físicos de uma estratégia de particionamento, talvez seja necessário estender a escalabilidade para um nível diferente . Por exemplo, se o particionamento estiver no nível do banco de dados, talvez seja necessário localizar ou replicar partições em vários bancos de dados. Se o particionamento já estiver no nível do banco de dados e as limitações físicas forem um problema, isso pode significar que você precisa localizar ou replicar partições em várias contas de hospedagem.


Evite transações que acessam dados em várias partições . Alguns armazenamentos de dados implementam consistência e integridade transacionais para operações que modificam dados, mas somente quando os dados estão localizados em uma única partição. Se você precisar de suporte transacional em várias partições, provavelmente precisará implementá-lo como parte da lógica do aplicativo, pois a maioria dos sistemas de particionamento não oferece suporte nativo.


Todos os armazenamentos de dados requerem alguma atividade de gerenciamento e monitoramento operacional. As tarefas podem variar de carregamento de dados, backup e restauração de dados, reorganização de dados e garantia de que o sistema está funcionando de forma correta e eficiente.


Considere os seguintes fatores que afetam o gerenciamento operacional:


Como implementar tarefas operacionais e de gerenciamento apropriadas quando os dados são particionados . Essas tarefas podem incluir backup e restauração, arquivamento de dados, monitoramento do sistema e outras tarefas administrativas. Por exemplo, manter a consistência lógica durante as operações de backup e restauração pode ser um desafio.


Como carregar os dados em várias partições e adicionar novos dados que chegam de outras fontes . Algumas ferramentas e utilitários podem não suportar operações de dados fragmentados, como carregar dados na partição correta.


Como arquivar e excluir os dados regularmente . Para evitar o crescimento excessivo de partições, você precisa arquivar e excluir dados regularmente (como mensalmente). Pode ser necessário transformar os dados para corresponder a um esquema de arquivo diferente.


Como localizar problemas de integridade de dados . Considere executar um processo periódico para localizar quaisquer problemas de integridade de dados, como dados em uma partição que fazem referência a informações ausentes em outra. O processo pode tentar corrigir esses problemas automaticamente ou gerar um relatório para revisão manual.


Reequilibrando partições
À medida que o sistema amadurece, talvez seja necessário ajustar o esquema de particionamento. Por exemplo, partições individuais podem começar a receber um volume desproporcional de tráfego e ficar quentes, levando a contenção excessiva. Ou você pode ter subestimado o volume de dados em algumas partições, fazendo com que algumas partições se aproximem dos limites de capacidade.


Alguns armazenamentos de dados, como o Cosmos DB, podem reequilibrar partições automaticamente. Em outros casos, o rebalanceamento é uma tarefa administrativa que consiste em duas etapas:


Determine uma nova estratégia de particionamento.


Quais partições precisam ser divididas (ou possivelmente combinadas)?
Qual é a nova chave de partição?
Migre dados do esquema de particionamento antigo para o novo conjunto de partições.


Dependendo do armazenamento de dados, você poderá migrar dados entre partições enquanto estiverem em uso. Isso é chamado de migração online . Se isso não for possível, talvez seja necessário tornar as partições indisponíveis enquanto os dados são realocados ( migração offline ).


Migração off-line
A migração offline é normalmente mais simples porque reduz as chances de ocorrência de contenção. Conceitualmente, a migração offline funciona da seguinte forma:


Marque a partição offline.
Split-merge e mova os dados para as novas partições.
Verifique os dados.
Coloque as novas partições online.
Remova a partição antiga.
Opcionalmente, você pode marcar uma partição como somente leitura na etapa 1, para que os aplicativos ainda possam ler os dados enquanto estão sendo movidos.


Migração online
A migração online é mais complexa de executar, mas menos disruptiva. O processo é semelhante à migração offline, exceto que a partição original não está marcada como offline. Dependendo da granularidade do processo de migração (por exemplo, item por item versus fragmento por fragmento), o código de acesso a dados nos aplicativos cliente pode ter que lidar com leitura e gravação de dados mantidos em dois locais, a partição original e a nova partição .


Padrões relacionados
Os padrões de design a seguir podem ser relevantes para seu cenário:


O padrão de fragmentação descreve algumas estratégias comuns para fragmentação de dados.


O padrão de tabela de índice mostra como criar índices secundários sobre dados. Um aplicativo pode recuperar dados rapidamente com essa abordagem, usando consultas que não fazem referência à chave primária de uma coleção.


O padrão de visualização materializada descreve como gerar visualizações pré-preenchidas que resumem dados para dar suporte a operações de consulta rápidas. Essa abordagem pode ser útil em um armazenamento de dados particionado se as partições que contêm os dados que estão sendo resumidos estiverem distribuídas em vários sites.


Estratégias de particionamento de dados




Este artigo descreve algumas estratégias para particionar dados em vários armazenamentos de dados do Azure. Para obter orientações gerais sobre quando particionar dados e práticas recomendadas, consulte Particionamento de dados .


Particionando o Banco de Dados SQL do Azure
Um único banco de dados SQL tem um limite para o volume de dados que pode conter. A taxa de transferência é limitada por fatores de arquitetura e pelo número de conexões simultâneas que ela suporta.


Os pools elásticos oferecem suporte ao dimensionamento horizontal para um banco de dados SQL. Usando pools elásticos, você pode particionar seus dados em estilhaços espalhados por vários bancos de dados SQL. Você também pode adicionar ou remover fragmentos à medida que o volume de dados que você precisa manipular aumenta e diminui. Os pools elásticos também podem ajudar a reduzir a contenção distribuindo a carga pelos bancos de dados.


Cada shard é implementado como um banco de dados SQL. Um shard pode conter mais de um conjunto de dados (chamado de shardlet ). Cada banco de dados mantém metadados que descrevem os shardlets que ele contém. Um shardlet pode ser um único item de dados ou um grupo de itens que compartilham a mesma chave de shardlet. Por exemplo, em um aplicativo multilocatário, a chave do shardlet pode ser a ID do inquilino e todos os dados de um inquilino podem ser mantidos no mesmo shardlet.


Os aplicativos cliente são responsáveis por associar um conjunto de dados a uma chave de shardlet. Um banco de dados SQL separado atua como um gerenciador de mapa de fragmentos global. Esse banco de dados possui uma lista de todos os shards e shardlets do sistema. O aplicativo se conecta ao banco de dados do gerenciador de mapa de fragmentos para obter uma cópia do mapa de fragmentos. Ele armazena em cache o mapa de fragmentos localmente e usa o mapa para rotear solicitações de dados para o fragmento apropriado. Essa funcionalidade está oculta por trás de uma série de APIs contidas na biblioteca cliente do Elastic Database , que está disponível para Java e .NET.


Para obter mais informações sobre pools elásticos, consulte Dimensionamento horizontal com o Banco de Dados SQL do Azure .


Para reduzir a latência e melhorar a disponibilidade, você pode replicar o banco de dados do gerenciador de mapa de fragmentos global. Com os níveis de preços Premium, você pode configurar a replicação geográfica ativa para copiar dados continuamente para bancos de dados em diferentes regiões.


Como alternativa, use o Azure SQL Data Sync ou o Azure Data Factory para replicar o banco de dados do gerenciador de mapa de fragmentos nas regiões. Essa forma de replicação é executada periodicamente e é mais adequada se o mapa de fragmentos for alterado com pouca frequência e não exigir a camada Premium.


O Elastic Database fornece dois esquemas para mapear dados para shardlets e armazená-los em shards:


Um mapa de fragmentos de lista associa uma única chave a um fragmento. Por exemplo, em um sistema multilocatário, os dados de cada locatário podem ser associados a uma chave exclusiva e armazenados em seu próprio shardlet. Para garantir o isolamento, cada shardlet pode ser mantido em seu próprio shard.






Um mapa de fragmentos de intervalo associa um conjunto de valores de chave contíguos a um fragmento. Por exemplo, você pode agrupar os dados de um conjunto de locatários (cada um com sua própria chave) no mesmo shardlet. Esse esquema é mais barato que o primeiro, porque os locatários compartilham o armazenamento de dados, mas tem menos isolamento.






Um único shard pode conter os dados de vários shardlets. Por exemplo, você pode usar shardlets de lista para armazenar dados de diferentes locatários não contíguos no mesmo shard. Você também pode misturar shardlets de intervalo e shardlets de lista no mesmo shard, embora eles sejam abordados por meio de mapas diferentes. O diagrama a seguir mostra essa abordagem:






Os pools elásticos permitem adicionar e remover fragmentos à medida que o volume de dados diminui e aumenta. Os aplicativos cliente podem criar e excluir shards dinamicamente e atualizar de forma transparente o gerenciador de mapa de shard. No entanto, remover um estilhaço é uma operação destrutiva que também requer a exclusão de todos os dados desse estilhaço.


Se um aplicativo precisar dividir um estilhaço em dois estilhaços separados ou combinar estilhaços, use a ferramenta de mesclagem de divisão . Essa ferramenta é executada como um serviço Web do Azure e migra dados com segurança entre estilhaços.


O esquema de particionamento pode afetar significativamente o desempenho do seu sistema. Também pode afetar a taxa na qual os estilhaços precisam ser adicionados ou removidos, ou que os dados devem ser reparticionados entre os estilhaços. Considere os seguintes pontos:


Agrupe dados usados juntos no mesmo estilhaço e evite operações que acessem dados de vários estilhaços. Um shard é um banco de dados SQL por si só, e as junções entre bancos de dados devem ser executadas no lado do cliente.


Embora o Banco de Dados SQL não dê suporte a associações entre bancos de dados, você pode usar as ferramentas do Elastic Database para realizar consultas de vários fragmentos . Uma consulta de vários fragmentos envia consultas individuais para cada banco de dados e mescla os resultados.


Não projete um sistema que tenha dependências entre shards. Restrições de integridade referencial, gatilhos e procedimentos armazenados em um banco de dados não podem fazer referência a objetos em outro.


Se você tiver dados de referência usados com frequência por consultas, considere replicar esses dados em estilhaços. Essa abordagem pode eliminar a necessidade de unir dados entre bancos de dados. Idealmente, esses dados devem ser estáticos ou de movimento lento, para minimizar o esforço de replicação e reduzir as chances de ficarem obsoletos.


Shardlets que pertencem ao mesmo mapa de fragmentos devem ter o mesmo esquema. Essa regra não é imposta pelo Banco de Dados SQL, mas o gerenciamento e a consulta de dados se tornam muito complexos se cada shardlet tiver um esquema diferente. Em vez disso, crie mapas de fragmentos separados para cada esquema. Lembre-se de que dados pertencentes a diferentes shardlets podem ser armazenados no mesmo shard.


As operações transacionais só têm suporte para dados em um estilhaço, e não entre estilhaços. As transações podem abranger shardlets desde que façam parte do mesmo shard. Portanto, se sua lógica de negócios precisar realizar transações, armazene os dados no mesmo shard ou implemente consistência eventual.


Coloque os estilhaços perto dos usuários que acessam os dados nesses estilhaços. Essa estratégia ajuda a reduzir a latência.


Evite ter uma mistura de fragmentos altamente ativos e relativamente inativos. Tente distribuir a carga uniformemente pelos fragmentos. Isso pode exigir o hash das chaves de fragmentação. Se você estiver geolocalizando fragmentos, certifique-se de que as chaves com hash sejam mapeadas para fragmentos mantidos em fragmentos armazenados próximos aos usuários que acessam esses dados.


Particionando o armazenamento de tabelas do Azure
O armazenamento de tabelas do Azure é um armazenamento de chave-valor projetado para particionamento. Todas as entidades são armazenadas em uma partição e as partições são gerenciadas internamente pelo armazenamento de tabelas do Azure. Cada entidade armazenada em uma tabela deve fornecer uma chave de duas partes que inclui:


A chave de partição . Este é um valor de cadeia de caracteres que determina a partição onde o armazenamento de tabelas do Azure colocará a entidade. Todas as entidades com a mesma chave de partição são armazenadas na mesma partição.


A tecla de linha . Este é um valor de string que identifica a entidade dentro da partição. Todas as entidades dentro de uma partição são classificadas lexicamente, em ordem crescente, por essa chave. A combinação de chave de partição/chave de linha deve ser exclusiva para cada entidade e não pode exceder 1 KB de comprimento.


Se uma entidade for adicionada a uma tabela com uma chave de partição não utilizada anteriormente, o armazenamento de tabelas do Azure cria uma nova partição para essa entidade. Outras entidades com a mesma chave de partição serão armazenadas na mesma partição.


Esse mecanismo implementa efetivamente uma estratégia de expansão automática. Cada partição é armazenada no mesmo servidor em um datacenter do Azure para ajudar a garantir que as consultas que recuperam dados de uma única partição sejam executadas rapidamente.


A Microsoft publicou metas de escalabilidade para o Armazenamento do Azure. Se o seu sistema provavelmente exceder esses limites, considere dividir as entidades em várias tabelas. Use o particionamento vertical para dividir os campos nos grupos com maior probabilidade de serem acessados juntos.


O diagrama a seguir mostra a estrutura lógica de uma conta de armazenamento de exemplo. A conta de armazenamento contém três tabelas: Informações do cliente, Informações do produto e Informações do pedido.



Cada tabela tem várias partições.


Na tabela Customer Info, os dados são particionados de acordo com a cidade onde o cliente está localizado. A chave de linha contém o ID do cliente.
Na tabela Informações do produto, os produtos são particionados por categoria de produto e a chave de linha contém o número do produto.
Na tabela Informações do pedido, os pedidos são particionados por data do pedido e a chave de linha especifica a hora em que o pedido foi recebido. Todos os dados são ordenados pela chave de linha em cada partição.
Considere os seguintes pontos ao projetar suas entidades para armazenamento de tabelas do Azure:


Selecione uma chave de partição e uma chave de linha de acordo com a forma como os dados são acessados. Escolha uma combinação de chave de partição/chave de linha que suporte a maioria de suas consultas. As consultas mais eficientes recuperam dados especificando a chave de partição e a chave de linha. As consultas que especificam uma chave de partição e um intervalo de chaves de linha podem ser concluídas verificando uma única partição. Isso é relativamente rápido porque os dados são mantidos em ordem de chave de linha. Se as consultas não especificarem qual partição verificar, todas as partições deverão ser verificadas.


Se uma entidade tiver uma chave natural, use-a como chave de partição e especifique uma string vazia como chave de linha. Se uma entidade tiver uma chave composta que consiste em duas propriedades, selecione a propriedade de alteração mais lenta como a chave de partição e a outra como a chave de linha. Se uma entidade tiver mais de duas propriedades de chave, use uma concatenação de propriedades para fornecer as chaves de partição e de linha.


Se você realizar consultas regularmente que pesquisam dados usando campos que não sejam as chaves de partição e linha, considere implementar o padrão Tabela de Índice ou considere usar um armazenamento de dados diferente que dê suporte à indexação, como Cosmos DB.


Se você gerar chaves de partição usando uma sequência monotônica (como "0001", "0002", "0003") e cada partição contiver apenas uma quantidade limitada de dados, o armazenamento de tabelas do Azure poderá agrupar fisicamente essas partições no mesmo servidor. O Armazenamento do Azure pressupõe que o aplicativo tem maior probabilidade de realizar consultas em um intervalo contíguo de partições (consultas de intervalo) e é otimizado para esse caso. No entanto, essa abordagem pode levar a hotspots, pois todas as inserções de novas entidades provavelmente estarão concentradas em uma extremidade da faixa contígua. Também pode reduzir a escalabilidade. Para distribuir a carga de maneira mais uniforme, considere o hash da chave de partição.


O armazenamento de tabelas do Azure dá suporte a operações transacionais para entidades que pertencem à mesma partição. Um aplicativo pode executar várias operações de inserção, atualização, exclusão, substituição ou mesclagem como uma unidade atômica, desde que a transação não inclua mais de 100 entidades e a carga útil da solicitação não exceda 4 MB. As operações que abrangem várias partições não são transacionais e podem exigir a implementação de consistência eventual. Para obter mais informações sobre armazenamento e transações de tabelas, consulte Executando transações de grupo de entidades .


Considere a granularidade da chave de partição:


Usar a mesma chave de partição para cada entidade resulta em uma única partição mantida em um servidor. Isso evita que a partição seja dimensionada e concentra a carga em um único servidor. Como resultado, essa abordagem é adequada apenas para armazenar um pequeno número de entidades. No entanto, garante que todas as entidades possam participar das transações do grupo de entidades.


O uso de uma chave de partição exclusiva para cada entidade faz com que o serviço de armazenamento de tabela crie uma partição separada para cada entidade, possivelmente resultando em um grande número de pequenas partições. Essa abordagem é mais escalável do que usar uma única chave de partição, mas as transações do grupo de entidades não são possíveis. Além disso, as consultas que buscam mais de uma entidade podem envolver a leitura de mais de um servidor. No entanto, se o aplicativo executar consultas de intervalo, o uso de uma sequência monotônica para as chaves de partição pode ajudar a otimizar essas consultas.


Compartilhar a chave de partição em um subconjunto de entidades possibilita agrupar entidades relacionadas na mesma partição. As operações que envolvem entidades relacionadas podem ser executadas usando transações de grupo de entidades, e as consultas que buscam um conjunto de entidades relacionadas podem ser atendidas acessando um único servidor.


Para obter mais informações, consulte o guia de design da tabela de armazenamento do Azure e a estratégia de particionamento escalonável .


Particionando o Armazenamento de Blobs do Azure
O Armazenamento de Blobs do Azure possibilita a retenção de grandes objetos binários. Use blobs de blocos em cenários em que você precisa carregar ou baixar grandes volumes de dados rapidamente. Use blobs de página para aplicativos que exigem acesso aleatório em vez de serial a partes dos dados.


Cada blob (bloco ou página) é mantido em um contêiner em uma conta de armazenamento do Azure. Você pode usar contêineres para agrupar blobs relacionados que tenham os mesmos requisitos de segurança. Este agrupamento é lógico e não físico. Dentro de um contêiner, cada blob tem um nome exclusivo.


A chave de partição para um blob é nome da conta + nome do contêiner + nome do blob. A chave de partição é usada para particionar dados em intervalos e esses intervalos têm balanceamento de carga em todo o sistema. Os blobs podem ser distribuídos em vários servidores para dimensionar o acesso a eles, mas um único blob só pode ser atendido por um único servidor.


Se o seu esquema de nomenclatura usa carimbos de data/hora ou identificadores numéricos, isso pode levar a tráfego excessivo para uma partição, limitando o sistema de balanceamento de carga eficaz. Por exemplo, se você tiver operações diárias que usam um objeto blob com um carimbo de data/hora como yyyy-mm-dd , todo o tráfego dessa operação irá para um único servidor de partição. Em vez disso, considere prefixar o nome com um hash de três dígitos. Para obter mais informações, consulte Convenção de nomenclatura de partição .


As ações de escrever um único bloco ou página são atômicas, mas as operações que abrangem blocos, páginas ou blobs não são. Se você precisar garantir a consistência ao realizar operações de gravação em blocos, páginas e blobs, elimine um bloqueio de gravação usando uma concessão de blob.


Particionando filas de armazenamento do Azure
As filas de armazenamento do Azure permitem que você implemente mensagens assíncronas entre processos. Uma conta de armazenamento do Azure pode conter qualquer número de filas e cada fila pode conter qualquer número de mensagens. A única limitação é o espaço disponível na conta de armazenamento. O tamanho máximo de uma mensagem individual é 64 KB. Se você precisar de mensagens maiores que isso, considere usar as filas do Barramento de Serviço do Azure.


Cada fila de armazenamento tem um nome exclusivo na conta de armazenamento que a contém. Filas de partições do Azure com base no nome. Todas as mensagens para a mesma fila são armazenadas na mesma partição, que é controlada por um único servidor. Filas diferentes podem ser gerenciadas por servidores diferentes para ajudar a equilibrar a carga. A alocação de filas para servidores é transparente para aplicativos e usuários.


Em um aplicativo de grande escala, não use a mesma fila de armazenamento para todas as instâncias do aplicativo, pois essa abordagem pode fazer com que o servidor que está hospedando a fila se torne um ponto de acesso. Em vez disso, use filas diferentes para diferentes áreas funcionais do aplicativo. As filas de armazenamento do Azure não dão suporte a transações, portanto, direcionar mensagens para filas diferentes deve ter pouco efeito na consistência das mensagens.


Uma fila de armazenamento do Azure pode lidar com até 2.000 mensagens por segundo. Se você precisar processar mensagens em uma taxa maior que essa, considere a criação de várias filas. Por exemplo, em um aplicativo global, crie filas de armazenamento separadas em contas de armazenamento separadas para lidar com instâncias de aplicativos em execução em cada região.


Particionando o Barramento de Serviço do Azure
O Barramento de Serviço do Azure usa um agente de mensagens para lidar com mensagens enviadas para uma fila ou tópico do Barramento de Serviço. Por padrão, todas as mensagens enviadas para uma fila ou tópico são tratadas pelo mesmo processo do intermediário de mensagens. Essa arquitetura pode limitar o rendimento geral da fila de mensagens. No entanto, você também pode particionar uma fila ou um tópico quando ele é criado. Você faz isso definindo a propriedade EnablePartitioning da fila ou descrição do tópico como true .


Uma fila ou tópico particionado é dividido em vários fragmentos, cada um dos quais é suportado por um armazenamento de mensagens e um intermediário de mensagens separados. O Barramento de Serviço assume a responsabilidade de criar e gerenciar esses fragmentos. Quando um aplicativo publica uma mensagem em uma fila ou tópico particionado, o Barramento de Serviço atribui a mensagem a um fragmento dessa fila ou tópico. Quando um aplicativo recebe uma mensagem de uma fila ou assinatura, o Barramento de Serviço verifica cada fragmento para a próxima mensagem disponível e a passa para o aplicativo para processamento.


Essa estrutura ajuda a distribuir a carga entre agentes de mensagens e armazenamentos de mensagens, aumentando a escalabilidade e melhorando a disponibilidade. Se o agente de mensagens ou armazenamento de mensagens de um fragmento estiver temporariamente indisponível, o Barramento de Serviço poderá recuperar mensagens de um dos fragmentos disponíveis restantes.


O Barramento de Serviço atribui uma mensagem a um fragmento da seguinte maneira:


Se a mensagem pertencer a uma sessão, todas as mensagens com o mesmo valor para a propriedade SessionId serão enviadas para o mesmo fragmento.


Se a mensagem não pertencer a uma sessão, mas o remetente tiver especificado um valor para a propriedade PartitionKey , todas as mensagens com o mesmo valor PartitionKey serão enviadas para o mesmo fragmento.


 Observação


Se as propriedades SessionId e PartitionKey forem especificadas, elas deverão ser definidas com o mesmo valor ou a mensagem será rejeitada.


Se as propriedades SessionId e PartitionKey de uma mensagem não forem especificadas, mas a detecção de duplicatas estiver habilitada, a propriedade MessageId será usada. Todas as mensagens com o mesmo MessageId serão direcionadas para o mesmo fragmento.


Se as mensagens não incluírem uma propriedade SessionId, PartitionKey ou MessageId , o Barramento de Serviço atribuirá mensagens a fragmentos sequencialmente. Se um fragmento não estiver disponível, o Barramento de Serviço passará para o próximo. Isso significa que uma falha temporária na infraestrutura do sistema de mensagens não causa falha na operação de envio de mensagens.


Considere os seguintes pontos ao decidir se ou como particionar uma fila de mensagens ou um tópico do Barramento de Serviço:


As filas e os tópicos do Barramento de Serviço são criados no escopo de um namespace do Barramento de Serviço. O Barramento de Serviço atualmente permite até 100 filas ou tópicos particionados por namespace.


Cada namespace do Barramento de Serviço impõe cotas aos recursos disponíveis, como o número de assinaturas por tópico, o número de solicitações simultâneas de envio e recebimento por segundo e o número máximo de conexões simultâneas que podem ser estabelecidas. Essas cotas estão documentadas nas cotas do Barramento de Serviço . Se você espera exceder esses valores, crie namespaces adicionais com suas próprias filas e tópicos e distribua o trabalho entre esses namespaces. Por exemplo, em um aplicativo global, crie namespaces separados em cada região e configure instâncias do aplicativo para usar as filas e os tópicos no namespace mais próximo.


As mensagens enviadas como parte de uma transação devem especificar uma chave de partição. Isso pode ser uma propriedade SessionId , PartitionKey ou MessageId . Todas as mensagens enviadas como parte da mesma transação devem especificar a mesma chave de partição porque devem ser tratadas pelo mesmo processo do intermediário de mensagens. Você não pode enviar mensagens para filas ou tópicos diferentes na mesma transação.


Filas e tópicos particionados não podem ser configurados para serem excluídos automaticamente quando ficarem ociosos.


Filas e tópicos particionados não podem ser usados no momento com o Advanced Message Queuing Protocol (AMQP) se você estiver criando soluções de plataforma cruzada ou híbridas.


Particionando o Cosmos DB
O Azure Cosmos DB é um banco de dados NoSQL que pode armazenar documentos JSON usando a API SQL do Azure Cosmos DB . Um documento em um banco de dados Cosmos DB é uma representação serializada em JSON de um objeto ou outro dado. Nenhum esquema fixo é aplicado, exceto que cada documento deve conter um ID exclusivo.


Os documentos são organizados em coleções. Você pode agrupar documentos relacionados em uma coleção. Por exemplo, em um sistema que mantém postagens de blog, você pode armazenar o conteúdo de cada postagem de blog como um documento em uma coleção. Você também pode criar coleções para cada tipo de assunto. Como alternativa, em um aplicativo multilocatário, como um sistema em que diferentes autores controlam e gerenciam suas próprias postagens de blog, você pode particionar blogs por autor e criar coleções separadas para cada autor. O espaço de armazenamento alocado para coleções é elástico e pode diminuir ou aumentar conforme necessário.


O Cosmos DB dá suporte ao particionamento automático de dados com base em uma chave de partição definida pelo aplicativo. Uma partição lógica é uma partição que armazena todos os dados para um único valor de chave de partição. Todos os documentos que compartilham o mesmo valor para a chave de partição são colocados na mesma partição lógica. O Cosmos DB distribui valores de acordo com o hash da chave de partição. Uma partição lógica tem um tamanho máximo de 10 GB. Portanto, a escolha da chave de partição é uma decisão importante em tempo de design. Escolha um imóvel com uma ampla gama de valores e até mesmo padrões de acesso. Para obter mais informações, consulte Partição e dimensionamento no Azure Cosmos DB .


 Observação


Cada banco de dados Cosmos DB tem um nível de desempenho que determina a quantidade de recursos que ele obtém. Um nível de desempenho está associado a um limite de taxa de unidade de solicitação (RU). O limite de taxa de RU especifica o volume de recursos reservado e disponível para uso exclusivo dessa coleção. O custo de uma coleção depende do nível de desempenho selecionado para essa coleção. Quanto maior o nível de desempenho (e o limite de taxa de RU), maior a carga. Você pode ajustar o nível de desempenho de uma coleção usando o portal do Azure. Para obter mais informações, consulte Unidades de solicitação no Azure Cosmos DB .


Se o mecanismo de particionamento fornecido pelo Cosmos DB não for suficiente, talvez seja necessário fragmentar os dados no nível do aplicativo. As coleções de documentos fornecem um mecanismo natural para particionar dados em um único banco de dados. A maneira mais simples de implementar a fragmentação é criar uma coleção para cada fragmento. Os contêineres são recursos lógicos e podem abranger um ou mais servidores. Os contêineres de tamanho fixo têm um limite máximo de 10 GB e taxa de transferência de 10.000 RU/s. Os contêineres ilimitados não têm um tamanho máximo de armazenamento, mas devem especificar uma chave de partição. Com a fragmentação do aplicativo, o aplicativo cliente deve direcionar as solicitações para o fragmento apropriado, geralmente implementando seu próprio mecanismo de mapeamento com base em alguns atributos dos dados que definem a chave do fragmento.


Todos os bancos de dados são criados no contexto de uma conta de banco de dados Cosmos DB. Uma única conta pode conter vários bancos de dados e especifica em quais regiões os bancos de dados são criados. Cada conta também impõe seu próprio controle de acesso. Você pode usar contas do Cosmos DB para localizar geograficamente fragmentos (coleções em bancos de dados) próximos aos usuários que precisam acessá-los e impor restrições para que apenas esses usuários possam se conectar a eles.


Considere os seguintes pontos ao decidir como particionar dados com a API SQL do Cosmos DB:


Os recursos disponíveis para um banco de dados Cosmos DB estão sujeitos às limitações de cota da conta . Cada banco de dados pode conter várias coleções e cada coleção está associada a um nível de desempenho que controla o limite de taxa de RU (taxa de transferência reservada) para essa coleção. Para obter mais informações, consulte limites de serviço e assinatura do Azure, cotas e restrições .


Cada documento deve ter um atributo que possa ser usado para identificar exclusivamente aquele documento dentro da coleção na qual ele é mantido . Esse atributo é diferente da chave de fragmentação, que define qual coleção contém o documento. Uma coleção pode conter um grande número de documentos. Em teoria, é limitado apenas pelo comprimento máximo do ID do documento. A ID do documento pode ter até 255 caracteres.


Todas as operações em um documento são executadas no contexto de uma transação. As transações têm como escopo a coleção na qual o documento está contido. Se uma operação falhar, o trabalho que ela executou será revertido. Enquanto um documento está sujeito a uma operação, quaisquer alterações feitas estão sujeitas ao isolamento em nível de instantâneo. Esse mecanismo garante que, se, por exemplo, uma solicitação para criar um novo documento falhar, outro usuário que estiver consultando o banco de dados simultaneamente não verá um documento parcial que será removido.


As consultas de banco de dados também têm o escopo definido para o nível de coleção . Uma única consulta pode recuperar dados de apenas uma coleção. Se você precisar recuperar dados de várias coleções, deverá consultar cada coleção individualmente e mesclar os resultados no código do aplicativo.


O Cosmos DB oferece suporte a itens programáveis que podem ser armazenados em uma coleção ao lado de documentos. Isso inclui procedimentos armazenados, funções definidas pelo usuário e gatilhos (escritos em JavaScript). Esses itens podem acessar qualquer documento dentro da mesma coleção. Além disso, esses itens são executados dentro do escopo da transação ambiente (no caso de um gatilho que é acionado como resultado de uma operação de criação, exclusão ou substituição realizada em um documento) ou iniciando uma nova transação (no caso de um procedimento armazenado que é executado como resultado de uma solicitação explícita do cliente). Se o código em um item programável lançar uma exceção, a transação será revertida. Você pode usar procedimentos armazenados e gatilhos para manter a integridade e a consistência entre os documentos, mas todos esses documentos devem fazer parte da mesma coleção.


As coleções que você pretende manter nos bancos de dados provavelmente não excederão os limites de rendimento definidos pelos níveis de desempenho das coleções . Para obter mais informações, consulte Unidades de solicitação no Azure Cosmos DB . Se você antecipar atingir esses limites, considere dividir as coleções entre bancos de dados em contas diferentes para reduzir a carga por coleção.


Particionando a Pesquisa do Azure
A capacidade de pesquisar dados geralmente é o principal método de navegação e exploração fornecido por muitos aplicativos da Web. Ele ajuda os usuários a encontrar recursos rapidamente (por exemplo, produtos em um aplicativo de comércio eletrônico) com base em combinações de critérios de pesquisa. O serviço de pesquisa do Azure fornece recursos de pesquisa de texto completo em conteúdo da Web e inclui recursos como digitação antecipada, consultas sugeridas com base em correspondências próximas e navegação facetada. Para obter mais informações, consulte O que é o Azure Search? .


O Azure Search armazena conteúdo pesquisável como documentos JSON em um banco de dados. Você define índices que especificam os campos pesquisáveis nesses documentos e fornece essas definições ao Azure Search. Quando um usuário envia uma solicitação de pesquisa, o Azure Search usa os índices apropriados para localizar itens correspondentes.


Para reduzir a contenção, o armazenamento usado pelo Azure Search pode ser dividido em 1, 2, 3, 4, 6 ou 12 partições, e cada partição pode ser replicada até 6 vezes. O produto do número de partições multiplicado pelo número de réplicas é chamado de unidade de pesquisa (SU). Uma única instância do Azure Search pode conter no máximo 36 SUs (um banco de dados com 12 partições dá suporte apenas a um máximo de 3 réplicas).


Você é cobrado por cada SU alocada ao seu serviço. À medida que o volume de conteúdo pesquisável aumenta ou a taxa de solicitações de pesquisa aumenta, você pode adicionar SUs a uma instância existente do Azure Search para lidar com a carga extra. O próprio Azure Search distribui os documentos uniformemente pelas partições. Nenhuma estratégia de particionamento manual é suportada atualmente.


Cada partição pode conter no máximo 15 milhões de documentos ou ocupar 300 GB de espaço de armazenamento (o que for menor). Você pode criar até 50 índices. O desempenho do serviço varia e depende da complexidade dos documentos, dos índices disponíveis e dos efeitos da latência da rede. Em média, uma única réplica (1 SU) deve ser capaz de lidar com 15 consultas por segundo (QPS), embora seja recomendável realizar o benchmarking com seus próprios dados para obter uma medida mais precisa da taxa de transferência. Para obter mais informações, confira Limites de serviço no Azure Search .


 Observação


Você pode armazenar um conjunto limitado de tipos de dados em documentos pesquisáveis, incluindo strings, booleanos, dados numéricos, dados de data e hora e alguns dados geográficos. Para obter mais informações, consulte a página Tipos de dados com suporte (Pesquisa do Azure) no site da Microsoft.


Você tem controle limitado sobre como o Azure Search particiona os dados para cada instância do serviço. No entanto, em um ambiente global, você pode melhorar o desempenho e reduzir ainda mais a latência e a contenção particionando o próprio serviço usando uma das seguintes estratégias:


Crie uma instância do Azure Search em cada região geográfica e certifique-se de que os aplicativos cliente sejam direcionados para a instância disponível mais próxima. Essa estratégia exige que todas as atualizações do conteúdo pesquisável sejam replicadas em tempo hábil em todas as instâncias do serviço.


Crie duas camadas do Azure Search:


Um serviço local em cada região que contém os dados acessados com mais frequência pelos usuários dessa região. Os usuários podem direcionar solicitações aqui para resultados rápidos, mas limitados.
Um serviço global que engloba todos os dados. Os usuários podem direcionar solicitações aqui para obter resultados mais lentos, porém mais completos.
Essa abordagem é mais adequada quando há uma variação regional significativa nos dados que estão sendo pesquisados.


Particionando o Cache do Azure para Redis
O Cache do Azure para Redis fornece um serviço de cache compartilhado na nuvem baseado no armazenamento de dados de chave-valor do Redis. Como o próprio nome indica, o Cache do Azure para Redis destina-se a ser uma solução de cache. Use-o apenas para armazenar dados transitórios e não como um armazenamento de dados permanente. Os aplicativos que usam o Cache do Azure para Redis devem continuar funcionando se o cache não estiver disponível. O Cache do Azure para Redis dá suporte à replicação primária/secundária para fornecer alta disponibilidade, mas atualmente limita o tamanho máximo do cache a 53 GB. Se você precisar de mais espaço do que isso, deverá criar caches adicionais. Para obter mais informações, consulte Cache do Azure para Redis .


O particionamento de um armazenamento de dados Redis envolve a divisão dos dados entre instâncias do serviço Redis. Cada instância constitui uma única partição. O Cache do Azure para Redis abstrai os serviços Redis por trás de uma fachada e não os expõe diretamente. A maneira mais simples de implementar o particionamento é criar várias instâncias do Cache do Azure para Redis e distribuir os dados entre elas.


Você pode associar cada item de dados a um identificador (uma chave de partição) que especifica qual cache armazena o item de dados. A lógica do aplicativo cliente pode usar esse identificador para rotear solicitações para a partição apropriada. Esse esquema é muito simples, mas se o esquema de particionamento for alterado (por exemplo, se instâncias adicionais do Cache do Azure para Redis forem criadas), talvez os aplicativos cliente precisem ser reconfigurados.


O Redis nativo (não o Cache do Azure para Redis) dá suporte ao particionamento do lado do servidor com base no cluster do Redis. Nessa abordagem, você pode dividir os dados uniformemente entre os servidores usando um mecanismo de hash. Cada servidor Redis armazena metadados que descrevem o intervalo de chaves de hash que a partição contém e também contém informações sobre quais chaves de hash estão localizadas nas partições em outros servidores.


Os aplicativos cliente simplesmente enviam solicitações para qualquer um dos servidores Redis participantes (provavelmente o mais próximo). O servidor Redis examina a solicitação do cliente. Se puder ser resolvido localmente, ele executa a operação solicitada. Caso contrário, ele encaminha a solicitação para o servidor apropriado.


Esse modelo é implementado usando o clustering do Redis e é descrito com mais detalhes na página do tutorial do cluster do Redis no site do Redis. O clustering do Redis é transparente para aplicativos cliente. Servidores Redis adicionais podem ser adicionados ao cluster (e os dados podem ser reparticionados) sem exigir que você reconfigure os clientes.


 Importante


Atualmente, o Cache do Azure para Redis dá suporte ao clustering do Redis apenas na camada premium .


A página Particionamento: como dividir dados entre várias instâncias do Redis no site do Redis fornece mais informações sobre como implementar o particionamento com o Redis. O restante desta seção pressupõe que você esteja implementando o particionamento do lado do cliente ou assistido por proxy.


Considere os seguintes pontos ao decidir como particionar dados com o Cache do Azure para Redis:


O Cache do Azure para Redis não se destina a atuar como um armazenamento de dados permanente, portanto, seja qual for o esquema de particionamento que você implementar, o código do aplicativo deve poder recuperar dados de um local que não seja o cache.


Os dados frequentemente acessados juntos devem ser mantidos na mesma partição. O Redis é um armazenamento de valor-chave poderoso que fornece vários mecanismos altamente otimizados para estruturar dados. Esses mecanismos podem ser um dos seguintes:


Strings simples (dados binários de até 512 MB de comprimento)
Tipos agregados, como listas (que podem atuar como filas e pilhas)
Conjuntos (ordenados e não ordenados)
Hashes (que podem agrupar campos relacionados, como os itens que representam os campos em um objeto)
Os tipos agregados permitem associar muitos valores relacionados à mesma chave. Uma chave Redis identifica uma lista, conjunto ou hash em vez dos itens de dados que ela contém. Esses tipos estão todos disponíveis com o Cache do Azure para Redis e são descritos pelos tipos de dadospágina no site Redis. Por exemplo, em parte de um sistema de comércio eletrônico que rastreia os pedidos feitos pelos clientes, os detalhes de cada cliente podem ser armazenados em um hash Redis que é codificado usando o ID do cliente. Cada hash pode conter uma coleção de IDs de pedido para o cliente. Um conjunto Redis separado pode conter os pedidos, novamente estruturados como hashes e codificados usando o ID do pedido. A Figura 8 mostra essa estrutura. Observe que o Redis não implementa nenhuma forma de integridade referencial, portanto, é responsabilidade do desenvolvedor manter os relacionamentos entre clientes e pedidos.



Figura 8. Estrutura sugerida no armazenamento Redis para registro de pedidos de clientes e seus detalhes.


 Observação


No Redis, todas as chaves são valores de dados binários (como strings do Redis) e podem conter até 512 MB de dados. Em teoria, uma chave pode conter quase qualquer informação. No entanto, recomendamos a adoção de uma convenção de nomenclatura consistente para chaves que seja descritiva do tipo de dados e que identifique a entidade, mas que não seja excessivamente longa. Uma abordagem comum é usar chaves no formato "entity_type:ID". Por exemplo, você pode usar "cliente:99" para indicar a chave de um cliente com o ID 99.


Você pode implementar o particionamento vertical armazenando informações relacionadas em diferentes agregações no mesmo banco de dados. Por exemplo, em um aplicativo de comércio eletrônico, você pode armazenar informações acessadas com frequência sobre produtos em um hash Redis e informações detalhadas usadas com menos frequência em outro. Ambos os hashes podem usar o mesmo ID do produto como parte da chave. Por exemplo, você pode usar "product: nn " (onde nn é o ID do produto) para as informações do produto e "product_details: nn " para os dados detalhados. Essa estratégia pode ajudar a reduzir o volume de dados que a maioria das consultas provavelmente recuperará.


Você pode reparticionar um armazenamento de dados Redis, mas lembre-se de que é uma tarefa complexa e demorada. O clustering do Redis pode reparticionar os dados automaticamente, mas esse recurso não está disponível com o Cache do Azure para Redis. Portanto, ao projetar seu esquema de particionamento, tente deixar espaço livre suficiente em cada partição para permitir o crescimento de dados esperado ao longo do tempo. No entanto, lembre-se de que o Cache do Azure para Redis destina-se a armazenar dados em cache temporariamente e que os dados mantidos no cache podem ter um tempo de vida limitado especificado como um valor de tempo de vida (TTL). Para dados relativamente voláteis, o TTL pode ser curto, mas para dados estáticos o TTL pode ser muito mais longo. Evite armazenar grandes quantidades de dados de longa duração no cache se o volume desses dados provavelmente preencher o cache.


 Observação


Ao usar o Cache do Azure para Redis, você especifica o tamanho máximo do cache (de 250 MB a 53 GB) selecionando o tipo de preço apropriado. No entanto, após a criação de um Cache do Azure para Redis, você não pode aumentar (ou diminuir) seu tamanho.


Lotes e transações do Redis não podem abranger várias conexões, portanto, todos os dados afetados por um lote ou transação devem ser mantidos no mesmo banco de dados (shard).


 Observação


Uma sequência de operações em uma transação Redis não é necessariamente atômica. Os comandos que compõem uma transação são verificados e enfileirados antes de serem executados. Se ocorrer um erro durante esta fase, toda a fila será descartada. No entanto, após a transação ter sido submetida com sucesso, os comandos enfileirados são executados em sequência. Se algum comando falhar, apenas esse comando parará de ser executado. Todos os comandos anteriores e subsequentes na fila são executados. Para obter mais informações, acesse a página Transações no site do Redis.


O Redis oferece suporte a um número limitado de operações atômicas. As únicas operações desse tipo que suportam várias chaves e valores são as operações MGET e MSET. As operações MGET retornam uma coleção de valores para uma lista especificada de chaves e as operações MSET armazenam uma coleção de valores para uma lista especificada de chaves. Se você precisar usar essas operações, os pares chave-valor referenciados pelos comandos MSET e MGET devem ser armazenados no mesmo banco de dados.


Particionando o Azure Service Fabric
O Azure Service Fabric é uma plataforma de microsserviços que fornece um tempo de execução para aplicativos distribuídos na nuvem. O Service Fabric dá suporte a executáveis de convidado .NET, serviços com estado e sem estado e contêineres. Os serviços com estado fornecem uma coleção confiável para armazenar dados persistentemente em uma coleção de chave-valor no cluster do Service Fabric. Para obter mais informações sobre estratégias de particionamento de chaves em uma coleção confiável, consulte diretrizes e recomendações para coleções confiáveis no Azure Service Fabric .


Mais Informações
Visão geral do Azure Service Fabric é uma introdução ao Azure Service Fabric.


Os serviços confiáveis do Partition Service Fabric fornecem mais informações sobre serviços confiáveis no Azure Service Fabric.


Particionando Hubs de Eventos do Azure
Os Hubs de Eventos do Azure foram projetados para streaming de dados em grande escala e o particionamento é integrado ao serviço para habilitar o dimensionamento horizontal. Cada consumidor lê apenas uma partição específica do fluxo de mensagens.


O editor de eventos está ciente apenas de sua chave de partição, não da partição na qual os eventos são publicados. Esse desacoplamento de chave e partição isola o remetente da necessidade de saber muito sobre o processamento downstream. (Também é possível enviar eventos diretamente para uma determinada partição, mas geralmente isso não é recomendado.)


Considere a escala de longo prazo ao selecionar a contagem de partição. Depois que um hub de eventos é criado, você não pode alterar o número de partições.


Para obter mais informações sobre como usar partições em Hubs de Eventos, consulte O que são Hubs de Eventos? .


Para considerações sobre compensações entre disponibilidade e consistência, consulte Disponibilidade e consistência nos Hubs de Eventos .


Entenda o esquema em estrela e a importância para o Power BI
Artigo15/01/2022
21 minutos para o fim da leitura
Esta página é útil?




Este artigo se destina a modeladores de dados do Power BI Desktop. Descreve o design do esquema em estrela e sua relevância para desenvolver modelos de dados do Power BI otimizados para desempenho e usabilidade.
Este artigo não pretende oferecer uma discussão completa sobre o design do esquema em estrela. Para obter mais detalhes, leia diretamente o conteúdo publicado, como The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling (3ª edição, 2013) de Ralph Kimball et al.


Visão geral do esquema em estrela
O esquema em estrela é uma abordagem de modelagem madura amplamente adotada por data warehouses relacionais. Ele requer que os modeladores classifiquem suas tabelas de modelo como dimensão ou fato.
Tabelas de dimensões descrevem as entidades de negócios – os itens que você modela. As entidades podem incluir produtos, pessoas, locais e conceitos, incluindo o próprio tempo. A tabela mais consistente que você encontrará em um esquema em estrela é uma tabela de dimensão de data. Uma tabela de dimensões contém uma ou mais colunas de chave, que atuam como um identificador exclusivo, e colunas descritivas.
Tabelas de fatos armazenam observações ou eventos e podem ser ordens de vendas, saldos de ações, taxas de câmbio, temperaturas, etc. Uma tabela de fatos contém colunas chave de dimensão relacionadas a tabelas de dimensões e colunas de medidas numéricas. As colunas de chave de dimensão determinam a dimensionalidade de uma tabela de fatos, enquanto os valores de chave de dimensão determinam a granularidade de uma tabela de fatos. Por exemplo, considere uma tabela de fatos projetada para armazenar os destinos de venda que têm duas colunas de chave de dimensão Data e ProductKey. É fácil entender que a tabela tem duas dimensões. No entanto, a granularidade não pode ser determinada sem considerar os valores de chave de dimensão. Neste exemplo, considere que os valores armazenados na coluna Data são o primeiro dia de cada mês. Nesse caso, a granularidade está no nível do mês-produto.
Em geral, as tabelas de dimensões contêm um número relativamente pequeno de linhas. As tabelas de fatos, por outro lado, podem conter um número muito grande de linhas e continuar crescendo ao longo do tempo.
 


Normalização vs. desnormalização
Para entender alguns conceitos de esquema em estrela descritos neste artigo, é importante conhecer dois termos: normalização e desnormalização.
Normalização é o termo usado para descrever os dados que são armazenados de uma maneira que reduz os dados do redundante. Considere uma tabela de produtos que tem uma coluna de valor de chave exclusiva, como a chave do produto (Product Key) e colunas adicionais que descrevem as características do produto, incluindo o nome do produto, a categoria, a cor e o tamanho. Uma tabela de vendas é considerada normalizada quando armazena apenas chaves, como a chave do produto (Product Key). Na imagem a seguir, observe que apenas a coluna ProductKey registra o produto.
 
No entanto, se a tabela Sales armazenar detalhes do produto além da chave, ela será considerada desnormalizada. Na imagem a seguir, observe que o ProductKey e outras colunas relacionadas ao produto registram o produto.
 
Quando você origina dados de um arquivo de exportação ou extração de dados, é provável que ele represente um conjunto desnormalizado de dados. Nesse caso, use Power Query para transformar e formatar os dados de origem em várias tabelas normalizadas.
conforme descrito neste artigo, você deve se esforçar para desenvolver modelos de dados de Power BI otimizados com tabelas que representam dados normalizados.


Relevância de esquema em estrela para modelos do Power BI
O design de esquema em estrela e muitos conceitos relacionados apresentados neste artigo são altamente relevantes para o desenvolvimento de modelos do Power BI otimizados para desempenho e usabilidade.
Considere que cada elemento do visual de relatório do Power BI gera uma consulta que é enviada para o modelo do Power BI (que o serviço do Power BI chama de conjunto de dados). Essas consultas são usadas para filtrar, agrupar e resumir dados de modelo. Assim, um modelo bem projetado fornece tabelas para filtragem e agrupamento e tabelas para resumo. Esse design se ajusta bem aos princípios de esquema em estrela:
As tabelas de dimensões são compatíveis com filtragem e agrupamento
Tabelas de fatos são compatíveis com resumo
Não há propriedades de tabela que os modeladores definem para configurar o tipo de tabela como dimensão ou fato. Na verdade, isso é determinado pelas relações de modelo. Uma relação de modelo estabelece um caminho de propagação de filtro entre duas tabelas, e é a propriedade de Cardinalidade da relação que determina o tipo de tabela. Uma cardinalidade em uma relação comum é uma relação de um para muitos ou seu inverso, de muitos para um. O lado "um" é sempre uma tabela de tipo de dimensão, enquanto o lado "muitos" é sempre uma tabela de tipo de fato. Para saber mais sobre as relações, confira Relações de modelo no Power BI Desktop.
 
Um design de modelo bem estruturado deve incluir tabelas que sejam do tipo de dimensão ou do tipo de fato. Evite misturar os dois tipos em uma única tabela. Também recomendamos que você busque fornecer o número correto de tabelas com as relações corretas em vigor. Também é importante que as tabelas do tipo fato sempre carreguem dados em um detalhamento consistente.
Por fim, é importante entender que o design ideal de modelos é parte ciência e parte arte. Às vezes, você pode dividir com uma boa orientação quando isso faz sentido.
Há muitos conceitos adicionais relacionados ao design de um esquema em estrela que podem ser aplicados a um modelo do Power BI. Esses conceitos incluem:
Medidas
Chaves alternativas
Dimensões de floco de neve
Dimensões com função múltipla
Dimensões de alteração lenta
Dimensões de lixo eletrônico
Dimensões de degeneração
Tabelas de fatos sem fatos


Medidas
No design de esquema em estrela uma medida é uma coluna de tabela de fatos que armazena valores a serem resumidos.
Em um modelo do Power BI, uma medida tem uma definição diferente, mas semelhante,. É uma fórmula escrita em DAX (Expressões de Análise de Dados) que realiza o resumo. As expressões de medida geralmente aproveitam funções de agregação DAX como SUM, MIN, MAX, AVERAGE etc. para produzir um resultado de valor escalar no momento da consulta (os valores nunca são armazenados no modelo). A expressão de medida pode variar de agregações de coluna simples a fórmulas mais sofisticadas que substituem o contexto de filtro e/ou a propagação de relação. Para obter mais informações, leia o artigo Noções básicas do DAX no Power BI Desktop.
É importante entender que modelos do Power BI dão suporte a um segundo método de obtenção de resumo. Qualquer coluna — e normalmente colunas numéricas — pode ser resumida por um relatório Visual ou p & a. Essas colunas são denominadas medidas implícitas. Elas oferecem uma conveniência para você como desenvolvedor de modelo, já que, em muitas instâncias, você não precisa criar medidas. Por exemplo, a coluna Valor de Vendas das vendas do revendedor da Adventure Works pode ser resumida de várias maneiras (soma, contagem, média, mediana, mín., máx. etc.), sem necessidade de criar uma medida para cada tipo de agregação possível.
 
No entanto, há três motivos convincentes para você criar medidas, mesmo para resumos simples em nível de coluna:
Quando você souber que seus autores de relatório consultarão o modelo usando MDX (Expressões Multidimensionais), o modelo deverá incluir medidas explícitas. Medidas explícitas são definidas usando o DAX. Essa abordagem de design é altamente relevante quando um conjunto de dados do Power BI é consultado usando o MDX, porque ele não pode obter o resumo dos valores de coluna. Notavelmente, o MDX será usado ao executar Analisar no Excel, pois as Tabelas Dinâmicas emitem consultas MDX.
Quando você souber que os autores do relatório criarão relatórios paginados do Power BI usando o designer de consulta do MDX, o modelo deverá incluir medidas explícitas. Somente o designer de consulta MDX dá suporte a agregações de servidor. Portanto, se os autores de relatório precisarem ter medidas avaliadas pelo Power BI (em vez de pelo mecanismo de relatório paginado), eles deverão usar o designer de consulta MDX.
Quando você precisar verificar se os autores de relatórios só podem resumir colunas de maneiras específicas. Por exemplo, a coluna Preço da Unidade de vendas do revendedor (que representa uma taxa por unidade) pode ser resumida, mas somente usando funções de agregação específicas. Ela nunca deve ser somada, mas é apropriado resumi-la usando outras funções de agregação, como mín., máx., média etc. Nessa instância, o modelador pode ocultar a coluna de Preço Unitário e criar medidas para todas as funções de agregação apropriadas.
essa abordagem de design funciona bem para relatórios criados no serviço de Power BI e para & a p A. No entanto, as conexões dinâmicas do Power BI Desktop permitem que os autores de relatório mostrem campos ocultos no painel Campos, o que pode resultar em contornar essa abordagem de design.


Chaves alternativas
Uma chave alternativa é um identificador exclusivo que você adiciona a uma tabela para dar suporte à modelagem de esquema em estrela. Por definição, não é definida nem armazenada nos dados de origem. Normalmente, as chaves substitutas são adicionadas às tabelas de dimensões relacionais do data warehouse para fornecer um identificador exclusivo para cada linha da tabela de dimensões.
As relações de modelo do Power BI baseiam-se em uma única coluna exclusiva em uma tabela, que propaga filtros para uma única coluna em uma tabela diferente. Quando uma tabela de tipo de dimensão em seu modelo não inclui uma única coluna exclusiva, você deve adicionar um identificador exclusivo para se tornar o lado "um" de um relacionamento. No Power BI Desktop, você pode facilmente atender a esse requisito criando uma coluna de índice do Power Query.
 
Você deve mesclar essa consulta com a consulta do lado "muitos" para poder adicionar a coluna de índice a ela também. Ao carregar essas consultas para o modelo, você pode criar uma relação de um para muitos entre as tabelas de modelo.


Dimensões de floco de neve
Uma dimensão de floco de neve é um conjunto de tabelas normalizadas para uma única entidade de negócios. Por exemplo, a Adventure Works classifica produtos por categoria e subcategoria. As categorias são atribuídas a subcategorias e os produtos, por sua vez, são atribuídos a subcategorias. No data warehouse relacional da Adventure Works, a dimensão de produto é normalizada e armazenada em três tabelas relacionadas: DimProductCategory, DimProductSubcategory e DimProduct.
Se você usar sua imaginação, poderá modelar as tabelas normalizadas posicionadas para cima da tabela de fatos, formando um design de floco de neve.
 
No Power BI Desktop, você pode optar por similar um design de dimensão floco de neve (talvez porque seus dados de origem fazem isso) ou integrar (desnormalizar) as tabelas de origem em uma única tabela de modelo. Em geral, os benefícios de uma tabela de modelo única superam os benefícios de várias tabelas de modelo. A decisão mais ideal pode depender dos volumes de dados e dos requisitos de usabilidade para o modelo.
Quando você opta por imitar um design de dimensão floco de neve:
O Power BI carrega mais tabelas, o que é menos eficiente das perspectivas de armazenamento e desempenho. Essas tabelas devem incluir colunas para dar suporte a relações de modelo e isso pode resultar em um tamanho de modelo maior.
Cadeias de propagação de filtro de relação mais longas precisarão ser atravessadas, o que provavelmente será menos eficiente do que os filtros aplicados a uma única tabela.
O painel Campos apresenta mais tabelas de modelo a autores do relatório, o que pode resultar em uma experiência menos intuitiva, especialmente quando as tabelas de dimensões de flocos de neve contêm apenas uma ou duas colunas.
Não é possível criar uma hierarquia que abranja as tabelas.
Quando você opta por integrar em uma única tabela de modelo, também pode definir uma hierarquia que abrange as granularidades mais alta e mais baixa da dimensão. Possivelmente, o armazenamento de dados desnormalizados redundantes pode resultar em maior tamanho de armazenamento de modelo, especialmente para tabelas de dimensões muito grandes.
 


Dimensões de alteração lenta
Uma SCD (dimensão de alteração lenta) é aquela que gerencia adequadamente a alteração dos membros da dimensão ao longo do tempo. Ela se aplica quando os valores da entidade comercial mudam com o tempo e de uma maneira ad hoc. Um bom exemplo de uma dimensão de alteração lenta é uma dimensão de cliente, especificamente, suas colunas de detalhes de contato, como endereço de email e número de telefone. Por outro lado, algumas dimensões são consideradas de alteração rápida quando um atributo de dimensão é alterado com frequência, como o preço de mercado de um estoque. A abordagem de design comum nesses casos é armazenar valores de atributos de alteração rápida em uma medida de tabela de fatos.
A teoria do design do esquema em estrela refere-se a dois tipos comuns de SCD: Tipo 1 e Tipo 2. Uma tabela de tipo de dimensão poderia ser do Tipo 1 ou do Tipo 2, ou dar suporte a ambos os tipos simultaneamente para colunas diferentes.


SCD do Tipo 1
UmaSCDtipo 1sempre reflete os valores mais recentes e quando as alterações nos dados de origem são detectadas, os dados da tabela de dimensões são substituídos. Essa abordagem de design é comum para colunas que armazenam valores suplementares, como o endereço de email ou o número de telefone de um cliente. Quando um endereço de email ou um número de telefone do cliente muda, a tabela de dimensões atualiza a linha de cliente com os novos valores. É como se o cliente sempre tivesse essas informações de contato.
Uma atualização não incremental de uma tabela de tipo de dimensão de modelo do Power BI atinge o resultado de uma SCD do Tipo 1. Ela atualiza os dados da tabela para garantir que os valores mais recentes sejam carregados.


SCD do Tipo 2
Uma SCD do Tipo 2 é compatível com a versão de membros de dimensão. Se o sistema de origem não armazenar versões, normalmente será usado o processo de carregamento de data warehouse que detecta alterações e gerencia adequadamente a alteração em uma tabela de dimensão. Nesse caso, a tabela de dimensões deve usar uma chave substituta para fornecer uma referência exclusiva a uma versão do membro da dimensão. Ele também inclui colunas que definem a validade do intervalo de datas da versão (por exemplo, StartDate e EndDate) e, possivelmente, uma coluna de sinalizador (por exemplo, IsCurrent) para filtrar facilmente por membros da dimensão atual.
Por exemplo, a Adventure Works atribui vendedores a uma região de vendas. Quando um vendedor realoca a região, uma nova versão do vendedor deve ser criada para garantir que os fatos históricos permaneçam associados à região anterior. Para dar suporte à análise histórica precisa das vendas por vendedor, a tabela de dimensões deve armazenar versões de vendedores e suas regiões associadas. A tabela também deve incluir valores de data de início e de término para definir a validade do tempo. Versões atuais podem definir uma data de término vazia (ou 31/12/9999), que indica que a linha é a versão atual. A tabela também deve definir uma chave substituta, pois a chave comercial (neste caso, ID de funcionário) não será exclusiva.
É importante entender que, quando os dados de origem não armazenam versões, você deve usar um sistema intermediário (como um data warehouse) para detectar e armazenar as alterações. O processo de carregamento de tabela deve preservar os dados existentes e detectar alterações. Quando uma alteração é detectada, o processo de carregamento de tabela deve expirar a versão atual. Ele registra essas alterações atualizando o valor EndDate e inserindo uma nova versão com o valor StartDate começando do valor EndDate anterior. Além disso, os fatos relacionados devem usar uma pesquisa baseada em tempo para recuperar o valor da chave de dimensão relevante para a data do fato. Um modelo do Power BI que usa o Power Query não consegue produzir esse resultado. No entanto, ele pode carregar dados de uma tabela de dimensão SCD Tipo 2 pré-carregada.
O modelo do Power BI deve dar suporte à consulta de dados históricos para um membro, independentemente da alteração, e para uma versão do membro, que representa um estado específico do membro no tempo. No contexto da Adventure Works, esse design permite consultar o vendedor, independentemente da região de vendas atribuída ou uma versão específica do vendedor.
Para obter esse requisito, a tabela de tipo de dimensão do modelo do Power BI deve incluir uma coluna para filtrar o vendedor e uma coluna diferente para filtrar uma versão específica do vendedor. É importante que a coluna da versão forneça uma descrição não ambígua, como "Michael Blythe (12/15/2008-06/26/2019)" ou "Michael Blythe (atual)". Também é importante treinar autores e consumidores de relatórios sobre os fundamentos de SCD Tipo 2 e como obter designs de relatórios apropriados aplicando os filtros corretos.
Também é uma boa prática de design incluir uma hierarquia que permita que os visuais façam uma busca detalhada no nível de versão.
 
 


Dimensões com função múltipla
Uma dimensão com função múltipla é uma dimensão que pode filtrar fatos relacionados de forma diferente. Por exemplo, na Adventure Works, a tabela de dimensões de data tem três relações com os fatos de vendas do revendedor. A mesma tabela de dimensão pode ser usada para filtrar os fatos por data do pedido, data de remessa ou data de entrega.
Em um data warehouse, a abordagem de design aceita é definir uma tabela de dimensão de data única. No momento da consulta, a "função" da dimensão de data é estabelecida por qual coluna de fato você usa para unir as tabelas. Por exemplo, quando você analisa as vendas pela data do pedido, a junção da tabela se relaciona à coluna data do pedido de venda do revendedor.
Em um modelo do Power BI, esse design pode ser imitado com a criação de várias relações entre duas tabelas. No exemplo da Adventure Works, as tabelas vendas de data e revendedor teriam três relações. Embora isso seja possível, é importante entender que só pode haver uma relação ativa entre duas tabelas de modelo do Power BI. Todas as relações restantes devem ser definidas como inativas. Ter uma única relação ativa significa que há uma propagação de filtro padrão de data para vendas do revendedor. Neste caso, a relação ativa é definida como o filtro mais comum usado pelos relatórios, que, na Adventure Works, é a relação de data do pedido.
 
A única maneira de usar uma relação inativa é definir uma expressão DAX que usa a função USERELATIONSHIP. Em nosso exemplo, o desenvolvedor do modelo deve criar medidas para habilitar a análise de vendas do revendedor por data de remessa e data de entrega. Esse trabalho pode ser entediante, especialmente quando a tabela de revendedores define muitas medidas. Também cria uma desordem no painel Campos, com uma superabundância de medidas. Também há outras limitações:
Quando os autores de relatório contam com colunas de resumo, em vez de definir medidas, eles não podem obter o resumo para as relações inativas sem escrever uma medida em nível de relatório. As medidas em nível de relatório só podem ser definidas durante a criação de relatórios no Power BI Desktop.
Com apenas um caminho de relação ativo entre a data e as vendas do revendedor, não é possível filtrar simultaneamente as vendas do revendedor por diferentes tipos de datas. Por exemplo, você não pode produzir um visual que plote vendas de datas do pedido por remessa de vendas.
Para superar essas limitações, uma técnica comum de modelagem do Power BI é criar uma tabela de tipo de dimensão para cada instância de execução de função. Normalmente, você cria tabelas de dimensões adicionais, como tabelas calculadas, usando DAX. Usando tabelas calculadas, o modelo pode conter uma tabela de Data, uma tabela de Data de Remessa e uma tabela de Data de Entrega, cada uma com uma relação única e ativa com a sua respectivas colunas de tabela de vendas do revendedor.
 
Essa abordagem de design não exige que você defina várias medidas para diferentes funções de data e permite a filtragem simultânea por diferentes funções de data. No entanto, um preço menor para pagar com essa abordagem de design é que haverá duplicação da tabela de dimensões de data, resultando em um tamanho de armazenamento de modelo maior. Como as tabelas de tipo de dimensão normalmente armazenam menos linhas em relação às tabelas de tipo de fato, isso raramente é uma preocupação.
Observe as boas práticas de design a seguir ao criar tabelas de tipo de dimensão de modelo para cada função:
Verifique se os nomes de coluna são autodescritivos. Embora seja possível ter uma coluna Ano em todas as tabelas de data (os nomes de coluna são exclusivos em sua tabela), ele não é autodescritivo por títulos visuais padrão. Considere renomear colunas em cada tabela de função de dimensão de modo que a tabela Data da Remessa tenha uma coluna chamada Ano da Remessa etc.
Quando relevante, garanta que as descrições da tabela fornecem comentários aos autores do relatório (por meio de dicas de ferramentas do painel Campos) sobre como a propagação do filtro é configurada. Essa clareza é importante quando o modelo contém uma tabela que tem um nome genérico, como Data, que é usado para filtrar muitas tabelas do tipo fato. Caso essa tabela tenha, por exemplo, uma relação ativa com a coluna data do pedido de venda do revendedor, considere fornecer uma descrição de tabela como "Filtra vendas do revendedor por data do pedido".
Para saber mais, confira Diretrizes de relações ativas vs inativas.


Dimensões de lixo eletrônico
Uma dimensão de lixo eletrônico é útil quando há muitas dimensões, especialmente consistindo em alguns atributos (talvez um) e quando esses atributos têm poucos valores. Bons candidatos incluem colunas de status do pedido ou colunas demográficas do cliente (gênero, faixa etária etc.).
O objetivo de design de uma dimensão de lixo eletrônico é consolidar muitas dimensões "pequenas" em uma única dimensão para reduzir o tamanho do armazenamento de modelo e também reduzir a confusão no painel Campos identificando tabelas de modelo.
Em geral, uma tabela de dimensão de lixo eletrônico é o produto cartesiano de todos os membros de atributo de dimensão, com uma coluna de chave substituta. A chave substituta fornece uma referência exclusiva para cada linha na tabela. Você pode criar a dimensão em um data warehouse ou usando o Power Query para criar uma consulta que execute junções de consulta externa completa e, em seguida, adicionar uma chave substituta (coluna de índice).
 
Você carrega essa consulta para o modelo como uma tabela de tipo dimensão. Você também precisa mesclar essa consulta com a consulta de fato, portanto, a coluna de índice é carregada no modelo para dar suporte à criação de uma relação de modelo "um para muitos".


Dimensões de degeneração
Uma dimensão de degeneração refere-se a um atributo da tabela de fatos que é necessário para a filtragem. Na Adventure Works, o número da ordem de venda do revendedor é um bom exemplo. Neste caso, não faz sentido em termos de design de modelo criar uma tabela independente que consiste apenas nesta coluna, pois isso aumentaria o tamanho do armazenamento do modelo e resultaria em desordem do painel de Campos.
No modelo do Power BI, pode ser apropriado adicionar a coluna número de ordem de venda à tabela de tipo de fato para permitir a filtragem ou o agrupamento por número de ordem de venda. Essa é uma exceção à regra anterior de que você não deve misturar tipos de tabela (em geral, as tabelas de modelo devem ser do tipo dimensão ou fato).
 
No entanto, se a tabela de vendas de revendedores da Adventure Works tiver um número de pedido e colunas de número de linha de pedido e elas forem necessárias para filtragem, uma tabela de dimensão de degeneração será um bom design. Para saber mais, confira as Diretrizes de relação um-para-um (Dimensões de degeneração).


Tabelas de fatos sem fatos
Uma tabela de fatos sem fatos não inclui nenhuma coluna de medida. Ela contém apenas chaves de dimensão.
Uma tabela de fatos sem fatos poderia armazenar observações definidas por chaves de dimensão. Por exemplo, em uma determinada data e hora, um cliente específico conectado ao seu site. Você pode definir uma medida para contar as linhas da tabela de fatos inexistentes para executar a análise de quando e quantos clientes fizeram logon.
Um uso mais atraente de uma tabela de fatos sem fatos é armazenar relações entre dimensões, e é a abordagem de design de modelo do Power BI que recomendamos definindo relações de dimensão de muitos para muitos. Em um design de relação de dimensão muitos para muitos, a tabela de fatos sem fatos é conhecida como uma tabela de pontes.
Por exemplo, considere que os vendedores podem ser atribuídos a uma ou mais regiões de vendas. A tabela de pontes seria projetada como uma tabela de fatos informativa que consiste em duas colunas: chave de vendedor e chave de região. Valores duplicados podem ser armazenados em ambas as colunas.
 
Essa abordagem de design de muitos para muitos é bem documentada e pode ser obtida sem uma tabela de pontes. No entanto, a abordagem da tabela de pontes é considerada a melhor prática ao relacionar duas dimensões. Confira mais informações em Diretrizes de relação de muitos para muitos (Relacionar duas tabelas de tipo dimensão).



Tabelas de metadados compartilhados do Azure Synapse Analytics



O Azure Synapse Analytics permite que os diferentes mecanismos computacionais do espaço de trabalho compartilhem bancos de dados e tabelas entre seus pools Apache Spark e o pool SQL sem servidor.


Depois que um banco de dados for criado por um trabalho do Spark, você poderá criar tabelas nele com o Spark que usem Parquet ou CSV como formato de armazenamento. Os nomes das tabelas serão convertidos em letras minúsculas e precisarão ser consultados usando o nome em letras minúsculas. Essas tabelas ficarão imediatamente disponíveis para consulta por qualquer um dos pools do Spark do espaço de trabalho do Azure Synapse. Eles também podem ser usados em qualquer um dos trabalhos do Spark sujeitos a permissões.


As tabelas criadas, gerenciadas e externas do Spark também são disponibilizadas como tabelas externas com o mesmo nome no banco de dados sincronizado correspondente no pool SQL sem servidor. A exposição de uma tabela Spark no SQL fornece mais detalhes sobre a sincronização da tabela.


Como as tabelas são sincronizadas com o pool SQL sem servidor de forma assíncrona, haverá um pequeno atraso até que elas apareçam.


Gerenciar uma tabela criada pelo Spark
Use o Spark para gerenciar bancos de dados criados pelo Spark. Por exemplo, exclua-o por meio de um trabalho de pool do Apache Spark sem servidor e crie tabelas nele do Spark.


Objetos em bancos de dados sincronizados não podem ser modificados do pool SQL sem servidor.


Expor uma tabela Spark no SQL
Tabelas do Spark compartilhadas
O Spark fornece dois tipos de tabelas que o Azure Synapse expõe no SQL automaticamente:


Tabelas gerenciadas


O Spark oferece muitas opções de como armazenar dados em tabelas gerenciadas, como TEXT, CSV, JSON, JDBC, PARQUET, ORC, HIVE, DELTA e LIBSVM. Esses arquivos são normalmente armazenados no warehousediretório onde os dados da tabela gerenciada são armazenados.


Tabelas externas


O Spark também fornece maneiras de criar tabelas externas sobre dados existentes, fornecendo a LOCATIONopção ou usando o formato Hive. Essas tabelas externas podem estar em vários formatos de dados, incluindo Parquet.


Atualmente, o Azure Synapse compartilha apenas tabelas Spark gerenciadas e externas que armazenam seus dados no formato Parquet ou CSV com os mecanismos SQL. As tabelas suportadas por outros formatos não são sincronizadas automaticamente. Você pode sincronizar essas tabelas explicitamente como uma tabela externa em seu próprio banco de dados SQL se o mecanismo SQL oferecer suporte ao formato subjacente da tabela.


Observação


Atualmente, apenas os formatos Parquet e CSV são sincronizados com o pool SQL sem servidor. Os metadados de uma tabela delta do Spark não serão sincronizados com o mecanismo SQL, mesmo que a tabela Delta use Parquet como o formato de armazenamento do instantâneo. As tabelas externas do Spark não estão sendo sincronizadas em bancos de dados de pool SQL dedicados.


Compartilhar tabelas do Spark
As tabelas Spark externas e gerenciadas compartilháveis expostas no mecanismo SQL como tabelas externas com as seguintes propriedades:


A fonte de dados da tabela externa SQL é a fonte de dados que representa a pasta de localização da tabela Spark.
O formato de arquivo da tabela externa SQL é Parquet ou CSV.
A credencial de acesso da tabela externa SQL é de passagem.
Como todos os nomes de tabela Spark são nomes de tabela SQL válidos e todos os nomes de coluna Spark são nomes de coluna SQL válidos, a tabela Spark e os nomes de coluna serão usados para a tabela externa SQL.


As tabelas Spark fornecem tipos de dados diferentes dos mecanismos Synapse SQL. A tabela a seguir mapeia os tipos de dados da tabela Spark para os tipos SQL:


COMPARTILHAR TABELAS DO SPARK
Tipo de dados do Spark Tipo de dados SQL Comentários
LongType, long,bigint bigint Spark : LongType representa números inteiros com sinal de 8 bytes.
SQL : Veja int, bigint, smallint e tinyint .
BooleanType,boolean bit(Parquet), varchar(6)(CSV) Spark : booleano.
SQL : Consulte [/sql/t-sql/data-types/bit-transact-sql).
DecimalType, decimal, dec,numeric decimal Spark : DecimalType representa números decimais assinados de precisão arbitrária. Apoiado internamente por java.math.BigDecimal. Um BigDecimal consiste em um valor inteiro sem escala de precisão arbitrária e uma escala inteira de 32 bits.
SQL : Precisão fixa e números de escala. Quando a precisão máxima é usada, os valores válidos são de - 10^38 +1 a 10^38 - 1. Os sinônimos ISO para decimal são dec e dec(p, s). numérico é funcionalmente idêntico ao decimal. Consulte decimal e numérico .
IntegerType, Integer,int int Spark IntegerType representa números inteiros com sinal de 4 bytes.
SQL : Veja int, bigint, smallint e tinyint .
ByteType, Byte,tinyint smallint Spark : ByteType representa números inteiros assinados de 1 byte [-128 a 127] e ShortType representa números inteiros assinados de 2 bytes [-32768 a 32767].
SQL : Tinyint representa números inteiros com sinal de 1 byte [0, 255] e smallint representa números inteiros com sinal de 2 bytes [-32768, 32767]. Veja int, bigint, smallint e tinyint .
ShortType, Short,smallint smallint O mesmo que acima.
DoubleType,Double float Spark : DoubleType representa números de ponto flutuante de precisão dupla de 8 bytes. SQL : Veja float e real .
FloatType, float,real real Spark : FloatType representa números de ponto flutuante de precisão dupla de 4 bytes. SQL : Veja float e real .
DateType,date date Spark : DateType representa valores que compreendem valores dos campos ano, mês e dia, sem fuso horário.
SQL : Ver data .
TimestampType,timestamp datetime2 Spark : TimestampType representa valores que compreendem valores dos campos ano, mês, dia, hora, minuto e segundo, com o fuso horário local da sessão. O valor do carimbo de data/hora representa um ponto absoluto no tempo.
SQL : Consulte datetime2 .
char char
StringType, String,varchar Varchar(n) Spark : StringType representa valores de cadeia de caracteres. VarcharType(n) é uma variante de StringType que tem uma limitação de comprimento. A gravação de dados falhará se a string de entrada exceder o limite de comprimento. Este tipo só pode ser usado em esquema de tabela, não em funções/operadores.
CharType(n) é uma variante de VarcharType(n) que tem comprimento fixo. A coluna de leitura do tipo CharType(n) sempre retorna valores de string de comprimento n. A comparação de colunas CharType(n) preencherá a mais curta com a mais longa.
SQL : Se houver um comprimento fornecido pelo Spark, n em varchar(n)será definido para esse comprimento. Se for uma coluna particionada, n pode ser no máximo 2048. Caso contrário, será varchar(max) . Veja char e varchar .
Use-o com agrupamento Latin1_General_100_BIN2_UTF8.
BinaryType,binary varbinary(n) SQL : Se houver um comprimento fornecido pelo Spark, nem Varbinary(n) será definido para esse comprimento. Se for uma coluna particionada, n pode ser no máximo 2048. Caso contrário, será Varbinary(max) . Veja binário e varbinary .
array, map,struct varchar(max) SQL : Serializa em JSON com agrupamento Latin1_General_100_BIN2_UTF8. Consulte Dados JSON .
Observação


O agrupamento de nível de banco de dados é Latin1_General_100_CI_AS_SC_UTF8.


Modelo de segurança
Os bancos de dados e tabelas do Spark, bem como suas representações sincronizadas no mecanismo SQL, serão protegidos no nível de armazenamento subjacente. Como eles não têm permissões nos próprios objetos, os objetos podem ser vistos no explorador de objetos.


A entidade de segurança que cria uma tabela gerenciada é considerada a proprietária dessa tabela e tem todos os direitos sobre a tabela, bem como sobre as pastas e arquivos subjacentes. Além disso, o proprietário do banco de dados se tornará automaticamente co-proprietário da tabela.


Se você criar uma tabela externa Spark ou SQL com passagem de autenticação, os dados serão protegidos apenas nos níveis de pasta e arquivo. Se alguém consultar esse tipo de tabela externa, a identidade de segurança do remetente da consulta será passada para o sistema de arquivos, que verificará os direitos de acesso.


Para obter mais informações sobre como definir permissões nas pastas e arquivos, consulte banco de dados compartilhado do Azure Synapse Analytics .


Preservar metadados e ACLs usando atividade de cópia no Azure Data Factory ou Synapse Analytics
 
 
Ao usar a atividade de cópia de pipelines do Azure Data Factory ou do Synapse Analytics para copiar dados da origem para o coletor, nos cenários a seguir, você também pode preservar os metadados e as ACLs.
Preservar metadados para migração de lagos
Ao migrar dados de um data lake para outro, incluindo Amazon S3 , Azure Blob , Azure Data Lake Storage Gen2 e Azure Files , você pode optar por preservar os metadados do arquivo junto com os dados.
A atividade de cópia suporta a preservação dos seguintes atributos durante a cópia de dados:
* Todos os metadados especificados pelo cliente
* E as cinco propriedades de sistema integradas do armazenamento de dados a seguir : contentType, contentLanguage(exceto para Amazon S3), contentEncoding, contentDisposition, cacheControl.
Lidar com diferenças nos metadados: o Amazon S3 e o Armazenamento do Azure permitem diferentes conjuntos de caracteres nas chaves dos metadados especificados pelo cliente. Quando você opta por preservar os metadados usando a atividade de cópia, o serviço substitui automaticamente os caracteres inválidos por '_'.
Quando você copia arquivos como estão do Amazon S3/Azure Data Lake Storage Gen2/Azure Blob storage/Azure Files para Azure Data Lake Storage Gen2/Azure Blob storage/Azure Files com formato binário, você pode encontrar a opção Preservar na atividade de cópia > guia Configurações para autoria de atividade ou a página Configurações na Ferramenta Copiar Dados.
Preservar ACLs do Data Lake Storage Gen1/Gen2 para Gen2
Ao atualizar do Azure Data Lake Storage Gen1 para Gen2 ou copiar dados entre ADLS Gen2, você pode optar por preservar as listas de controle de acesso (ACLs) POSIX junto com os arquivos de dados. Para obter mais informações sobre controle de acesso, confira Controle de acesso no Azure Data Lake Storage Gen1 e Controle de acesso no Azure Data Lake Storage Gen2 .
A atividade de cópia dá suporte à preservação dos seguintes tipos de ACLs durante a cópia de dados. Você pode selecionar um ou mais tipos:
* ACL : Copie e preserve listas de controle de acesso POSIX em arquivos e diretórios. Ele copia todas as ACLs existentes da origem para o coletor.
* Proprietário : Copie e preserve o usuário proprietário de arquivos e diretórios. O acesso de superusuário ao coletor Data Lake Storage Gen2 é necessário.
* Grupo : Copie e preserve o grupo proprietário de arquivos e diretórios. É necessário o acesso de superusuário ao coletor Data Lake Storage Gen2 ou o usuário proprietário (se o usuário proprietário também for membro do grupo de destino).
Se você especificar para copiar de uma pasta, o serviço replicará as ACLs dessa pasta e os arquivos e diretórios nela, se recursiveestiver definido como true. Se você especificar para copiar de um único arquivo, as ACLs nesse arquivo serão copiadas.
 Observação
Quando você usa a atividade de cópia para preservar as ACLs do Data Lake Storage Gen1/Gen2 para Gen2, as ACLs existentes na pasta/arquivos correspondentes do coletor Gen2 serão substituídas.
 Importante
Ao optar por preservar as ACLs, certifique-se de conceder permissões altas o suficiente para que o serviço funcione em sua conta do coletor Data Lake Storage Gen2. Por exemplo, use a autenticação de chave de conta ou atribua a função Proprietário de Dados de Blob de Armazenamento à entidade de serviço ou identidade gerenciada.
Ao configurar a origem como Data Lake Storage Gen1/Gen2 com formato binário ou a opção de cópia binária e o coletor como Data Lake Storage Gen2 com formato binário ou a opção de cópia binária, você pode encontrar a opção Preservar na página Configurações na ferramenta Copiar dados ou na guia Copiar atividade > Configurações para autoria de atividade.
Usando IDENTITY para criar chaves substitutas usando o pool SQL dedicado no AzureSynapse Analytics
 
Neste artigo, você encontrará recomendações e exemplos para usar a propriedade IDENTITY para criar chaves substitutas em tabelas no pool SQL dedicado.
O que é uma chave substituta
Uma chave substituta em uma tabela é uma coluna com um identificador exclusivo para cada linha. A chave não é gerada a partir dos dados da tabela. Os modeladores de dados gostam de criar chaves substitutas em suas tabelas quando projetam modelos de data warehouse. Você pode usar a propriedade IDENTITY para atingir esse objetivo de forma simples e eficaz sem afetar o desempenho da carga.
Criando uma tabela com uma coluna IDENTITY
A propriedade IDENTITY foi projetada para escalar horizontalmente em todas as distribuições no pool SQL dedicado sem afetar o desempenho da carga. Portanto, a implementação do IDENTITY está orientada para atingir esses objetivos.
Alocação de valores
A propriedade IDENTITY não garante a ordem em que os valores substitutos são alocados devido à arquitetura distribuída do data warehouse. A propriedade IDENTITY foi projetada para escalar horizontalmente em todas as distribuições no pool SQL dedicado sem afetar o desempenho da carga.
Dados distorcidos
O intervalo de valores para o tipo de dados é distribuído uniformemente pelas distribuições. Se uma tabela distribuída sofre de dados distorcidos, o intervalo de valores disponíveis para o tipo de dados pode ser esgotado prematuramente. Por exemplo, se todos os dados terminarem em uma única distribuição, então efetivamente a tabela terá acesso a apenas um sexagésimo dos valores do tipo de dados. Por esse motivo, a propriedade IDENTITY é limitada apenas a tipos de dados INT.BIGINT
SELECIONE.. PARA
Quando uma coluna IDENTITY existente é selecionada em uma nova tabela, a nova coluna herda a propriedade IDENTITY, a menos que uma das seguintes condições seja verdadeira:
* A instrução SELECT contém uma junção.
* Várias instruções SELECT são unidas usando UNION.
* A coluna IDENTITY é listada mais de uma vez na lista SELECT.
* A coluna IDENTITY faz parte de uma expressão.
Se qualquer uma dessas condições for verdadeira, a coluna será criada NOT NULL em vez de herdar a propriedade IDENTITY.
CRIAR TABELA COMO SELECIONAR
CREATE TABLE AS SELECT (CTAS) segue o mesmo comportamento do SQL Server documentado para SELECT..INTO. No entanto, você não pode especificar uma propriedade IDENTITY na definição de coluna da CREATE TABLEparte da instrução. Você também não pode usar a função IDENTIDADE na SELECTparte do CTAS. Para preencher uma tabela, você precisa usar CREATE TABLEpara definir a tabela seguido de INSERT..SELECTpara preenchê-la.
Inserindo valores explicitamente em uma coluna IDENTITY
O pool SQL dedicado dá suporte à SET IDENTITY_INSERT <your table> ON|OFFsintaxe. Você pode usar essa sintaxe para inserir valores explicitamente na coluna IDENTITY.
Muitos modeladores de dados gostam de usar valores negativos predefinidos para determinadas linhas em suas dimensões. Um exemplo é a linha -1 ou "membro desconhecido".
Carregando dados
A presença da propriedade IDENTITY tem algumas implicações para seu código de carregamento de dados. Esta seção destaca alguns padrões básicos para carregar dados em tabelas usando IDENTITY.
Para carregar dados em uma tabela e gerar uma chave substituta usando IDENTITY, crie a tabela e use INSERT..SELECT ou INSERT..VALUES para realizar o carregamento.
Visualizações do sistema
Você pode usar a exibição de catálogo sys.identity_columns para identificar uma coluna que tenha a propriedade IDENTITY.
Para ajudar você a entender melhor o esquema do banco de dados, este exemplo mostra como integrar sys.identity_column` com outras
Limitações
A propriedade IDENTITY não pode ser usada:
* Quando o tipo de dados da coluna não é INT ou BIGINT
* Quando a coluna também é a chave de distribuição
* Quando a mesa é uma mesa externa
As seguintes funções relacionadas não são compatíveis com o pool SQL dedicado:
* IDENTIDADE()
* @@IDENTIDADE
* SCOPE_IDENTITY
* IDENT_CURRENT
* IDENT_INCR
* IDENT_SEED
Tarefas comuns
Esta seção fornece alguns códigos de exemplo que você pode usar para executar tarefas comuns ao trabalhar com colunas IDENTITY.
Criptografia de dados transparente do Azure SQL com chave gerenciada pelo cliente
A criptografia de dados transparentes (TDE) do SQL do Azure com chave gerenciada pelo cliente permite o cenário Bring Your Own Key (BYOK) para proteção de dados em repouso e permite que as organizações implementem a separação de funções no gerenciamento de chaves e dados. Com o TDE gerenciado pelo cliente, o cliente é responsável e tem controle total do gerenciamento do ciclo de vida de chaves (criação de chaves, upload, rotação, exclusão), permissões de uso de chaves e auditoria de operações em chaves.
Nesse cenário, a chave usada para criptografia da Chave de Criptografia de Banco de Dados (DEK), chamada de protetor TDE, é uma chave assimétrica gerenciada pelo cliente armazenada em um Azure Key Vault (AKV) de propriedade do cliente e gerenciado pelo cliente , um sistema externo de gerenciamento de chaves. O Key Vault é um armazenamento seguro altamente disponível e escalável para chaves criptográficas RSA, opcionalmente suportado por módulos de segurança de hardware (HSMs) validados FIPS 140-2 Nível 2. Não permite o acesso direto a uma chave armazenada, mas disponibiliza serviços de encriptação/desencriptação utilizando a chave às entidades autorizadas. A chave pode ser gerada pelo cofre de chaves, importada ou transferida para o cofre de chaves de um dispositivo HSM local .
Para o Banco de Dados SQL do Azure e o Azure Synapse Analytics, o protetor TDE é definido no nível do servidor e é herdado por todos os bancos de dados criptografados associados a esse servidor. Para a Instância Gerenciada de SQL do Azure, o protetor TDE é definido no nível da instância e é herdado por todos os bancos de dados criptografados nessa instância. O termo servidor refere-se a um servidor no Banco de Dados SQL e no Azure Synapse e a uma instância gerenciada na Instância Gerenciada do SQL em todo este documento, a menos que indicado de forma diferente.
 Observação
Este artigo se aplica ao Banco de Dados SQL do Azure, Instância Gerenciada de SQL do Azure e Azure Synapse Analytics (pools SQL dedicados (anteriormente SQL DW)). Para obter documentação sobre criptografia de dados transparente para pools SQL dedicados dentro de espaços de trabalho Synapse, confira Criptografia do Azure Synapse Analytics .
 Importante
Para aqueles que usam TDE gerenciado por serviço que gostariam de começar a usar TDE gerenciado pelo cliente, os dados permanecem criptografados durante o processo de comutação e não há tempo de inatividade nem recriptografia dos arquivos de banco de dados. A mudança de uma chave gerenciada pelo serviço para uma chave gerenciada pelo cliente requer apenas a recriptografia da DEK, que é uma operação rápida e online.
 Observação
Para fornecer aos clientes do Azure SQL duas camadas de criptografia de dados em repouso, a criptografia de infraestrutura (usando o algoritmo de criptografia AES-256) com chaves gerenciadas pela plataforma está sendo lançada. Isso fornece uma camada adicional de criptografia em repouso junto com TDE com chaves gerenciadas pelo cliente, que já está disponível. Para o Banco de Dados SQL do Azure e a Instância Gerenciada, todos os bancos de dados, incluindo o banco de dados mestre e outros bancos de dados do sistema, serão criptografados quando a criptografia de infraestrutura estiver ativada. Neste momento, os clientes devem solicitar acesso a esse recurso. Se você estiver interessado nesse recurso, entre em contato com AzureSQLDoubleEncryptionAtRest@service.microsoft.com.
Benefícios do TDE gerenciado pelo cliente
A TDE gerenciada pelo cliente oferece os seguintes benefícios ao cliente:
* Controle total e granular sobre o uso e gerenciamento do protetor TDE;

* Transparência do uso do protetor TDE;

* Capacidade de implementar a separação de funções na gestão de chaves e dados dentro da organização;

* O administrador do Key Vault pode revogar as permissões de acesso de chave para tornar o banco de dados criptografado inacessível;

* Gestão central de chaves em AKV;

* Maior confiança de seus clientes finais, uma vez que o AKV foi projetado para que a Microsoft não possa ver nem extrair chaves de criptografia;

Como funciona o TDE gerenciado pelo cliente
Para que o servidor SQL do Azure use o protetor TDE armazenado no AKV para criptografia do DEK, o administrador do cofre de chaves precisa conceder os seguintes direitos de acesso ao servidor usando sua identidade exclusiva do Azure Active Directory (Azure AD):
get - para recuperar a parte pública e as propriedades da chave no Key Vault
wrapKey - para poder proteger (criptografar) DEK
unwrapKey - para poder desproteger (descriptografar) DEK
O administrador do cofre de chaves também pode habilitar o registro de eventos de auditoria do cofre de chaves , para que possam ser auditados posteriormente.
Quando o servidor é configurado para usar um protetor TDE do AKV, o servidor envia o DEK de cada banco de dados habilitado para TDE para o cofre de chaves para criptografia. O cofre de chaves retorna o DEK criptografado, que é armazenado no banco de dados do usuário.
Quando necessário, o servidor envia a DEK protegida para o cofre de chaves para descriptografia.
Os auditores podem usar o Azure Monitor para revisar os logs de AuditEvent do cofre de chaves, se o log estiver habilitado.
Observação
Pode levar cerca de 10 minutos para que as alterações de permissão entrem em vigor no cofre de chaves. Isso inclui revogar as permissões de acesso ao protetor TDE no AKV, e os usuários dentro desse período ainda podem ter permissões de acesso.
Requisitos para configurar o TDE gerenciado pelo cliente
Requisitos para configurar o AKV
Cofre de chaves e banco de dados SQL/instância gerenciada devem pertencer ao mesmo locatário do Azure Active Directory. Cofre de chaves entre locatários e interações de servidor não são compatíveis. Para mover recursos posteriormente, TDE com AKV terá que ser reconfigurado. Saiba mais sobre como mover recursos .
Os recursos de proteção de exclusão reversível e de limpeza devem ser habilitados no cofre de chaves para proteger contra perda de dados devido à exclusão acidental de chave (ou cofre de chaves).
Os recursos excluídos por software são retidos por 90 dias, a menos que sejam recuperados ou eliminados pelo cliente. As ações de recuperação e limpeza têm suas próprias permissões associadas a uma política de acesso ao cofre de chaves. O recurso de exclusão reversível pode ser habilitado usando o portal do Azure, PowerShell ou CLI do Azure .
A proteção contra limpeza pode ser ativada usando a CLI do Azure ou o PowerShell . Quando a proteção contra limpeza está habilitada, um cofre ou um objeto no estado excluído não pode ser limpo até que o período de retenção tenha passado. O período de retenção padrão é de 90 dias, mas é configurável de 7 a 90 dias por meio do portal do Azure.
Importante
A proteção de exclusão reversível e de limpeza deve ser habilitada no(s) cofre(s) de chaves para servidores que estão sendo configurados com TDE gerenciado pelo cliente, bem como servidores existentes usando TDE gerenciado pelo cliente.
Conceda ao servidor ou à instância gerenciada acesso ao cofre de chaves ( get , wrapKey , unwrapKey ) usando sua identidade do Azure Active Directory. A identidade do servidor pode ser uma identidade gerenciada atribuída pelo sistema ou uma identidade gerenciada atribuída pelo usuário atribuída ao servidor. Ao usar o portal do Azure, a identidade do Azure AD é criada automaticamente quando o servidor é criado. Ao usar o PowerShell ou a CLI do Azure, a identidade do Azure AD deve ser criada explicitamente e verificada. Consulte Configurar TDE com BYOK e Configurar TDE com BYOK para SQL Managed Instance para obter instruções detalhadas passo a passo ao usar o PowerShell.
Dependendo do modelo de permissão do cofre de chaves (política de acesso ou RBAC do Azure), o acesso ao cofre de chaves pode ser concedido criando uma política de acesso no cofre de chaves ou criando uma nova atribuição de função de RBAC do Azure com a função Key Vault Crypto Service Usuário de criptografia .
Ao usar firewall com AKV, você deve habilitar a opção Permitir que serviços confiáveis da Microsoft ignorem o firewall .
Requisitos para configurar o protetor TDE
O protetor TDE só pode ser uma chave assimétrica, RSA ou RSA HSM. Os comprimentos de chave suportados são 2048 bytes e 3072 bytes.
A data de ativação da chave (se definida) deve ser uma data e hora no passado. A data de expiração (se definida) deve ser uma data e hora futuras.
A chave deve estar no estado Ativado .
Se você estiver importando uma chave existente para o cofre de chaves, certifique-se de fornecê-la nos formatos de arquivo suportados ( .pfx, .byok, ou .backup).
Observação
O SQL do Azure agora dá suporte ao uso de uma chave RSA armazenada em um HSM Gerenciado como TDE Protector. O Azure Key Vault Managed HSM é um serviço de nuvem totalmente gerenciado, altamente disponível, de locatário único e compatível com padrões que permite proteger chaves criptográficas para seus aplicativos de nuvem, usando HSMs validados por FIPS 140-2 Nível 3. Saiba mais sobre HSMs gerenciados .
Recomendações ao configurar o TDE gerenciado pelo cliente
Recomendações ao configurar o AKV
Associe no máximo 500 bancos de dados de uso geral ou 200 bancos de dados críticos aos negócios com um cofre de chaves em uma única assinatura para garantir alta disponibilidade quando o servidor acessar o protetor TDE no cofre de chaves. Esses números são baseados na experiência e documentados nos limites de serviço do cofre de chaves . A intenção aqui é evitar problemas após o failover do servidor, pois ele acionará tantas operações de chave no cofre quantos os bancos de dados nesse servidor.
Defina um bloqueio de recurso no cofre de chaves para controlar quem pode excluir esse recurso crítico e evitar a exclusão acidental ou não autorizada. Saiba mais sobre bloqueios de recursos .
Habilite a auditoria e a geração de relatórios em todas as chaves de criptografia: o cofre de chaves fornece logs fáceis de injetar em outras informações de segurança e ferramentas de gerenciamento de eventos. O Operations Management Suite Log Analytics é um exemplo de um serviço que já está integrado.
Vincule cada servidor a dois cofres de chaves que residam em regiões diferentes e mantenham o mesmo material de chave, para garantir alta disponibilidade de bancos de dados criptografados. Marque a chave de um dos cofres de chaves como o protetor TDE. O sistema mudará automaticamente para o cofre de chaves na segunda região com o mesmo material de chave, se houver uma interrupção afetando o cofre de chaves na primeira região.
Observação
Para permitir maior flexibilidade na configuração do TDE gerenciado pelo cliente, o servidor do Banco de Dados SQL do Azure e a Instância Gerenciada em uma região agora podem ser vinculados ao cofre de chaves em qualquer outra região. O servidor e o cofre de chaves não precisam estar localizados na mesma região.
Recomendações ao configurar o protetor TDE
Mantenha uma cópia do protetor TDE em um local seguro ou caução para o serviço de custódia.
Se a chave for gerada no cofre de chaves, crie um backup de chave antes de usar a chave no AKV pela primeira vez. O backup pode ser restaurado apenas para um Azure Key Vault. Saiba mais sobre o comando Backup-AzKeyVaultKey .
Crie um novo backup sempre que forem feitas alterações na chave (por exemplo, atributos de chave, tags, ACLs).
Mantenha as versões anteriores da chave no cofre de chaves ao girar as chaves, para que os backups de banco de dados mais antigos possam ser restaurados. Quando o protetor TDE é alterado para um banco de dados, os backups antigos do banco de dados não são atualizados para usar o protetor TDE mais recente. No momento da restauração, cada backup precisa do protetor TDE com o qual foi criptografado no momento da criação. As rotações de chave podem ser executadas seguindo as instruções em Girar o protetor de criptografia de dados transparente usando o PowerShell .
Mantenha todas as chaves usadas anteriormente no AKV mesmo depois de alternar para chaves gerenciadas pelo serviço. Ele garante que os backups de banco de dados possam ser restaurados com os protetores TDE armazenados no AKV. Os protetores TDE criados com o Azure Key Vault devem ser mantidos até que todos os backups armazenados restantes tenham sido criados com chaves gerenciadas pelo serviço. Faça cópias de backup recuperáveis dessas chaves usando Backup-AzKeyVaultKey .
Para remover uma chave potencialmente comprometida durante um incidente de segurança sem o risco de perda de dados, siga as etapas de Remover uma chave potencialmente comprometida .
Protetor TDE inacessível
Quando o TDE é configurado para usar uma chave gerenciada pelo cliente, o acesso contínuo ao protetor TDE é necessário para que o banco de dados permaneça online. Se o servidor perder o acesso ao protetor TDE gerenciado pelo cliente no AKV, em até 10 minutos um banco de dados começará a negar todas as conexões com a mensagem de erro correspondente e mudará seu estado para Inacessível . A única ação permitida em um banco de dados no estado Inacessível é excluí-lo.
Observação
Se o banco de dados estiver inacessível devido a uma interrupção de rede intermitente, não será necessária nenhuma ação e os bancos de dados voltarão a ficar online automaticamente.
Depois que o acesso à chave é restaurado, colocar o banco de dados online novamente requer tempo e etapas extras, que podem variar de acordo com o tempo decorrido sem acesso à chave e o tamanho dos dados no banco de dados:
Se o acesso à chave for restaurado em 30 minutos, o banco de dados será recuperado automaticamente na próxima hora.
Se o acesso à chave for restaurado após mais de 30 minutos, a recuperação automática não será possível e a recuperação do banco de dados exigirá etapas extras no portal e poderá levar um tempo significativo dependendo do tamanho do banco de dados. Quando o banco de dados estiver on-line novamente, as configurações de nível de servidor definidas anteriormente, como configuração de grupo de failover , histórico de restauração pontual e tags , serão perdidas . Portanto, é recomendável implementar um sistema de notificação que permita identificar e resolver os problemas de acesso de chave subjacentes em 30 minutos.
Abaixo está uma visão das etapas extras necessárias no portal para colocar um banco de dados inacessível novamente online.
Revogação acidental de acesso do protetor TDE
Pode acontecer que alguém com direitos de acesso suficientes ao cofre de chaves desative acidentalmente o acesso do servidor à chave:
   * revogando as permissões get , wrapKey , unwrapKey do cofre de chaves do servidor

   * deletando a chave

   * excluindo o cofre de chaves

   * alterando as regras de firewall do cofre de chaves

   * excluindo a identidade gerenciada do servidor no Azure Active Directory

Saiba mais sobre as causas comuns para o banco de dados se tornar inacessível .
Monitoramento do TDE gerenciado pelo cliente
Para monitorar o estado do banco de dados e habilitar alertas para perda de acesso ao protetor TDE, configure os seguintes recursos do Azure:
      * Integridade dos Recursos do Azure . Um banco de dados inacessível que perdeu o acesso ao protetor TDE será exibido como "Indisponível" depois que a primeira conexão com o banco de dados for negada.
      * Log de atividades quando o acesso ao protetor TDE no cofre de chaves gerenciado pelo cliente falha, as entradas são adicionadas ao log de atividades. A criação de alertas para esses eventos permitirá que você restabeleça o acesso o mais rápido possível.
      * Os Grupos de Ação podem ser definidos para enviar notificações e alertas com base em suas preferências, por exemplo, Email/SMS/Push/Voz, Aplicativo Lógico, Webhook, ITSM ou Automation Runbook.
Backup e restauração de banco de dados com TDE gerenciado pelo cliente
Depois que um banco de dados é criptografado com TDE usando uma chave do Key Vault, todos os backups recém-gerados também são criptografados com o mesmo protetor TDE. Quando o protetor TDE é alterado, os backups antigos do banco de dados não são atualizados para usar o protetor TDE mais recente.
Para restaurar um backup criptografado com um protetor TDE do Key Vault, certifique-se de que o material da chave esteja disponível para o servidor de destino. Portanto, recomendamos que você mantenha todas as versões antigas do protetor TDE no cofre de chaves, para que os backups de banco de dados possam ser restaurados.
 Importante
Em nenhum momento pode haver mais de um protetor TDE definido para um servidor. É a chave marcada com "Tornar a chave o protetor TDE padrão" na folha do portal do Azure. No entanto, várias chaves adicionais podem ser vinculadas a um servidor sem marcá-las como um protetor TDE. Essas chaves não são usadas para proteger a DEK, mas podem ser usadas durante a restauração de um backup, se o arquivo de backup for criptografado com a chave com a impressão digital correspondente.
Se a chave necessária para restaurar um backup não estiver mais disponível para o servidor de destino, a seguinte mensagem de erro será retornada na tentativa de restauração: "O servidor de destino <Servername>não tem acesso a todos os URIs AKV criados entre <Timestamp #1> e < Timestamp #2>. Repita a operação após restaurar todos os URIs do AKV."
Para atenuá-lo, execute o cmdlet Get-AzSqlServerKeyVaultKey para o servidor de destino ou Get-AzSqlInstanceKeyVaultKey para a instância gerenciada de destino para retornar a lista de chaves disponíveis e identificar as ausentes. Para garantir que todos os backups possam ser restaurados, certifique-se de que o servidor de destino da restauração tenha acesso a todas as chaves necessárias. Essas chaves não precisam ser marcadas como protetoras TDE.
Para saber mais sobre a recuperação de backup para o Banco de Dados SQL, consulte Recuperar um banco de dados no Banco de Dados SQL . Para saber mais sobre a recuperação de backup para o pool SQL dedicado no Azure Synapse Analytics, confira Recuperar um pool SQL dedicado . Para backup/restauração nativo do SQL Server com SQL Managed Instance, consulte Quickstart: Restore a database to SQL Managed Instance
Outra consideração para arquivos de log: Os arquivos de log de backup permanecem criptografados com o protetor TDE original, mesmo que ele tenha sido girado e o banco de dados agora esteja usando um novo protetor TDE. No momento da restauração, ambas as chaves serão necessárias para restaurar o banco de dados. Se o arquivo de log estiver usando um protetor TDE armazenado no Azure Key Vault, essa chave será necessária no momento da restauração, mesmo que o banco de dados tenha sido alterado para usar TDE gerenciado pelo serviço nesse meio tempo.
Alta disponibilidade com TDE gerenciado pelo cliente
Mesmo nos casos em que não há redundância geográfica configurada para o servidor, é altamente recomendável configurar o servidor para usar dois cofres de chaves diferentes em duas regiões diferentes com o mesmo material de chave. A chave no cofre de chaves secundária na outra região não deve ser marcada como protetora TDE e nem é permitida. Se houver uma interrupção afetando o cofre de chave primária, e somente então, o sistema alternará automaticamente para a outra chave vinculada com a mesma impressão digital no cofre de chave secundária, se existir. Observe que essa troca não acontecerá se o protetor TDE estiver inacessível devido a direitos de acesso revogados ou porque a chave ou o cofre de chaves foi excluído, pois pode indicar que o cliente intencionalmente queria restringir o acesso do servidor à chave.
Como alternativa, isso pode ser feito gerando a chave usando o cofre de chaves primárias em uma região e clonando a chave em um cofre de chaves em uma região diferente do Azure. Use o cmdlet Backup-AzKeyVaultKey para recuperar a chave em formato criptografado do cofre de chave primária e, em seguida, use o cmdlet Restore-AzKeyVaultKey e especifique um cofre de chave na segunda região para clonar a chave. Como alternativa, use o portal do Azure para fazer backup e restaurar a chave. A operação de backup/restauração de chaves só é permitida entre cofres de chaves dentro da mesma assinatura do Azure e geografia do Azure .
Geo-DR e TDE gerenciado pelo cliente
Em cenários de replicação geográfica ativa e grupos de failover , os servidores primário e secundário envolvidos podem ser vinculados ao mesmo cofre de chaves (em qualquer região) ou a cofres de chaves separados. Se cofres de chave separados estiverem vinculados aos servidores primário e secundário, o cliente é responsável por manter o material de chave nos cofres de chave consistente, para que o geosecundário esteja sincronizado e possa assumir o controle usando a mesma chave de seu cofre de chave vinculado, se primário torna-se inacessível devido a uma interrupção na região e um failover é acionado. Até quatro secundários podem ser configurados e não há suporte para encadeamento (secundário de secundários).
Para evitar problemas ao estabelecer ou durante a replicação geográfica devido ao material de chave incompleto, é importante seguir estas regras ao configurar o TDE gerenciado pelo cliente (se forem usados cofres de chave separados para os servidores primário e secundário):
Todos os cofres de chaves envolvidos devem ter as mesmas propriedades e os mesmos direitos de acesso para os respectivos servidores.
Todos os cofres de chaves envolvidos devem conter material de chave idêntico. Aplica-se não apenas ao protetor TDE atual, mas a todos os protetores TDE anteriores que podem ser usados nos arquivos de backup.
Tanto a configuração inicial quanto a rotação do protetor TDE devem ser feitas primeiro no secundário e depois no primário.
Para testar um failover, siga as etapas em Visão geral da replicação geográfica ativa . O teste de failover deve ser feito regularmente para validar se o Banco de Dados SQL manteve a permissão de acesso a ambos os cofres de chaves.
O servidor do Banco de Dados SQL do Azure e a Instância Gerenciada em uma região agora podem ser vinculados ao cofre de chaves em qualquer outra região. O servidor e o cofre de chaves não precisam estar localizados na mesma região. Com isso, por simplicidade, os servidores primário e secundário podem ser conectados ao mesmo cofre de chaves (em qualquer região). Isso ajudará a evitar cenários em que o material da chave pode estar fora de sincronia se forem usados cofres de chave separados para ambos os servidores. O Azure Key Vault tem várias camadas de redundância para garantir que suas chaves e cofres de chaves permaneçam disponíveis em caso de falhas de serviço ou região. Disponibilidade e redundância do Azure Key Vault
Política do Azure para TDE gerenciado pelo cliente
A Política do Azure pode ser usada para impor o TDE gerenciado pelo cliente durante a criação ou atualização de um servidor do Banco de Dados SQL do Azure ou uma Instância Gerenciada do SQL do Azure. Com essa política em vigor, qualquer tentativa de criar ou atualizar um servidor lógico no Azure ou instância gerenciada falhará se não estiver configurado com uma chave gerenciada pelo cliente. A Política do Azure pode ser aplicada a toda a assinatura do Azure ou apenas em um grupo de recursos.
Para obter mais informações sobre o Azure Policy, consulte o que é o Azure Policy? e estrutura de definição do Azure Policy .
As duas políticas internas a seguir têm suporte para TDE gerenciada pelo cliente no Azure Policy:
O SQL Server deve usar chaves gerenciadas pelo cliente para criptografar dados em repouso
As instâncias gerenciadas do SQL devem usar chaves gerenciadas pelo cliente para criptografar dados em repouso
A política de TDE gerenciada pelo cliente pode ser gerenciada acessando o portal do Azure e pesquisando o serviço de política . Em Definições , pesquise a chave gerenciada pelo cliente.
Existem três efeitos para essas políticas:
Auditoria - A configuração padrão e capturará apenas um relatório de auditoria nos logs de atividades do Azure Policy
Negar - Impede a criação ou atualização de servidor lógico ou instância gerenciada sem uma chave gerenciada pelo cliente configurada
Desativado - Desativará a política e não restringirá os usuários de criar ou atualizar um servidor lógico ou instância gerenciada sem o TDE gerenciado pelo cliente habilitado
Se a Política do Azure para TDE gerenciada pelo cliente estiver definida como Negar , a criação do servidor lógico SQL do Azure ou da instância gerenciada falhará. Os detalhes dessa falha serão registrados no log de atividades do grupo de recursos.
Importante
Versões anteriores de políticas internas para TDE gerenciada pelo cliente contendo o AuditIfNotExistefeito foram preteridas. As atribuições de política existentes usando as políticas obsoletas não são afetadas e continuarão a funcionar como antes.
 
Proteja um pool SQL dedicado (anteriormente SQL DW) no Azure Synapse Analytics
 
Este artigo irá orientá-lo no básico sobre como proteger seu pool SQL dedicado (anteriormente SQL DW). Em particular, este artigo apresenta recursos para limitar o acesso, proteger dados e monitorar atividades usando o pool SQL dedicado (anteriormente SQL DW).
Segurança da conexão
Segurança de conexão refere-se a como você restringe e protege conexões com seu banco de dados usando regras de firewall e criptografia de conexão.
As regras de firewall são usadas pelo servidor SQL lógico e seus bancos de dados para rejeitar tentativas de conexão de endereços IP que não foram aprovados explicitamente. Para permitir conexões do endereço IP público do seu aplicativo ou computador cliente, você deve primeiro criar uma regra de firewall no nível do servidor usando o portal do Azure, a API REST ou o PowerShell.
Como prática recomendada, você deve restringir o máximo possível os intervalos de endereços IP permitidos pelo firewall no nível do servidor. Para acessar seu pool SQL dedicado (anteriormente SQL DW) de seu computador local, certifique-se de que o firewall em sua rede e computador local permita a comunicação de saída na porta TCP 1433.
O pool SQL dedicado (anteriormente SQL DW) usa regras de firewall IP no nível do servidor. Ele não oferece suporte a regras de firewall IP em nível de banco de dados. Para obter mais informações, consulte regras de firewall do banco de dados SQL do Azure
As conexões com seu pool SQL dedicado (anteriormente SQL DW) são criptografadas por padrão. A modificação das configurações de conexão para desabilitar a criptografia é ignorada.
Autenticação
Autenticação refere-se a como você prova sua identidade ao se conectar ao banco de dados. O pool de SQL dedicado (anteriormente SQL DW) atualmente dá suporte à autenticação do SQL Server com um nome de usuário e senha e com o Azure Active Directory.
Quando você criou o servidor para seu banco de dados, você especificou um login de "administrador do servidor" com um nome de usuário e senha. Usando essas credenciais, você pode se autenticar em qualquer banco de dados nesse servidor como proprietário do banco de dados ou "dbo" por meio da Autenticação do SQL Server.
No entanto, como prática recomendada, os usuários da sua organização devem usar uma conta diferente para autenticar. Dessa forma, você pode limitar as permissões concedidas ao aplicativo e reduzir os riscos de atividades maliciosas caso o código do aplicativo seja vulnerável a um ataque de injeção de SQL.
Para criar um usuário autenticado do SQL Server, conecte-se ao banco de dados mestre em seu servidor com o logon de administrador do servidor e crie um novo logon de servidor. É uma boa ideia também criar um usuário no banco de dados mestre. A criação de um usuário no mestre permite que um usuário efetue login usando ferramentas como o SSMS sem especificar um nome de banco de dados. Também permite que eles usem o explorador de objetos para visualizar todos os bancos de dados em um servidor.
SQL
cópia de
-- Connect to master database and create a login
CREATE LOGIN ApplicationLogin WITH PASSWORD = 'Str0ng_password';
CREATE USER ApplicationUser FOR LOGIN ApplicationLogin;
Em seguida, conecte-se ao seu pool SQL dedicado (anteriormente SQL DW) com seu login de administrador do servidor e crie um usuário de banco de dados com base no login do servidor que você criou.
SQL
cópia de
-- Connect to the database and create a database user
CREATE USER ApplicationUser FOR LOGIN ApplicationLogin;
Para conceder a um usuário permissão para executar operações adicionais, como criar logons ou novos bancos de dados, atribua ao usuário as funções Loginmanagere no banco de dados mestre.dbmanager
Para obter mais informações sobre essas funções adicionais e autenticação em um Banco de Dados SQL, consulte Gerenciando bancos de dados e logons no Banco de Dados SQL do Azure . Para obter mais informações sobre como se conectar usando o Azure Active Directory, consulte Conectando-se usando a autenticação do Azure Active Directory .
Autorização
Autorização refere-se ao que você pode fazer em um banco de dados depois de autenticado e conectado. Os privilégios de autorização são determinados pelas associações e permissões da função. Como prática recomendada, você deve conceder aos usuários o mínimo de privilégios necessários. Para gerenciar funções, você pode usar os seguintes procedimentos armazenados:
SQL
cópia de
EXEC sp_addrolemember 'db_datareader', 'ApplicationUser'; -- allows ApplicationUser to read data
EXEC sp_addrolemember 'db_datawriter', 'ApplicationUser'; -- allows ApplicationUser to write data
A conta de administrador do servidor com a qual você está se conectando é membro de db_owner, que tem autoridade para fazer qualquer coisa no banco de dados. Salve esta conta para implantar atualizações de esquema e outras operações de gerenciamento. Use a conta "ApplicationUser" com permissões mais limitadas para se conectar de seu aplicativo ao banco de dados com os privilégios mínimos necessários para seu aplicativo.
Existem maneiras de limitar ainda mais o que um usuário pode fazer no banco de dados:
Permissões granulares permitem controlar quais operações você pode fazer em colunas, tabelas, exibições, esquemas, procedimentos e outros objetos individuais no banco de dados. Use permissões granulares para ter o máximo de controle e conceder as permissões mínimas necessárias.
Funções de banco de dados diferentes de db_datareader e db_datawriter podem ser usadas para criar contas de usuário de aplicativo mais poderosas ou contas de gerenciamento menos poderosas. As funções de banco de dados fixas internas fornecem uma maneira fácil de conceder permissões, mas podem resultar na concessão de mais permissões do que o necessário.
Os procedimentos armazenados podem ser usados para limitar as ações que podem ser executadas no banco de dados.
O exemplo a seguir concede acesso de leitura a um esquema definido pelo usuário.
SQL
cópia de
--CREATE SCHEMA Test
GRANT SELECT ON SCHEMA::Test to ApplicationUser
O gerenciamento de bancos de dados e servidores do portal do Azure ou o uso da API do Azure Resource Manager é controlado pelas atribuições de função da sua conta de usuário do portal. Para obter mais informações, consulte Atribuir funções do Azure usando o portal do Azure .
Criptografia
A Criptografia de Dados Transparente (TDE) ajuda a proteger contra a ameaça de atividade maliciosa criptografando e descriptografando seus dados em repouso. Quando você criptografa seu banco de dados, os backups associados e os arquivos de log de transações são criptografados sem exigir nenhuma alteração em seus aplicativos. A TDE criptografa o armazenamento de um banco de dados inteiro usando uma chave simétrica chamada chave de criptografia do banco de dados.
No Banco de Dados SQL, a chave de criptografia do banco de dados é protegida por um certificado de servidor interno. O certificado de servidor integrado é exclusivo para cada servidor. A Microsoft alterna automaticamente esses certificados pelo menos a cada 90 dias. O algoritmo de criptografia usado é AES-256. Para obter uma descrição geral do TDE, consulte Transparent Data Encryption .
Você pode criptografar seu banco de dados usando o portal do Azure ou T-SQL .
Use tabelas externas com Synapse SQL
Uma tabela externa aponta para dados localizados no Hadoop, blob de armazenamento do Azure ou Azure Data Lake Storage. As tabelas externas são usadas para ler dados de arquivos ou gravar dados em arquivos no Armazenamento do Azure. Com o Synapse SQL, você pode usar tabelas externas para ler dados externos usando pool SQL dedicado ou pool SQL sem servidor.
Dependendo do tipo de fonte de dados externa, você pode usar dois tipos de tabelas externas:
      * Tabelas externas do Hadoop que você pode usar para ler e exportar dados em vários formatos de dados, como CSV, Parquet e ORC. As tabelas externas do Hadoop estão disponíveis em pools SQL dedicados, mas não estão disponíveis em pools SQL sem servidor.
      * Tabelas externas nativas que você pode usar para ler e exportar dados em vários formatos de dados, como CSV e Parquet. As tabelas externas nativas estão disponíveis em pools SQL sem servidor e estão em visualização pública em pools SQL dedicados.
As principais diferenças entre o Hadoop e as tabelas externas nativas são apresentadas na tabela a seguir:
Tipo de mesa externa
	Hadoop
	Nativo
	Pool SQL dedicado
	Disponível
	As tabelas Parquet estão disponíveis em pré-visualização pública .
	Pool de SQL sem servidor
	Não disponível
	Disponível
	Formatos compatíveis
	Delimitado/CSV, Parquet, ORC, Hive RC e RC
	Pool de SQL sem servidor: Delimitado/CSV, Parquet e Delta Lake (versão prévia)
Pool de SQL dedicado: Parquet
	Eliminação de partição de pasta
	No
	Somente para tabelas particionadas sincronizadas de pools do Apache Spark no espaço de trabalho Synapse para pools SQL sem servidor
	Formato personalizado para localização
	sim
	Sim, usando curingas como/year=*/month=*/day=*
	Verificação de pasta recursiva
	No
	Somente em pools SQL sem servidor quando especificado /**no final do caminho do local
	Empurrão do filtro de armazenamento
	No
	Sim no pool SQL sem servidor. Para o empilhamento de string, você precisa usar Latin1_General_100_BIN2_UTF8o agrupamento nas VARCHARcolunas.
	Autenticação de armazenamento
	Chave de acesso ao armazenamento (SAK), passagem AAD, identidade gerenciada, identidade do Azure AD de aplicativo personalizado
	Assinatura de acesso compartilhado (SAS), passagem AAD, identidade gerenciada
	 Observação
As tabelas externas nativas no formato Delta Lake estão em visualização pública. Para obter mais informações, consulte Consultar arquivos do Delta Lake (versão prévia) . O CETAS não suporta a exportação de conteúdo no formato Delta Lake.
Tabelas externas em pool SQL dedicado e pool SQL sem servidor
Você pode usar tabelas externas para:
      * Consulte o Armazenamento de Blobs do Azure e o Azure Data Lake Gen2 com instruções Transact-SQL.
      * Armazene os resultados da consulta em arquivos no Armazenamento de Blobs do Azure ou Azure Data Lake Storage usando CETAS .
      * Importe dados do Armazenamento de Blobs do Azure e do Armazenamento do Azure Data Lake e armazene-os em um pool SQL dedicado (somente tabelas do Hadoop no pool dedicado).
 Observação
Quando usado em conjunto com a instrução CREATE TABLE AS SELECT , a seleção de uma tabela externa importa dados para uma tabela dentro do pool SQL dedicado . Além da instrução COPY , as tabelas externas são úteis para carregar dados.
Para obter um tutorial de carregamento, confira Usar o PolyBase para carregar dados do Armazenamento de Blobs do Azure .
Você pode criar tabelas externas em pools do Synapse SQL através das seguintes etapas:
      1. CREATE EXTERNAL DATA SOURCE para fazer referência a um armazenamento externo do Azure e especificar a credencial que deve ser usada para acessar o armazenamento.
      2. CRIAR FORMATO DE ARQUIVO EXTERNO para descrever o formato de arquivos CSV ou Parquet.
      3. CRIE EXTERNAL TABLE em cima dos arquivos colocados na fonte de dados com o mesmo formato de arquivo.
egurança
O usuário deve ter SELECTpermissão em uma tabela externa para ler os dados. As tabelas externas acessam o armazenamento do Azure subjacente usando a credencial no escopo do banco de dados definida na fonte de dados usando as seguintes regras:
      * A fonte de dados sem credencial permite que tabelas externas acessem arquivos disponíveis publicamente no armazenamento do Azure.
      * A fonte de dados pode ter uma credencial que permite que as tabelas externas acessem apenas os arquivos no armazenamento do Azure usando o token SAS ou a identidade gerenciada do espaço de trabalho - para obter exemplos, consulte o artigo Desenvolver controle de acesso ao armazenamento de arquivos de armazenamento
      *  
      * Tabelas de metadados compartilhados do Azure Synapse Analytics

      *  

 O Azure Synapse Analytics permite que os diferentes mecanismos computacionais do espaço de trabalho compartilhem bancos de dados e tabelas entre seus pools Apache Spark e o pool SQL sem servidor.

Depois que um banco de dados for criado por um trabalho do Spark, você poderá criar tabelas nele com o Spark que usem Parquet ou CSV como formato de armazenamento. Os nomes das tabelas serão convertidos em letras minúsculas e precisarão ser consultados usando o nome em letras minúsculas. Essas tabelas ficarão imediatamente disponíveis para consulta por qualquer um dos pools do Spark do espaço de trabalho do Azure Synapse. Eles também podem ser usados em qualquer um dos trabalhos do Spark sujeitos a permissões.

As tabelas criadas, gerenciadas e externas do Spark também são disponibilizadas como tabelas externas com o mesmo nome no banco de dados sincronizado correspondente no pool SQL sem servidor. A exposição de uma tabela Spark no SQL fornece mais detalhes sobre a sincronização da tabela.

Como as tabelas são sincronizadas com o pool SQL sem servidor de forma assíncrona, haverá um pequeno atraso até que elas apareçam.

Gerenciar uma tabela criada pelo Spark
Use o Spark para gerenciar bancos de dados criados pelo Spark. Por exemplo, exclua-o por meio de um trabalho de pool do Apache Spark sem servidor e crie tabelas nele do Spark.

Objetos em bancos de dados sincronizados não podem ser modificados do pool SQL sem servidor.

Expor uma tabela Spark no SQL
Tabelas do Spark compartilhadas
O Spark fornece dois tipos de tabelas que o Azure Synapse expõe no SQL automaticamente:

Tabelas gerenciadas

O Spark oferece muitas opções de como armazenar dados em tabelas gerenciadas, como TEXT, CSV, JSON, JDBC, PARQUET, ORC, HIVE, DELTA e LIBSVM. Esses arquivos são normalmente armazenados no warehousediretório onde os dados da tabela gerenciada são armazenados.

Tabelas externas

O Spark também fornece maneiras de criar tabelas externas sobre dados existentes, fornecendo a LOCATIONopção ou usando o formato Hive. Essas tabelas externas podem estar em vários formatos de dados, incluindo Parquet.

Atualmente, o Azure Synapse compartilha apenas tabelas Spark gerenciadas e externas que armazenam seus dados no formato Parquet ou CSV com os mecanismos SQL. As tabelas suportadas por outros formatos não são sincronizadas automaticamente. Você pode sincronizar essas tabelas explicitamente como uma tabela externa em seu próprio banco de dados SQL se o mecanismo SQL oferecer suporte ao formato subjacente da tabela.

Observação

Atualmente, apenas os formatos Parquet e CSV são sincronizados com o pool SQL sem servidor. Os metadados de uma tabela delta do Spark não serão sincronizados com o mecanismo SQL, mesmo que a tabela Delta use Parquet como o formato de armazenamento do instantâneo. As tabelas externas do Spark não estão sendo sincronizadas em bancos de dados de pool SQL dedicados.

Compartilhar tabelas do Spark
As tabelas Spark externas e gerenciadas compartilháveis expostas no mecanismo SQL como tabelas externas com as seguintes propriedades:

A fonte de dados da tabela externa SQL é a fonte de dados que representa a pasta de localização da tabela Spark.
O formato de arquivo da tabela externa SQL é Parquet ou CSV.
A credencial de acesso da tabela externa SQL é de passagem.
Como todos os nomes de tabela Spark são nomes de tabela SQL válidos e todos os nomes de coluna Spark são nomes de coluna SQL válidos, a tabela Spark e os nomes de coluna serão usados para a tabela externa SQL.

As tabelas Spark fornecem tipos de dados diferentes dos mecanismos Synapse SQL. A tabela a seguir mapeia os tipos de dados da tabela Spark para os tipos SQL:

COMPARTILHAR TABELAS DO SPARK
Tipo de dados do Spark Tipo de dados SQL Comentários
LongType, long,bigint bigint Spark : LongType representa números inteiros com sinal de 8 bytes.
SQL : Veja int, bigint, smallint e tinyint .
BooleanType,boolean bit(Parquet), varchar(6)(CSV) Spark : booleano.
SQL : Consulte [/sql/t-sql/data-types/bit-transact-sql).
DecimalType, decimal, dec,numeric decimal Spark : DecimalType representa números decimais assinados de precisão arbitrária. Apoiado internamente por java.math.BigDecimal. Um BigDecimal consiste em um valor inteiro sem escala de precisão arbitrária e uma escala inteira de 32 bits.
SQL : Precisão fixa e números de escala. Quando a precisão máxima é usada, os valores válidos são de - 10^38 +1 a 10^38 - 1. Os sinônimos ISO para decimal são dec e dec(p, s). numérico é funcionalmente idêntico ao decimal. Consulte decimal e numérico .
IntegerType, Integer,int int Spark IntegerType representa números inteiros com sinal de 4 bytes.
SQL : Veja int, bigint, smallint e tinyint .
ByteType, Byte,tinyint smallint Spark : ByteType representa números inteiros assinados de 1 byte [-128 a 127] e ShortType representa números inteiros assinados de 2 bytes [-32768 a 32767].
SQL : Tinyint representa números inteiros com sinal de 1 byte [0, 255] e smallint representa números inteiros com sinal de 2 bytes [-32768, 32767]. Veja int, bigint, smallint e tinyint .
ShortType, Short,smallint smallint O mesmo que acima.
DoubleType,Double float Spark : DoubleType representa números de ponto flutuante de precisão dupla de 8 bytes. SQL : Veja float e real .
FloatType, float,real real Spark : FloatType representa números de ponto flutuante de precisão dupla de 4 bytes. SQL : Veja float e real .
DateType,date date Spark : DateType representa valores que compreendem valores dos campos ano, mês e dia, sem fuso horário.
SQL : Ver data .
TimestampType,timestamp datetime2 Spark : TimestampType representa valores que compreendem valores dos campos ano, mês, dia, hora, minuto e segundo, com o fuso horário local da sessão. O valor do carimbo de data/hora representa um ponto absoluto no tempo.
SQL : Consulte datetime2 .
char char
StringType, String,varchar Varchar(n) Spark : StringType representa valores de cadeia de caracteres. VarcharType(n) é uma variante de StringType que tem uma limitação de comprimento. A gravação de dados falhará se a string de entrada exceder o limite de comprimento. Este tipo só pode ser usado em esquema de tabela, não em funções/operadores.
CharType(n) é uma variante de VarcharType(n) que tem comprimento fixo. A coluna de leitura do tipo CharType(n) sempre retorna valores de string de comprimento n. A comparação de colunas CharType(n) preencherá a mais curta com a mais longa.
SQL : Se houver um comprimento fornecido pelo Spark, n em varchar(n)será definido para esse comprimento. Se for uma coluna particionada, n pode ser no máximo 2048. Caso contrário, será varchar(max) . Veja char e varchar .
Use-o com agrupamento Latin1_General_100_BIN2_UTF8.
BinaryType,binary varbinary(n) SQL : Se houver um comprimento fornecido pelo Spark, nem Varbinary(n) será definido para esse comprimento. Se for uma coluna particionada, n pode ser no máximo 2048. Caso contrário, será Varbinary(max) . Veja binário e varbinary .
array, map,struct varchar(max) SQL : Serializa em JSON com agrupamento Latin1_General_100_BIN2_UTF8. Consulte Dados JSON .
Observação

O agrupamento de nível de banco de dados é Latin1_General_100_CI_AS_SC_UTF8.

Modelo de segurança
Os bancos de dados e tabelas do Spark, bem como suas representações sincronizadas no mecanismo SQL, serão protegidos no nível de armazenamento subjacente. Como eles não têm permissões nos próprios objetos, os objetos podem ser vistos no explorador de objetos.

A entidade de segurança que cria uma tabela gerenciada é considerada a proprietária dessa tabela e tem todos os direitos sobre a tabela, bem como sobre as pastas e arquivos subjacentes. Além disso, o proprietário do banco de dados se tornará automaticamente co-proprietário da tabela.

Se você criar uma tabela externa Spark ou SQL com passagem de autenticação, os dados serão protegidos apenas nos níveis de pasta e arquivo. Se alguém consultar esse tipo de tabela externa, a identidade de segurança do remetente da consulta será passada para o sistema de arquivos, que verificará os direitos de acesso.

Para obter mais informações sobre como definir permissões nas pastas e arquivos, consulte banco de dados compartilhado do Azure Synapse Analytics .

Mantendo as métricas do Azure Data Factory e os dados executados pelo pipeline

 O Data Factory armazena dados executados por pipeline por apenas 45 dias. Use o Azure Monitor se quiser manter esses dados por mais tempo. Com o Monitor, você pode rotear logs de diagnóstico para análise para vários destinos diferentes.

         * Conta de armazenamento : salve seus logs de diagnóstico em uma conta de armazenamento para auditoria ou inspeção manual. Você pode usar as configurações de diagnóstico para especificar o tempo de retenção em dias.
         * Hub de Eventos : transmita os logs para os Hubs de Eventos do Azure. Os logs se tornam entrada para uma solução de análise personalizada/serviço de parceiro, como o Power BI.
         * Log Analytics : Analise os logs com Log Analytics. A integração do Data Factory com o Azure Monitor é útil nos seguintes cenários:
         * Você deseja escrever consultas complexas em um rico conjunto de métricas que são publicadas pelo Data Factory para Monitorar. Você pode criar alertas personalizados nessas consultas por meio do Monitor.
         * Você deseja monitorar em todas as fábricas de dados. Você pode rotear dados de várias fábricas de dados para um único espaço de trabalho do Monitor.
         * Você também pode usar uma conta de armazenamento ou namespace de hub de eventos que não esteja na assinatura do recurso que emite logs. O usuário que define a configuração deve ter acesso apropriado ao controle de acesso baseado em função do Azure (Azure RBAC) para ambas as assinaturas.
 .
data movement in Azure Data Factory
Este artigo descreve a infraestrutura de segurança básica que os serviços de movimentação de dados no Azure Data Factory usam para ajudar a proteger seus dados. Os recursos de gerenciamento do Data Factory são criados na infraestrutura de segurança do Azure e usam todas as medidas de segurança possíveis oferecidas pelo Azure.
Em uma solução de Data Factory, você cria um ou mais pipelines de dados . Um pipeline é um agrupamento lógico de atividades que juntas executam uma tarefa. Esses pipelines residem na região em que o data factory foi criado.
Embora o Data Factory esteja disponível apenas em algumas regiões, o serviço de movimentação de dados está disponível globalmente para garantir conformidade de dados, eficiência e custos reduzidos de saída de rede.
O Azure Data Factory, incluindo o Azure Integration Runtime e o Integration Runtime auto-hospedado, não armazena dados temporários, dados de cache ou logs, exceto credenciais de serviço vinculadas para armazenamentos de dados na nuvem, que são criptografados usando certificados. Com o Data Factory, você cria fluxos de trabalho controlados por dados para orquestrar a movimentação de dados entre armazenamentos de dados com suporte e o processamento de dados usando serviços de computação em outras regiões ou em um ambiente local. Você também pode monitorar e gerenciar fluxos de trabalho usando SDKs e Azure Monitor.
A Data Factory foi certificada para:
Certificação CSA STAR
	ISO 20000-1:2011
	ISO 22301:2012
	ISO 27001:2013
	ISO 27017:2015
	ISO 27018:2014
	ISO 9001:2015
	SO 1, 2, 3
	HIPAA BAA
	HITRUST
	Se você estiver interessado na conformidade do Azure e em como o Azure protege sua própria infraestrutura, visite a Central de Confiabilidade da Microsoft . Para obter a lista mais recente de todas as ofertas de conformidade do Azure, verifique - https://aka.ms/AzureCompliance .
Neste artigo, revisamos as considerações de segurança nos dois cenários de movimentação de dados a seguir:
            * Cenário de nuvem : neste cenário, sua origem e seu destino são acessíveis publicamente pela Internet. Isso inclui serviços de armazenamento em nuvem gerenciados, como Azure Storage, Azure Synapse Analytics, Azure SQL Database, Azure Data Lake Store, Amazon S3, Amazon Redshift, serviços SaaS, como Salesforce, e protocolos da Web, como FTP e OData. Encontre uma lista completa de fontes de dados com suporte em Armazenamentos e formatos de dados com suporte .
            * Cenário híbrido : neste cenário, sua origem ou destino está atrás de um firewall ou dentro de uma rede corporativa local. Ou o armazenamento de dados está em uma rede privada ou rede virtual (na maioria das vezes a fonte) e não é acessível publicamente. Os servidores de banco de dados hospedados em máquinas virtuais também se enquadram nesse cenário.
 Observação
Este artigo usa o módulo Azure Az PowerShell, que é o módulo PowerShell recomendado para interagir com o Azure. Para começar com o módulo Az PowerShell, consulte Instalar o Azure PowerShell . Para saber como migrar para o módulo Az PowerShell, consulte Migrar Azure PowerShell do AzureRM para Az .
Cenários de nuvem
Protegendo credenciais de armazenamento de dados
            * Armazene credenciais criptografadas em um repositório gerenciado do Azure Data Factory . O Data Factory ajuda a proteger suas credenciais de armazenamento de dados criptografando-as com certificados gerenciados pela Microsoft. Esses certificados são alternados a cada dois anos (o que inclui renovação de certificado e migração de credenciais). Para obter mais informações sobre a segurança do armazenamento do Azure, consulte visão geral da segurança do armazenamento do Azure .
            * Armazenar credenciais no Azure Key Vault . Você também pode armazenar a credencial do armazenamento de dados no Azure Key Vault . O Data Factory recupera a credencial durante a execução de uma atividade. Para obter mais informações, consulte Armazenar credencial no Azure Key Vault .
Criptografia de dados em trânsito
Se o armazenamento de dados na nuvem oferecer suporte a HTTPS ou TLS, todas as transferências de dados entre os serviços de movimentação de dados no Data Factory e um armazenamento de dados na nuvem serão por meio de um canal seguro HTTPS ou TLS.
 Observação
Todas as conexões com o Banco de Dados SQL do Azure e o Azure Synapse Analytics exigem criptografia (SSL/TLS) enquanto os dados estão em trânsito de e para o banco de dados. Ao criar um pipeline usando JSON, adicione a propriedade de criptografia e defina-a como true na cadeia de conexão. Para o Armazenamento do Azure, você pode usar HTTPS na cadeia de conexão.
 Observação
Para habilitar a criptografia em trânsito ao mover dados do Oracle, siga uma das opções abaixo:
            1. No servidor Oracle, vá para Oracle Advanced Security (OAS) e defina as configurações de criptografia, que suportam Criptografia Triple-DES (3DES) e Padrão de Criptografia Avançada (AES), consulte aqui para obter detalhes. O ADF negocia automaticamente o método de criptografia para usar aquele que você configura no OAS ao estabelecer conexão com o Oracle.
            2. No ADF, você pode adicionar EncryptionMethod=1 na string de conexão (no Linked Service). Isso usará SSL/TLS como método de criptografia. Para usar isso, você precisa desabilitar as configurações de criptografia não SSL no OAS no lado do servidor Oracle para evitar conflito de criptografia.
 Observação
A versão TLS usada é 1.2.
Criptografia de dados em repouso
Alguns armazenamentos de dados oferecem suporte à criptografia de dados em repouso. Recomendamos que você habilite o mecanismo de criptografia de dados para esses armazenamentos de dados.
Azure Synapse Analytics
A Criptografia de Dados Transparente (TDE) no Azure Synapse Analytics ajuda a proteger contra a ameaça de atividade maliciosa executando criptografia e descriptografia em tempo real de seus dados em repouso. Esse comportamento é transparente para o cliente. Para obter mais informações, consulte Proteger um banco de dados no Azure Synapse Analytics .
Banco de Dados SQL do Azure
O Banco de Dados SQL do Azure também oferece suporte à criptografia de dados transparente (TDE), que ajuda a proteger contra a ameaça de atividade maliciosa executando criptografia e descriptografia em tempo real dos dados, sem exigir alterações no aplicativo. Esse comportamento é transparente para o cliente. Para obter mais informações, consulte Criptografia de dados transparente para Banco de Dados SQL e Data Warehouse .
Loja do Azure Data Lake
O Azure Data Lake Store também fornece criptografia para dados armazenados na conta. Quando habilitado, o Data Lake Store criptografa automaticamente os dados antes de persistir e descriptografa antes da recuperação, tornando-os transparentes para o cliente que acessa os dados. Para obter mais informações, consulte Segurança no Azure Data Lake Store .
Armazenamento de Blobs do Azure e armazenamento de Tabelas do Azure
O armazenamento de BLOBs do Azure e o armazenamento de tabelas do Azure dão suporte à criptografia do serviço de armazenamento (SSE), que criptografa automaticamente seus dados antes de persistir no armazenamento e descriptografa antes da recuperação. Para obter mais informações, consulte Criptografia do serviço de armazenamento do Azure para dados em repouso .
Amazon S3
O Amazon S3 oferece suporte à criptografia de dados em repouso do cliente e do servidor. Para obter mais informações, consulte Protegendo dados usando criptografia .
Amazon Redshift
O Amazon Redshift oferece suporte à criptografia de cluster para dados em repouso. Para obter mais informações, consulte Criptografia de banco de dados do Amazon Redshift .
Força de vendas
O Salesforce oferece suporte à criptografia da plataforma Shield, que permite a criptografia de todos os arquivos, anexos e campos personalizados. Para obter mais informações, consulte Noções básicas sobre o fluxo de autenticação OAuth do servidor Web .
Cenários híbridos
Cenários híbridos exigem que o tempo de execução de integração auto-hospedado seja instalado em uma rede local, dentro de uma rede virtual (Azure) ou dentro de uma nuvem privada virtual (Amazon). O Integration Runtime auto-hospedado deve ser capaz de acessar os armazenamentos de dados locais. Para obter mais informações sobre o tempo de execução de integração auto-hospedado, consulte Como criar e configurar o tempo de execução de integração auto-hospedado .
 
O canal de comando permite a comunicação entre os serviços de movimentação de dados no Data Factory e o tempo de execução de integração auto-hospedado. A comunicação contém informações relacionadas à atividade. O canal de dados é usado para transferir dados entre armazenamentos de dados locais e armazenamentos de dados em nuvem.
Credenciais de armazenamento de dados local
As credenciais podem ser armazenadas no data factory ou referenciadas pelo data factory durante o tempo de execução do Azure Key Vault. Se estiver armazenando credenciais no data factory, ele sempre será armazenado criptografado no tempo de execução de integração auto-hospedado.
            * Armazene as credenciais localmente . Se você usar diretamente o cmdlet Set-AzDataFactoryV2LinkedService com as cadeias de conexão e credenciais em linha no JSON, o serviço vinculado será criptografado e armazenado no tempo de execução de integração auto-hospedado. Nesse caso, as credenciais fluem pelo serviço de back-end do Azure, que é extremamente seguro, para a máquina de integração auto-hospedada, onde é finalmente criptografada e armazenada. O tempo de execução de integração auto-hospedado usa o Windows DPAPI para criptografar os dados confidenciais e as informações de credenciais.

            * Armazenar credenciais no Azure Key Vault . Você também pode armazenar a credencial do armazenamento de dados no Azure Key Vault . O Data Factory recupera a credencial durante a execução de uma atividade. Para obter mais informações, consulte Armazenar credencial no Azure Key Vault .

            * Armazene credenciais localmente sem passar as credenciais por meio do back-end do Azure para o tempo de execução de integração auto-hospedado . Se você quiser criptografar e armazenar credenciais localmente no tempo de execução de integração auto-hospedado sem ter que passar as credenciais pelo back-end do data factory, siga as etapas em Criptografar credenciais para armazenamentos de dados locais no Azure Data Factory . Todos os conectores suportam esta opção. O tempo de execução de integração auto-hospedado usa o Windows DPAPI para criptografar os dados confidenciais e as informações de credenciais.

            * Use o cmdlet New-AzDataFactoryV2LinkedServiceEncryptedCredential para criptografar credenciais de serviço vinculadas e detalhes confidenciais no serviço vinculado. Você pode usar o JSON retornado (com o elemento EncryptedCredential na cadeia de conexão) para criar um serviço vinculado usando o cmdlet Set-AzDataFactoryV2LinkedService .

Portas usadas ao criptografar o serviço vinculado no tempo de execução de integração auto-hospedado
Por padrão, quando o acesso remoto da intranet está habilitado, o PowerShell usa a porta 8060 na máquina com tempo de execução de integração auto-hospedado para comunicação segura. Se necessário, esta porta pode ser alterada no Integration Runtime Configuration Manager na guia Configurações:
 
Criptografia em trânsito
Todas as transferências de dados são feitas por meio de um canal seguro HTTPS e TLS sobre TCP para evitar ataques man-in-the-middle durante a comunicação com os serviços do Azure.
Você também pode usar IPSec VPN ou Azure ExpressRoute para proteger ainda mais o canal de comunicação entre sua rede local e o Azure.
A Rede Virtual do Azure é uma representação lógica de sua rede na nuvem. Você pode conectar uma rede local à sua rede virtual configurando VPN IPSec (site a site) ou ExpressRoute (peering privado).
A tabela a seguir resume as recomendações de configuração de tempo de execução de integração auto-hospedada e de rede com base em diferentes combinações de locais de origem e destino para movimentação de dados híbridos.
Fonte
	Destino
	Configuração de rede
	Configuração do tempo de execução da integração
	No local
	Máquinas virtuais e serviços em nuvem implantados em redes virtuais
	VPN IPSec (ponto a site ou site a site)
	O tempo de execução de integração auto-hospedado deve ser instalado em uma máquina virtual do Azure na rede virtual.
	No local
	Máquinas virtuais e serviços em nuvem implantados em redes virtuais
	ExpressRoute (peering privado)
	O tempo de execução de integração auto-hospedado deve ser instalado em uma máquina virtual do Azure na rede virtual.
	No local
	Serviços baseados no Azure que têm um ponto de extremidade público
	ExpressRoute (emparelhamento da Microsoft)
	O tempo de execução de integração auto-hospedado pode ser instalado localmente ou em uma máquina virtual do Azure.
	As imagens a seguir mostram o uso do Integration Runtime auto-hospedado para mover dados entre um banco de dados local e serviços do Azure usando ExpressRoute e VPN IPSec (com a Rede Virtual do Azure):
Configurações de firewall e configuração de lista de permissões para endereços IP
 Observação
Talvez seja necessário gerenciar portas ou configurar uma lista de permissões para domínios no nível de firewall corporativo conforme exigido pelas respectivas fontes de dados. Esta tabela usa apenas o Banco de Dados SQL do Azure, Azure Synapse Analytics e Azure Data Lake Store como exemplos.
 Observação
Para obter detalhes sobre estratégias de acesso a dados por meio do Azure Data Factory, consulte este artigo .
Requisitos de firewall para rede local/privada
Em uma empresa, um firewall corporativo é executado no roteador central da organização. O Firewall do Windows é executado como um daemon na máquina local em que o tempo de execução de integração auto-hospedado está instalado.
A tabela a seguir fornece os requisitos de porta e domínio de saída para firewalls corporativos:
Nomes de domínio
	Portas de saída
	Descrição
	*.servicebus.windows.net
	443
	Requerido pelo tempo de execução de integração auto-hospedado para autoria interativa.
	{datafactory}.{region}.datafactory.azure.net
ou *.frontend.clouddatahub.net
	443
	Requerido pelo Integration Runtime auto-hospedado para se conectar ao serviço Data Factory.
Para o Data Factory recém-criado, localize o FQDN da chave do Integration Runtime auto-hospedada que está no formato {datafactory}.{region}.datafactory.azure.net. Para o Data factory antigo, se você não vir o FQDN em sua chave de integração auto-hospedada, use *.frontend.clouddatahub.net.
	download.microsoft.com
	443
	Requerido pelo Integration Runtime auto-hospedado para baixar as atualizações. Se você desativou a atualização automática, pode pular a configuração deste domínio.
	*.core.windows.net
	443
	Usado pelo tempo de execução de integração auto-hospedado para se conectar à conta de armazenamento do Azure ao usar o recurso de cópia em estágios .
	*.database.windows.net
	1433
	Necessário apenas quando você copia de ou para o Banco de Dados SQL do Azure ou Azure Synapse Analytics e opcional de outra forma. Use o recurso de cópia em estágios para copiar dados para o Banco de Dados SQL ou Azure Synapse Analytics sem abrir a porta 1433.
	*.azuredatalakestore.net
login.microsoftonline.com/<tenant>/oauth2/token
	443
	Necessário apenas quando você copia de ou para o Azure Data Lake Store e opcional caso contrário.
	 Observação
Talvez seja necessário gerenciar portas ou configurar uma lista de permissões para domínios no nível de firewall corporativo conforme exigido pelas respectivas fontes de dados. Esta tabela usa apenas o Banco de Dados SQL do Azure, Azure Synapse Analytics e Azure Data Lake Store como exemplos.
A tabela a seguir fornece os requisitos de porta de entrada para o Firewall do Windows:
Portas de entrada
	Descrição
	8060 (TCP)
	Obrigatório pelo cmdlet de criptografia do PowerShell, conforme descrito em Criptografar credenciais para armazenamentos de dados locais no Azure Data Factory e pelo aplicativo do gerenciador de credenciais para definir com segurança credenciais para armazenamentos de dados locais no tempo de execução de integração auto-hospedado.
	Configurações de IP e configuração de lista de permissões em armazenamentos de dados
Alguns armazenamentos de dados na nuvem também exigem que você permita que o endereço IP da máquina acesse o armazenamento. Certifique-se de que o endereço IP da máquina de tempo de execução de integração auto-hospedada seja permitido ou configurado no firewall adequadamente.
Os armazenamentos de dados na nuvem a seguir exigem que você permita o endereço IP da máquina de tempo de execução de integração auto-hospedada. Alguns desses armazenamentos de dados, por padrão, podem não exigir a lista de permissões.
               * Banco de Dados SQL do Azure
               * Azure Synapse Analytics
               * Loja do Azure Data Lake
               * Azure Cosmos DB
               * Amazon Redshift
Perguntas frequentes
O tempo de execução de integração auto-hospedado pode ser compartilhado entre diferentes fábricas de dados?
sim. Mais detalhes aqui .
Quais são os requisitos de porta para que o tempo de execução de integração auto-hospedado funcione?
O tempo de execução de integração auto-hospedado faz conexões baseadas em HTTP para acessar a Internet. As portas de saída 443 devem ser abertas para o tempo de execução de integração auto-hospedado para fazer essa conexão. Abra a porta de entrada 8060 apenas no nível da máquina (não no nível do firewall corporativo) para o aplicativo do gerenciador de credenciais. Se o Banco de Dados SQL do Azure ou o Azure Synapse Analytics for usado como origem ou destino, você também precisará abrir a porta 1433. Para obter mais informações, consulte a seção Configurações de firewall e configuração da lista de permissões para endereços IP .
Lidando com problemas de codificação de dados ao carregar dados no SQL Data Warehouse
Este blog destina-se a fornecer informações sobre alguns dos problemas de codificação de dados que você pode encontrar ao usar o Polybase para carregar dados no SQL Data Warehouse. Este artigo também fornece algumas opções que você pode usar para superar esses problemas e carregar os dados com êxito.
Problema
Na maioria dos casos, você estará migrando dados de um sistema externo para o SQL Data Warehouse ou trabalhando com dados que foram exportados em formato de arquivo simples. Se os dados forem formatados usando o padrão de codificação UTF-8 ou UTF-16, você poderá usar o Polybase para carregar os dados. No entanto, o formato de seus dados depende das opções de codificação suportadas pelo sistema de origem. Alguns sistemas não oferecem suporte para codificação UTF-8 ou UTF-16. Se os dados com os quais você está trabalhando estiverem formatados em um formato alternativo, como ISO-8859-1, poder converter os dados para o formato UTF-8/UTF-16 pode economizar tempo e esforço valiosos.
O fluxo de dados de um sistema de origem para o Armazenamento de Blobs do Azure e, em seguida, para o Azure SQL Data Warehouse (DW) é mostrado no gráfico a seguir:
O Armazenamento de Blobs do Azure é um local conveniente para armazenar dados para uso por serviços do Azure, como SQL DW. O PolyBase facilita o acesso aos dados usando o T-SQL, por exemplo, criando tabelas externas para os dados no Armazenamento de Blobs do Azure e carregando os dados em tabelas internas do SQL Data Warehouse usando uma consulta SELECT simples.
Se o volume dos dados carregados for pequeno, pode ser mais fácil exportar os dados do sistema de origem novamente, desta vez usando a codificação UTF-8/UTF-16. No entanto, para volumes de dados maiores, a reexportação, a compactação de dados e o carregamento de dados para o Armazenamento de Blobs do Azure podem levar semanas. Para evitar esse atraso, você precisa converter a codificação nos arquivos de dados no ambiente do Azure sem acessar o sistema de origem novamente.
Solução
As seções abaixo fornecem detalhes sobre as opções que você tem para converter a codificação do arquivo de origem para UTF-8/UTF-16.
Importante: o PolyBase suporta UTF16-LE. Isso não deve importar para os clientes no ecossistema do Windows, mas um cliente pode especificar UTF16-BE e sua carga falhar.
Opção 1: Bloco de Notas++
Você pode usar a ferramenta Notepad++ para alterar a codificação de um arquivo em um computador local. Basta baixar o arquivo de dados para um computador local, abrir o arquivo no Notepad++ e converter a codificação do arquivo para UTF-8/UTF-16.
1. Para visualizar a codificação de um arquivo de origem , clique no menu Codificação , conforme mostrado no gráfico a seguir:
 
O arquivo de origem no exemplo acima está codificado em ANSI.
2. Para converter a codificação do arquivo em UTF-8 , no menu Codificação , selecione Converter em UTF-8 .
 
3. Salve o arquivo, use o menu Codificação para visualizar a codificação e confirme se o arquivo agora está codificado usando UTF-8.
 
Depois que o arquivo for salvo na codificação UTF-8, você poderá usar o Polybase para carregá-lo no Armazenamento de Blobs do Azure e carregá-lo no SQL Data Warehouse.
Embora essa seja uma abordagem viável, existem algumas desvantagens, listadas abaixo:
               * Tempo de download
               * Espaço disponível no sistema local
               * Hora do upload
               * Funciona apenas com arquivos pequenos devido a restrições de memória e espaço
Opção 2: VM do Azure
Para superar algumas das desvantagens associadas ao uso do Notepad++, você pode usar uma VM do Azure para converter a codificação do arquivo de dados. Com esse método, todo o processo ocorre no ambiente do Azure, eliminando assim os atrasos associados à transferência de dados entre o Azure e o sistema local. Esse processo é mostrado no gráfico a seguir:
 
Essa abordagem tem as seguintes etapas de alto nível:
               1. Configurar uma VM do Azure (Windows ou Linux)
               2. Baixe o arquivo de dados do Armazenamento de Blobs do Azure para o armazenamento local na VM do Azure
               3. Extrair arquivo de dados (se aplicável)
               4. Converta a codificação do arquivo de dados usando um utilitário (personalizado/integrado)
               5. Carregue o arquivo de dados convertido do armazenamento local na VM do Azure para o Armazenamento de Blobs do Azure
Observe que essa abordagem tem suas próprias desvantagens:
               * Tempo de download
               * Espaço disponível no sistema local
               * Hora do upload
Opção 3: Armazenamento de Arquivos do Azure
Para superar as limitações associadas ao tempo de download e upload ao usar VMs do Azure, você pode usar o Armazenamento de Arquivos do Azure, que oferece compartilhamentos de arquivos SMB baseados em nuvem que você pode usar para migrar rapidamente aplicativos herdados que dependem de compartilhamentos de arquivos para o Azure sem reescritas dispendiosas. Com o Armazenamento de Arquivos do Azure, os aplicativos executados em máquinas virtuais ou serviços de nuvem do Azure podem montar um compartilhamento de arquivos na nuvem, assim como um aplicativo de desktop monta um compartilhamento SMB típico. Qualquer número de componentes do aplicativo pode montar e acessar o compartilhamento do File Storage simultaneamente, conforme mostrado no gráfico a seguir:
 
Observação: saiba mais sobre o armazenamento do Azure .
Ao usar o Armazenamento de Arquivos do Azure, esteja ciente dos limites de capacidade identificados na tabela a seguir:
 
Observação: uma lista completa de escalabilidade de armazenamento do Azure e metas de desempenho já está disponível.
Com essa abordagem, você pode ter todos os arquivos de dados no Armazenamento de Arquivos do Azure e ter uma VM do Azure que pode montar o Armazenamento de Arquivos do Azure. Após a montagem, a VM do Azure pode ler e gravar arquivos diretamente de/para o Armazenamento de Arquivos do Azure sem precisar baixar ou carregar do armazenamento local na VM do Azure.
Essa abordagem inclui as seguintes etapas de alto nível:
               1. Configurar uma VM do Azure (Windows ou Linux)
               2. Monte o Armazenamento de Arquivos do Azure na VM do Azure (consulte o procedimento abaixo)
               3. Extrair arquivo de dados (se aplicável)
               4. Converta a codificação do arquivo de dados usando um utilitário (personalizado/integrado)
O diagrama abaixo mostra o fluxo completo de compactação, transferência, extração, transformação e carregamento de dados via PolyBase no SQL DW:
 
Montando o Armazenamento de Arquivos do Azure para VM
O processo de montagem do Armazenamento de Arquivos do Azure para VM, Ubuntu Linux VM neste caso, envolve três etapas de alto nível:
               1. Instalando as bibliotecas/pacotes necessários.
               * sudo apt-get install cifs-utils
               2. Criando o local do ponto de montagem na VM do Azure para o qual o Armazenamento de Arquivos do Azure será mapeado.
               * sudo mkdir /mnt/mountpoint
               3. Montando o local do Armazenamento de Arquivos do Azure para o ponto de montagem da VM do Azure.
               * sudo mount -t cifs //myaccountname.file.core.windows.net/mysharename /mnt/ mountpoint -o vers=3.0 ,user=myaccountname,password=StorageAccountKeyEndingIn==,dir_mode=0777,file_mode=0777,serverino
Observação : obtenha detalhes completos sobre como montar o armazenamento de arquivos do Azure de uma VM do Linux .
Automatizando a conversão de codificação de dados
Esta seção fornece alguns detalhes sobre um projeto que aproveitou essa abordagem para converter a codificação de um arquivo de dados:
               * Dados de 131 tabelas exportados do sistema Netezza
               * 4 arquivos de dados por tabela de origem organizados sob o nome da pasta que representa a tabela de origem
               * Todos os arquivos de dados codificados no formato ANSI (ISO-8859-1)
               * Todos os arquivos de dados compactados usando a compactação GZ
               * O tamanho total dos arquivos de dados compactados era de 750 GB
               * O tamanho total dos arquivos de dados convertidos não compactados foi de 7,6 TB
Os arquivos de dados foram organizados no Armazenamento de Arquivos do Azure na seguinte estrutura:
 
Um instantâneo do script bash na VM do Ubuntu que foi usado para converter a codificação nos arquivos de dados automaticamente é mostrado no gráfico a seguir:
 
Este script executou o seguinte:
               1. Aceitou o nome da tabela como argumento
               2. Percorreu cada um dos 4 arquivos de dados para a tabela fornecida
               3. Para cada arquivo de dados
               * Extraiu o arquivo GZ compactado usando o comando gunzip
               * Converteu a codificação de cada arquivo usando o comando iconv onde a codificação do arquivo de origem é especificada como ISO-8859-1 e a codificação do arquivo de destino é especificada como UTF-8
               * Escreveu o arquivo convertido em uma pasta com o nome da tabela em ConvertedData
O script foi aprimorado ainda mais para percorrer uma lista de nomes de tabelas e repetir o processo acima, em vez de aceitar o nome da tabela como um argumento.
Converter de qualquer codificação para qualquer outra codificação
O script pode ser modificado para aceitar a codificação de e para como argumentos em vez de codificá-los no script. Uma lista completa de codificações suportadas pelo comando iconv pode ser recuperada executando o comando iconv -l no computador que você usará para converter a codificação de dados. 
Tutorial de streaming estruturado


Sensores, dispositivos IoT, redes sociais e transações online geram dados que precisam ser monitorados constantemente e tratados rapidamente. Como resultado, a necessidade de processamento de fluxo em grande escala e em tempo real é mais evidente do que nunca. Este módulo de tutorial apresenta o Structured Streaming, o modelo principal para lidar com conjuntos de dados de streaming no Apache Spark. No Streaming Estruturado, um fluxo de dados é tratado como uma tabela que está sendo continuamente anexada. Isso leva a um modelo de processamento de fluxo muito semelhante a um modelo de processamento em lote. Você expressa sua computação de streaming como uma consulta padrão em lote como em uma tabela estática, mas o Spark a executa como uma consulta incremental na tabela de entrada ilimitada.
 
Considere o fluxo de dados de entrada como a tabela de entrada. Cada item de dados que chega no fluxo é como uma nova linha sendo anexada à tabela de entrada.
 
Uma consulta na entrada gera uma tabela de resultados. A cada intervalo de disparo (digamos, a cada 1 segundo), novas linhas são anexadas à tabela de entrada, que eventualmente atualiza a tabela de resultados. Sempre que a tabela de resultados é atualizada, as linhas de resultados alteradas são gravadas em um coletor externo. A saída é definida como o que é gravado no armazenamento externo. A saída pode ser configurada em diferentes modos:
Modo Completo : Toda a tabela de resultados atualizada é gravada no armazenamento externo. Cabe ao conector de armazenamento decidir como lidar com a escrita de toda a tabela.
Append Mode : Somente novas linhas anexadas na tabela de resultados desde o último gatilho são gravadas no armazenamento externo. Isso é aplicável apenas para as consultas em que não se espera que as linhas existentes na Tabela de resultados sejam alteradas.
Modo de atualização : somente as linhas que foram atualizadas na tabela de resultados desde o último acionador são gravadas no armazenamento externo. Isso é diferente do Modo Completo, pois o Modo de Atualização gera apenas as linhas que foram alteradas desde o último acionador. Se a consulta não contiver agregações, será equivalente ao modo Anexar.
Neste módulo tutorial, você aprenderá como:
Carregar dados de amostra
Inicializar um fluxo
Iniciar um trabalho de fluxo
Consultar um fluxo
Também fornecemos um bloco de anotações de exemplo que você pode importar para acessar e executar todos os exemplos de código incluídos no módulo.
Carregar dados de amostra
A maneira mais fácil de começar com o Streaming Estruturado é usar um exemplo de conjunto de dados do Azure Databricks disponível na /databricks-datasetspasta acessível no espaço de trabalho do Azure Databricks. O Azure Databricks tem dados de eventos de exemplo como arquivos /databricks-datasets/structured-streaming/events/a serem usados para criar um aplicativo de streaming estruturado. Vamos dar uma olhada no conteúdo deste diretório.
Conceitos de ponto de verificação e repetição em trabalhos do Azure Stream Analytics


Este artigo descreve o ponto de verificação interno e os conceitos de repetição no Azure Stream Analytics e o impacto que eles têm na recuperação do trabalho. Cada vez que um trabalho do Stream Analytics é executado, as informações de estado são mantidas internamente. Essas informações de estado são salvas em um ponto de verificação periodicamente. Em alguns cenários, as informações do ponto de verificação são usadas para recuperação do trabalho se ocorrer uma falha ou atualização do trabalho. Em outras circunstâncias, o ponto de verificação não pode ser usado para recuperação e uma repetição é necessária.
Lógica de consulta com estado em elementos temporais
Um dos recursos exclusivos do trabalho do Azure Stream Analytics é executar processamento com estado, como agregações em janela, junções temporais e funções analíticas temporais. Cada um desses operadores mantém as informações de estado quando o trabalho é executado. O tamanho máximo da janela para esses elementos de consulta é de sete dias.
O conceito de janela temporal aparece em vários elementos de consulta do Stream Analytics:
Agregados com janelas (GROUP BY de janelas rolantes, saltitantes e deslizantes)
Junções temporais (JOIN com DATEDIFF)
Funções analíticas temporais (ISFIRST, LAST e LAG com LIMIT DURATION)
Recuperação de trabalho de falha de nó, incluindo atualização do SO
Cada vez que um trabalho do Stream Analytics é executado, internamente ele é dimensionado para trabalhar em vários nós do trabalhador. O estado de cada nó do trabalhador é verificado a cada poucos minutos, o que ajuda o sistema a se recuperar se ocorrer uma falha.
Às vezes, um determinado nó do trabalhador pode falhar ou pode ocorrer um upgrade do sistema operacional para esse nó do trabalhador. Para recuperar automaticamente, o Stream Analytics adquire um novo nó íntegro e o estado do nó do trabalhador anterior é restaurado a partir do último ponto de verificação disponível. Para retomar o trabalho, uma pequena quantidade de repetição é necessária para restaurar o estado a partir do momento em que o ponto de verificação é obtido. Normalmente, o intervalo de restauração é de apenas alguns minutos. Quando unidades de streaming suficientes forem selecionadas para o trabalho, a reprodução deverá ser concluída rapidamente.
Em uma consulta totalmente paralela, o tempo necessário para recuperar o atraso após uma falha de nó do trabalhador é proporcional a:
[a taxa de eventos de entrada] x [o comprimento do intervalo] / [número de partições de processamento]
Se você observar um atraso de processamento significativo devido à falha do nó e à atualização do SO, considere tornar a consulta totalmente paralela e dimensionar o trabalho para alocar mais unidades de streaming. Para obter mais informações, confira Dimensionar um trabalho do Azure Stream Analytics para aumentar a taxa de transferência .
O Stream Analytics atual não mostra um relatório quando esse tipo de processo de recuperação está ocorrendo.
Recuperação de trabalho de uma atualização de serviço
A Microsoft ocasionalmente atualiza os binários que executam os trabalhos do Stream Analytics no serviço do Azure. Nesses momentos, os trabalhos em execução dos usuários são atualizados para uma versão mais recente e o trabalho é reiniciado automaticamente.
Atualmente, o formato do ponto de verificação de recuperação não é preservado entre as atualizações. Como resultado, o estado da consulta de streaming deve ser totalmente restaurado usando a técnica de reprodução. Para permitir que os trabalhos do Stream Analytics reproduzam exatamente a mesma entrada de antes, é importante definir a política de retenção dos dados de origem para pelo menos os tamanhos de janela em sua consulta. Não fazer isso pode resultar em resultados incorretos ou parciais durante a atualização do serviço, pois os dados de origem podem não ser retidos o suficiente para incluir o tamanho total da janela.
Em geral, a quantidade de repetição necessária é proporcional ao tamanho da janela multiplicado pela taxa média de eventos. Por exemplo, para um trabalho com uma taxa de entrada de 1.000 eventos por segundo, considera-se que um tamanho de janela maior que uma hora tem um tamanho de repetição grande. Até uma hora de dados pode precisar ser reprocessado para inicializar o estado para que possa produzir resultados completos e corretos, o que pode causar atraso na saída (sem saída) por algum período prolongado. Consultas sem janelas ou outros operadores temporais, como JOINou LAG, não teriam repetição.
Estimar o tempo de recuperação de repetição
Para estimar a duração do atraso devido a uma atualização de serviço, você pode seguir esta técnica:
Carregue o Hub de Eventos de entrada com dados suficientes para cobrir o maior tamanho de janela em sua consulta, na taxa de eventos esperada. O carimbo de data/hora dos eventos deve estar próximo ao horário do relógio de parede durante esse período de tempo, como se fosse um feed de entrada ao vivo. Por exemplo, se você tiver uma janela de 3 dias em sua consulta, envie eventos para o Hub de Eventos por três dias e continue a enviar eventos.
Inicie o trabalho usando Agora como a hora de início.
Meça o tempo entre a hora de início e quando a primeira saída é gerada. O tempo é aproximado quanto atraso o trabalho incorreria durante uma atualização de serviço.
Se o atraso for muito longo, tente particionar seu trabalho e aumentar o número de SUs, para que a carga seja distribuída para mais nós. Como alternativa, considere reduzir os tamanhos de janela em sua consulta e execute mais agregação ou outro processamento com estado na saída produzida pelo trabalho do Stream Analytics no coletor downstream (por exemplo, usando o Banco de Dados SQL do Azure).
Para preocupações gerais de estabilidade de serviço durante a atualização de trabalhos de missão crítica, considere a execução de trabalhos duplicados em regiões do Azure emparelhadas. Para obter mais informações, consulte Garantir a confiabilidade do trabalho do Stream Analytics durante as atualizações do serviço .
Recuperação de trabalho de uma parada e partida iniciada pelo usuário
Para editar a sintaxe de consulta em um trabalho de streaming ou para ajustar entradas e saídas, o trabalho precisa ser interrompido para fazer as alterações e atualizar o design do trabalho. Nesses cenários, quando um usuário interrompe o trabalho de streaming e o inicia novamente, o cenário de recuperação é semelhante à atualização do serviço.
Os dados do ponto de verificação não podem ser usados para uma reinicialização de trabalho iniciada pelo usuário. Para estimar o atraso da saída durante essa reinicialização, use o mesmo procedimento descrito na seção anterior e aplique uma mitigação semelhante se o atraso for muito longo.
 
Janela de sessão (Azure Stream Analytics)
 
As janelas de sessão agrupam eventos que chegam em horários semelhantes, filtrando períodos de tempo em que não há dados. A função da janela de sessão tem três parâmetros principais: tempo limite, duração máxima e chave de particionamento (opcional).
O diagrama a seguir ilustra um fluxo com uma série de eventos e como eles são mapeados em janelas de sessão com tempo limite de 5 minutos e duração máxima de 10 minutos.
 
Uma janela de sessão começa quando ocorre o primeiro evento. Se outro evento ocorrer dentro do tempo limite especificado do último evento ingerido, a janela será estendida para incluir o novo evento. Caso contrário, se nenhum evento ocorrer dentro do tempo limite, a janela será fechada no tempo limite.
Se os eventos continuarem ocorrendo dentro do tempo limite especificado, a janela da sessão continuará se estendendo até que a duração máxima seja atingida. Observe que os intervalos de verificação de duração máxima são definidos para serem do mesmo tamanho que a duração máxima especificada. Por exemplo, se a duração máxima for 10, as verificações se a janela exceder a duração máxima ocorrerão em t = 0, 10, 20, 30 etc. Isso significa que a duração real de uma janela de sessão pode ser de até duas vezes maxDuration.
Assim, matematicamente, nossa janela de sessão termina se a seguinte condição for satisfeita:
Quando uma chave de partição é fornecida, os eventos são agrupados pela chave e a janela de sessão é aplicada a cada grupo independentemente. Isso é útil para casos em que você precisa de janelas de sessão diferentes para usuários ou dispositivos diferentes.


 Observação
A janela de sessão pode ser usada das duas maneiras acima.
Argumentos
timeunit É a unidade de tempo para o tamanho da janela. A tabela a seguir lista todos os argumentos de unidade de tempo válidos.
Unidade de tempo
	Abreviaturas
	dia
	dd, d
	hora
	hh
	minuto
	mi, n
	segundo
	ss, s
	milissegundo
	em
	microssegundo
	mcs
	tempo limite
Um grande número inteiro que descreve o tamanho do intervalo da janela da sessão. Os dados que ocorrem dentro do tamanho do intervalo são agrupados na mesma janela.
tamanho máximo
Se o tamanho total da janela exceder o maxDurationSize especificado em um ponto de verificação, a janela será fechada e uma nova janela será aberta no mesmo ponto. Atualmente, o tamanho do intervalo de verificação é igual a maxDurationSize.
chave de partição
Um parâmetro opcional que especifica a chave sobre a qual a janela de sessão opera. Se especificado, a janela agrupará apenas eventos da mesma chave.
Use pontos de extremidade privados para armazenamento do Azure


Você pode usar pontos de extremidade privados para suas contas de armazenamento do Azure para permitir que clientes em uma rede virtual (VNet) acessem dados com segurança por meio de um link privado . O ponto de extremidade privado usa um endereço IP separado do espaço de endereço VNet para cada serviço de conta de armazenamento. O tráfego de rede entre os clientes na VNet e a conta de armazenamento atravessa a VNet e um link privado na rede de backbone da Microsoft, eliminando a exposição da Internet pública.
O uso de endpoints privados para sua conta de armazenamento permite:
Proteja sua conta de armazenamento configurando o firewall de armazenamento para bloquear todas as conexões no ponto de extremidade público para o serviço de armazenamento.
Aumente a segurança da rede virtual (VNet), permitindo que você bloqueie a exfiltração de dados da VNet.
Conecte-se com segurança a contas de armazenamento de redes locais que se conectam à VNet usando VPN ou ExpressRoutes com peering privado.
Visão geral conceitual


Um ponto de extremidade privado é uma interface de rede especial para um serviço do Azure em sua rede virtual (VNet). Quando você cria um ponto de extremidade privado para sua conta de armazenamento, ele fornece conectividade segura entre clientes em sua VNet e seu armazenamento. O ponto de extremidade privado é atribuído a um endereço IP do intervalo de endereços IP de sua VNet. A conexão entre o endpoint privado e o serviço de armazenamento usa um link privado seguro.
Os aplicativos na VNet podem se conectar ao serviço de armazenamento pelo ponto de extremidade privado perfeitamente, usando as mesmas cadeias de conexão e mecanismos de autorização que usariam de outra forma . Os pontos de extremidade privados podem ser usados com todos os protocolos compatíveis com a conta de armazenamento, incluindo REST e SMB.
Pontos de extremidade privados podem ser criados em sub-redes que usam pontos de extremidade de serviço . Os clientes em uma sub-rede podem, assim, conectar-se a uma conta de armazenamento usando endpoints privados, enquanto usam endpoints de serviço para acessar outros.
Quando você cria um ponto de extremidade privado para um serviço de armazenamento em sua VNet, uma solicitação de consentimento é enviada para aprovação ao proprietário da conta de armazenamento. Se o usuário que solicita a criação do ponto de extremidade privado também for proprietário da conta de armazenamento, essa solicitação de consentimento será aprovada automaticamente.
Os proprietários de contas de armazenamento podem gerenciar solicitações de consentimento e pontos de extremidade privados por meio da guia ' Pontos de extremidade privados ' da conta de armazenamento no portal do Azure .
Dica
Se você quiser restringir o acesso à sua conta de armazenamento apenas por meio do endpoint privado, configure o firewall de armazenamento para negar ou controlar o acesso por meio do endpoint público.
Você pode proteger sua conta de armazenamento para aceitar apenas conexões de sua VNet, configurando o firewall de armazenamento para negar acesso por meio de seu ponto de extremidade público por padrão. Você não precisa de uma regra de firewall para permitir o tráfego de uma VNet que tenha um ponto de extremidade privado, pois o firewall de armazenamento controla apenas o acesso por meio do ponto de extremidade público. Em vez disso, os endpoints privados dependem do fluxo de consentimento para conceder acesso às sub-redes ao serviço de armazenamento.
Observação
Ao copiar blobs entre contas de armazenamento, seu cliente deve ter acesso à rede para ambas as contas. Portanto, se você optar por usar um link privado para apenas uma conta (seja a origem ou o destino), certifique-se de que seu cliente tenha acesso à rede para a outra conta. Para saber mais sobre outras formas de configurar o acesso à rede, consulte Configurar firewalls de armazenamento do Azure e redes virtuais .


Como criar um endpoint privado
Para criar um ponto de extremidade privado usando o Portal do Azure, confira Conectar-se de forma privada a uma conta de armazenamento da experiência da conta de armazenamento no portal do Azure .
Para criar um ponto de extremidade privado usando o PowerShell ou a CLI do Azure, consulte um desses artigos. Ambos apresentam um aplicativo Web do Azure como serviço de destino, mas as etapas para criar um link privado são as mesmas para uma conta de armazenamento do Azure.
Criar um ponto de extremidade privado usando a CLI do Azure
Criar um ponto de extremidade privado usando o Azure PowerShell
Ao criar um ponto de extremidade privado, você deve especificar a conta de armazenamento e o serviço de armazenamento ao qual ele se conecta.
Você precisa de um ponto de extremidade privado separado para cada recurso de armazenamento que precisa acessar, ou seja, Blobs , Data Lake Storage Gen2 , Files , Queues , Tables ou Static Websites . No ponto de extremidade privado, esses serviços de armazenamento são definidos como o sub-recurso de destino da conta de armazenamento associada.
Se você criar um ponto de extremidade privado para o recurso de armazenamento Data Lake Storage Gen2, também deverá criar um para o recurso de armazenamento de Blobs. Isso ocorre porque as operações que visam o ponto de extremidade do Data Lake Storage Gen2 podem ser redirecionadas para o ponto de extremidade do Blob. Ao criar um endpoint privado para ambos os recursos, você garante que as operações possam ser concluídas com êxito.
Dica
Crie um endpoint privado separado para a instância secundária do serviço de armazenamento para melhor desempenho de leitura em contas RA-GRS. Certifique-se de criar uma conta de armazenamento v2 (Standard ou Premium) de uso geral.
Para acesso de leitura à região secundária com uma conta de armazenamento configurada para armazenamento com redundância geográfica, você precisa de pontos de extremidade privados separados para as instâncias primária e secundária do serviço. Você não precisa criar um endpoint privado para a instância secundária para failover . O endpoint privado se conectará automaticamente à nova instância primária após o failover. Para obter mais informações sobre as opções de redundância de armazenamento, consulte redundância de armazenamento do Azure .


Conectando-se a um endpoint privado
Os clientes em uma VNet usando o ponto de extremidade privado devem usar a mesma cadeia de conexão para a conta de armazenamento, como clientes que se conectam ao ponto de extremidade público. Contamos com a resolução de DNS para rotear automaticamente as conexões da VNet para a conta de armazenamento por meio de um link privado.
Importante
Use a mesma cadeia de conexão para se conectar à conta de armazenamento usando pontos de extremidade privados, como você usaria de outra forma. Por favor, não se conecte à conta de armazenamento usando seu privatelinkURL de subdomínio.
Criamos uma zona DNS privada anexada à VNet com as atualizações necessárias para os endpoints privados, por padrão. No entanto, se você estiver usando seu próprio servidor DNS, talvez seja necessário fazer alterações adicionais em sua configuração de DNS. A seção sobre alterações de DNS abaixo descreve as atualizações necessárias para endpoints privados.
Alterações de DNS para endpoints privados
Quando você cria um ponto de extremidade privado, o registro de recurso DNS CNAME da conta de armazenamento é atualizado para um alias em um subdomínio com o prefixo privatelink. Por padrão, também criamos uma zona DNS privada , correspondente ao privatelinksubdomínio, com os registros de recursos DNS A para os endpoints privados.
Quando você resolve a URL do ponto de extremidade de armazenamento de fora da VNet com o ponto de extremidade privado, ela resolve para o ponto de extremidade público do serviço de armazenamento. Quando resolvido da VNet que hospeda o ponto de extremidade privado, a URL do ponto de extremidade de armazenamento resolve para o endereço IP do ponto de extremidade privado.
Para o exemplo ilustrado acima, os registros de recursos DNS para a conta de armazenamento 'StorageAccountA', quando resolvidos de fora da VNet que hospeda o ponto de extremidade privado, serão:
ALTERAÇÕES DE DNS PARA ENDPOINTS PRIVADOS
Nome Modelo Valor
StorageAccountA.blob.core.windows.net CNAME StorageAccountA.privatelink.blob.core.windows.net
StorageAccountA.privatelink.blob.core.windows.net CNAME <endpoint público do serviço de armazenamento>
<endpoint público do serviço de armazenamento> UMA <endereço IP público do serviço de armazenamento>
Conforme mencionado anteriormente, você pode negar ou controlar o acesso de clientes fora da VNet por meio do ponto de extremidade público usando o firewall de armazenamento.
Os registros de recursos DNS para StorageAccountA, quando resolvidos por um cliente na VNet que hospeda o ponto de extremidade privado, serão:
MESA 2
Nome Modelo Valor
StorageAccountA.blob.core.windows.net CNAME StorageAccountA.privatelink.blob.core.windows.net
StorageAccountA.privatelink.blob.core.windows.net UMA 10.1.1.5
Essa abordagem permite o acesso à conta de armazenamento usando a mesma cadeia de conexão para clientes na VNet que hospeda os pontos de extremidade privados, bem como clientes fora da VNet.
Se você estiver usando um servidor DNS personalizado em sua rede, os clientes deverão poder resolver o FQDN do ponto de extremidade da conta de armazenamento para o endereço IP do ponto de extremidade privado. Você deve configurar seu servidor DNS para delegar seu subdomínio de link privado à zona DNS privada para a VNet ou configurar os registros A para StorageAccountA.privatelink.blob.core.windows.netcom o endereço IP do ponto de extremidade privado.
Dica
Ao usar um servidor DNS personalizado ou local, você deve configurar seu servidor DNS para resolver o nome da conta de armazenamento no privatelinksubdomínio para o endereço IP do endpoint privado. Você pode fazer isso delegando o privatelinksubdomínio à zona DNS privada da VNet ou configurando a zona DNS em seu servidor DNS e adicionando os registros DNS A.
Os nomes de zona DNS recomendados para endpoints privados para serviços de armazenamento e os sub-recursos de destino de endpoint associados são:
TABELA 3
Serviço de armazenamento Sub-recurso de destino Nome da zona
Serviço de blob bolha privatelink.blob.core.windows.net
Data Lake Storage Gen2 dfs privatelink.dfs.core.windows.net
Serviço de arquivo Arquivo privatelink.file.core.windows.net
Serviço de fila fila privatelink.queue.core.windows.net
Bancada de serviço tabela privatelink.table.core.windows.net
Sites estáticos rede privatelink.web.core.windows.net
Para obter mais informações sobre como configurar seu próprio servidor DNS para oferecer suporte a endpoints privados, consulte os seguintes artigos:
Resolução de nomes para recursos em redes virtuais do Azure
Configuração de DNS para endpoints privados
Preços
Para obter detalhes de preços, consulte preços do Link Privado do Azure .
Problemas conhecidos
Lembre-se dos seguintes problemas conhecidos sobre pontos de extremidade privados para armazenamento do Azure.
Restrições de acesso ao armazenamento para clientes em VNets com endpoints privados
Os clientes em VNets com pontos de extremidade privados existentes enfrentam restrições ao acessar outras contas de armazenamento que possuem pontos de extremidade privados. Por exemplo, suponha que uma VNet N1 tenha um ponto de extremidade privado para uma conta de armazenamento A1 para armazenamento de Blobs. Se a conta de armazenamento A2 tiver um ponto de extremidade privado em uma VNet N2 para armazenamento de Blobs, os clientes na VNet N1 também deverão acessar o armazenamento de Blobs na conta A2 usando um ponto de extremidade privado. Se a conta de armazenamento A2 não tiver pontos de extremidade privados para armazenamento de Blobs, os clientes na VNet N1 poderão acessar o armazenamento de Blobs nessa conta sem um ponto de extremidade privado.
Essa restrição é resultado das alterações de DNS feitas quando a conta A2 cria um endpoint privado.
Regras do grupo de segurança de rede para sub-redes com endpoints privados
Atualmente, você não pode configurar regras do Network Security Group (NSG) e rotas definidas pelo usuário para endpoints privados. As regras NSG aplicadas à sub-rede que hospeda o endpoint privado não são aplicadas ao endpoint privado. Eles são aplicados apenas a outros terminais (por exemplo: controladores de interface de rede). Uma solução limitada para esse problema é implementar suas regras de acesso para pontos de extremidade privados nas sub-redes de origem, embora essa abordagem possa exigir uma sobrecarga de gerenciamento mais alta.
Copiando blobs entre contas de armazenamento
Você pode copiar blobs entre contas de armazenamento usando pontos de extremidade privados somente se usar a API REST do Azure ou ferramentas que usam a API REST. Essas ferramentas incluem AzCopy, Gerenciador de Armazenamento, Azure PowerShell, CLI do Azure e os SDKs do Armazenamento de Blobs do Azure.
Apenas os pontos de extremidade privados direcionados ao recurso de armazenamento de BLOBs têm suporte. Os pontos de extremidade privados que visam o Data Lake Storage Gen2 ou o recurso Arquivo ainda não têm suporte. Além disso, a cópia entre contas de armazenamento usando o protocolo NFS (Network File System) ainda não é compatível.
 
Copie o guia de desempenho e escalabilidade da atividade


Selecione a versão do Azure Data Factory que você está usando:
Versão Atual
APLICA-SE A: Azure Data Factory Azure Synapse Analytics
Às vezes, você deseja realizar uma migração de dados em grande escala do data lake ou data warehouse empresarial (EDW) para o Azure. Outras vezes, você deseja ingerir grandes quantidades de dados, de diferentes fontes no Azure, para análise de big data. Em cada caso, é fundamental alcançar o desempenho e a escalabilidade ideais.
Os pipelines Azure Data Factory e Azure Synapse Analytics fornecem um mecanismo para ingerir dados, com as seguintes vantagens:
Lida com grandes quantidades de dados
É altamente performático
É rentável
Essas vantagens são uma excelente opção para engenheiros de dados que desejam criar pipelines de ingestão de dados escaláveis com alto desempenho.
Depois de ler este artigo, você será capaz de responder às seguintes perguntas:
Que nível de desempenho e escalabilidade posso alcançar usando a atividade de cópia para cenários de migração e ingestão de dados?
Que passos devo tomar para ajustar o desempenho da atividade de cópia?
Quais otimizações de desempenho posso utilizar para uma única execução de atividade de cópia?
Que outros fatores externos devem ser considerados ao otimizar o desempenho da cópia?
Observação
Se você não estiver familiarizado com a atividade de cópia em geral, consulte a visão geral da atividade de cópia antes de ler este artigo.
Desempenho de cópia e escalabilidade alcançáveis usando pipelines do Azure Data Factory e Synapse
Os pipelines do Azure Data Factory e Synapse oferecem uma arquitetura sem servidor que permite o paralelismo em diferentes níveis.
Essa arquitetura permite desenvolver pipelines que maximizam a taxa de transferência de dados para seu ambiente. Esses pipelines utilizam totalmente os seguintes recursos:
Largura de banda de rede entre os armazenamentos de dados de origem e destino
Operações de entrada/saída de armazenamento de dados de origem ou destino por segundo (IOPS) e largura de banda
Essa utilização total significa que você pode estimar a taxa de transferência geral medindo a taxa de transferência mínima disponível com os seguintes recursos:
Armazenamento de dados de origem
Armazenamento de dados de destino
Largura de banda de rede entre os armazenamentos de dados de origem e destino
A tabela abaixo mostra o cálculo da duração da movimentação de dados. A duração em cada célula é calculada com base em uma determinada rede e largura de banda de armazenamento de dados e um determinado tamanho de carga útil de dados.
Observação
A duração fornecida abaixo destina-se a representar o desempenho alcançável em uma solução de integração de dados de ponta a ponta usando uma ou mais técnicas de otimização de desempenho descritas em Copiar recursos de otimização de desempenho , incluindo o uso de ForEach para particionar e gerar várias atividades de cópia simultâneas. Recomendamos que você siga as etapas descritas em Etapas de ajuste de desempenho para otimizar o desempenho da cópia para seu conjunto de dados específico e configuração do sistema. Você deve usar os números obtidos em seus testes de ajuste de desempenho para planejamento de implantação de produção, planejamento de capacidade e projeção de faturamento.
 
TABELA 1
Tamanho dos dados /
largura de banda 50 Mbps 100 Mbps 500 Mbps 1 Gbps 5 Gbps 10 Gbps 50 Gbps
1 GB 2,7 minutos 1,4 min 0,3 min 0,1 minuto 0,03 min 0,01 min 0,0 min
10 GB 27,3 minutos 13,7 minutos 2,7 minutos 1,3 min 0,3 min 0,1 minuto 0,03 min
100 GB 4,6 horas 2,3 horas 0,5 hora 0,2 horas 0,05 horas 0,02 horas 0,0 hora
1 TB 46,6 horas 23,3 horas 4,7 horas 2,3 horas 0,5 hora 0,2 horas 0,05 horas
10TB 19,4 dias 9,7 dias 1,9 dias 0,9 dias 0,2 dias 0,1 dias 0,02 dias
100 TB 194,2 dias 97,1 dias 19,4 dias 9,7 dias 1,9 dias 1 dia 0,2 dias
1 PB 64,7 meses 32,4 meses 6,5 meses 3,2 meses 0,6 meses 0,3 meses 0,06 meses
10 PB 647,3 meses 323,6 meses 64,7 meses 31,6 meses 6,5 meses 3,2 meses 0,6 meses
A cópia é escalável em diferentes níveis:
 
O fluxo de controle pode iniciar várias atividades de cópia em paralelo, por exemplo, usando For Each loop .
Uma única atividade de cópia pode aproveitar os recursos de computação escalonáveis.
Ao usar o tempo de execução de integração do Azure (IR), você pode especificar até 256 unidades de integração de dados (DIUs) para cada atividade de cópia, sem servidor.
Ao usar o IR auto-hospedado, você pode adotar uma das seguintes abordagens:
Aumente manualmente a máquina.
Dimensione para várias máquinas ( até 4 nós ) e uma única atividade de cópia particionará seu conjunto de arquivos em todos os nós.
Uma única atividade de cópia lê e grava no armazenamento de dados usando vários threads em paralelo .
Etapas de ajuste de desempenho
Siga as etapas a seguir para ajustar o desempenho de seu serviço com a atividade de cópia:
Pegue um conjunto de dados de teste e estabeleça uma linha de base.
Durante o desenvolvimento, teste seu pipeline usando a atividade de cópia em uma amostra de dados representativa. O conjunto de dados escolhido deve representar seus padrões de dados típicos com os seguintes atributos:
Estrutura de pastas
Padrão de arquivo
Esquema de dados
E seu conjunto de dados deve ser grande o suficiente para avaliar o desempenho da cópia. Um bom tamanho leva pelo menos 10 minutos para que a atividade de cópia seja concluída. Colete detalhes de execução e características de desempenho após o monitoramento da atividade de cópia .
Como maximizar o desempenho de uma única atividade de cópia :
Recomendamos que você primeiro maximize o desempenho usando uma única atividade de cópia.
Se a atividade de cópia estiver sendo executada em um tempo de execução de integração do Azure :
Comece com valores padrão para unidades de integração de dados (DIU) e configurações de cópia paralela .
Se a atividade de cópia estiver sendo executada em um Integration Runtime auto-hospedado :
Recomendamos que você use uma máquina dedicada para hospedar o IR. A máquina deve ser separada do servidor que hospeda o armazenamento de dados. Comece com os valores padrão para a configuração de cópia paralela e use um único nó para o IR auto-hospedado.
Faça um teste de desempenho. Anote o desempenho alcançado. Inclua os valores reais usados, como DIUs e cópias paralelas. Consulte o monitoramento de atividade de cópia sobre como coletar resultados de execução e configurações de desempenho usadas. Saiba como solucionar problemas de desempenho da atividade de cópia para identificar e resolver o gargalo.
Iterar para realizar execuções de teste de desempenho adicionais seguindo as orientações de solução de problemas e ajuste. Uma vez que a atividade de cópia única não consiga obter uma melhor taxa de transferência, considere se deseja maximizar a taxa de transferência agregada executando várias cópias simultaneamente. Essa opção é discutida no próximo marcador numerado.
Como maximizar a taxa de transferência agregada executando várias cópias simultaneamente:
Até agora você maximizou o desempenho de uma única atividade de cópia. Se você ainda não atingiu os limites superiores de taxa de transferência de seu ambiente, poderá executar várias atividades de cópia em paralelo. Você pode executar em paralelo usando construções de fluxo de controle. Uma dessas construções é o loop For Each . Para obter mais informações, consulte os seguintes artigos sobre modelos de solução:
Copiar arquivos de vários contêineres
Migrar dados do Amazon S3 para ADLS Gen2
Cópia em massa com uma tabela de controle
Expanda a configuração para todo o seu conjunto de dados.
Quando estiver satisfeito com os resultados e o desempenho da execução, você poderá expandir a definição e o pipeline para cobrir todo o conjunto de dados.
Solucionar problemas de desempenho da atividade de cópia
Siga as etapas de ajuste de desempenho para planejar e conduzir o teste de desempenho para seu cenário. E saiba como solucionar o problema de desempenho de cada execução de atividade de cópia em Solucionar problemas de desempenho de atividade de cópia .
Copiar recursos de otimização de desempenho
O serviço fornece os seguintes recursos de otimização de desempenho:
Unidades de Integração de Dados
Escalabilidade de tempo de execução de integração auto-hospedada
Cópia paralela
Cópia encenada
Unidades de Integração de Dados
Uma Unidade de Integração de Dados (DIU) é uma medida que representa o poder de uma única unidade nos pipelines do Azure Data Factory e Synapse. A energia é uma combinação de CPU, memória e alocação de recursos de rede. A DIU aplica-se apenas ao tempo de execução de integração do Azure . A DIU não se aplica ao tempo de execução de integração auto-hospedado . Saiba mais aqui .
Escalabilidade de tempo de execução de integração auto-hospedada
Você pode querer hospedar uma carga de trabalho simultânea crescente. Ou talvez você queira obter um desempenho mais alto em seu nível de carga de trabalho atual. Você pode aprimorar a escala de processamento com as seguintes abordagens:
Você pode escalar o IR auto-hospedado, aumentando o número de trabalhos simultâneos que podem ser executados em um nó.
A escala vertical funciona apenas se o processador e a memória do nó estiverem sendo menos do que totalmente utilizados.
Você pode dimensionar o IR auto-hospedado adicionando mais nós (máquinas).
Para mais informações, veja:
Recursos de otimização de desempenho de atividade de cópia: escalabilidade de tempo de execução de integração auto-hospedada
Criar e configurar um Integration Runtime auto-hospedado: considerações de escala
Cópia paralela
Você pode definir a parallelCopiespropriedade para indicar o paralelismo que deseja que a atividade de cópia use. Pense nessa propriedade como o número máximo de encadeamentos na atividade de cópia. As roscas operam em paralelo. Os encadeamentos leem da sua fonte ou gravam nos armazenamentos de dados do coletor. Saiba mais .
Cópia encenada
Uma operação de cópia de dados pode enviar os dados diretamente para o armazenamento de dados do coletor. Como alternativa, você pode optar por usar o armazenamento de blobs como um armazenamento temporário de teste . Saiba mais .
Solucionar problemas de orquestração de pipeline e gatilhos no Azure Data Factory


APLICA-SE A: Azure Data Factory Azure Synapse Analytics
Um pipeline executado no Azure Data Factory define uma instância de execução de pipeline. Por exemplo, digamos que você tenha um pipeline que é executado às 8h, 9h e 10h. Nesse caso, há três execuções de pipeline separadas. Cada execução de pipeline tem um ID de execução de pipeline exclusivo. Um ID de execução é um identificador global exclusivo (GUID) que define essa execução de pipeline específica.
As execuções de pipeline geralmente são instanciadas passando argumentos para parâmetros que você define no pipeline. Você pode executar um pipeline manualmente ou usando um gatilho. Consulte Execução de pipeline e gatilhos no Azure Data Factory para obter detalhes.
Problemas comuns, causas e soluções
Um pipeline de aplicativo do Azure Functions gera um erro com conectividade de ponto de extremidade privado
Você tem Data Factory e um aplicativo de funções em execução em um ponto de extremidade privado no Azure. Você está tentando executar um pipeline que interage com o aplicativo de funções. Você tentou três métodos diferentes, mas um retorna o erro "Bad Request" e os outros dois métodos retornam "103 Error Forbidden".
Causa
Atualmente, o Data Factory não oferece suporte a um conector de ponto de extremidade privado para aplicativos de funções. O Azure Functions rejeita chamadas porque está configurado para permitir apenas conexões de um link privado.
Resolução
Crie um endpoint PrivateLinkService e forneça o DNS do seu aplicativo de funções.
Uma execução de pipeline é cancelada, mas o monitor ainda mostra o status do progresso
Causa
Quando você cancela uma execução de pipeline, o monitoramento de pipeline geralmente ainda mostra o status do andamento. Isso acontece devido a um problema de cache do navegador. Você também pode não ter os filtros de monitoramento corretos.
Resolução
Atualize o navegador e aplique os filtros de monitoramento corretos.
Você vê um erro "DelimitedTextMoreColumnsThanDefined" ao copiar um pipeline
Causa
Se uma pasta que você está copiando contiver arquivos com esquemas diferentes, como número variável de colunas, delimitadores diferentes, configurações de caracteres de cotação ou algum problema de dados, o pipeline do Data Factory poderá gerar este erro:
Operation on target Copy_sks failed: Failure happened on 'Sink' side. ErrorCode=DelimitedTextMoreColumnsThanDefined, 'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException, Message=Error found when processing 'Csv/Tsv Format Text' source '0_2020_11_09_11_43_32.avro' with row number 53: found more columns than expected column count 27., Source=Microsoft.DataTransfer.Common,'
Resolução
Selecione a opção Cópia Binária ao criar a atividade de Cópia. Dessa forma, para cópias em massa ou migração de seus dados de um data lake para outro, o Data Factory não abrirá os arquivos para ler o esquema. Em vez disso, o Data Factory tratará cada arquivo como binário e o copiará para o outro local.
Uma execução de pipeline falha quando você atinge o limite de capacidade do tempo de execução de integração para fluxo de dados
Emitir
Mensagem de erro:
Type=Microsoft.DataTransfer.Execution.Core.ExecutionException,Message=There are substantial concurrent MappingDataflow executions which is causing failures due to throttling under Integration Runtime 'AutoResolveIntegrationRuntime'.
Causa
Você atingiu o limite de capacidade do Integration Runtime. Você pode estar executando uma grande quantidade de fluxo de dados usando o mesmo tempo de execução de integração ao mesmo tempo. Consulte limites, cotas e restrições de assinatura e serviço do Azure para obter detalhes.
Resolução
Execute seus pipelines em diferentes tempos de disparo.
Crie um novo tempo de execução de integração e divida seus pipelines em vários tempos de execução de integração.
Um erro de execução de pipeline ao invocar a API REST em uma atividade da Web
Emitir
Mensagem de erro:
Operation on target Cancel failed: {“error”:{“code”:”AuthorizationFailed”,”message”:”The client ‘<client>’ with object id ‘<object>’ does not have authorization to perform action ‘Microsoft.DataFactory/factories/pipelineruns/cancel/action’ over scope ‘/subscriptions/<subscription>/resourceGroups/<resource group>/providers/Microsoft.DataFactory/factories/<data factory name>/pipelineruns/<pipeline run id>’ or the scope is invalid. If access was recently granted, please refresh your credentials.”}}
Causa
Os pipelines podem usar a atividade da Web para chamar métodos de API REST do ADF se e somente se o membro do Azure Data Factory for atribuído à função de Colaborador. Você deve primeiro configurar e adicionar a identidade gerenciada do Azure Data Factory à função de segurança do Colaborador.
Resolução
Antes de usar a API REST do Azure Data Factory na guia Configurações de uma atividade da Web, a segurança deve ser configurada. Os pipelines do Azure Data Factory podem usar a atividade da Web para chamar os métodos da API REST do ADF se e somente se a identidade gerenciada do Azure Data Factory for atribuída à função de Colaborador . Comece abrindo o portal do Azure e clicando no link Todos os recursos no menu à esquerda. Selecione Azure Data Factory para adicionar a identidade gerenciada do ADF com a função de Colaborador clicando no botão Adicionar na caixa Adicionar uma atribuição de função.
Como verificar e ramificar o sucesso e a falha no nível de atividade em pipelines
Causa
A orquestração do Azure Data Factory permite a lógica condicional e permite que os usuários sigam caminhos diferentes com base no resultado de uma atividade anterior. Ele permite quatro caminhos condicionais: Após Sucesso (passagem padrão), Após Falha , Após Conclusão e Após Ignorar .
O Azure Data Factory avalia o resultado de todas as atividades de nível de folha. Os resultados do pipeline são bem-sucedidos somente se todas as folhas forem bem-sucedidas. Se uma atividade folha foi ignorada, avaliamos sua atividade pai.
Resolução
Implemente verificações de nível de atividade seguindo Como lidar com falhas e erros de pipeline .
Use os Aplicativos Lógicos do Azure para monitorar pipelines em intervalos regulares após a Consulta por Fábrica .
Monitorar visualmente o pipeline
Como monitorar falhas de pipeline em intervalos regulares
Causa
Pode ser necessário monitorar pipelines de Data Factory com falha em intervalos, digamos, 5 minutos. Você pode consultar e filtrar as execuções de pipeline de uma fábrica de dados usando o ponto de extremidade.
Resolução
Você pode configurar um aplicativo lógico do Azure para consultar todos os pipelines com falha a cada 5 minutos, conforme descrito em Query By Factory . Em seguida, você pode relatar incidentes ao seu sistema de emissão de bilhetes.
Você pode executar novamente pipelines e atividades conforme descrito aqui.
Você pode executar novamente as atividades se tiver cancelado a atividade ou tiver uma falha conforme Reexecutar a partir de falhas de atividade.
Monitorar visualmente o pipeline
O aumento do grau de paralelismo não resulta em maior taxa de transferência
Causa
O grau de paralelismo no ForEach é, na verdade, o grau máximo de paralelismo. Não podemos garantir um número específico de execuções acontecendo ao mesmo tempo, mas esse parâmetro garantirá que nunca ultrapassemos o valor definido. Você deve ver isso como um limite, a ser aproveitado ao controlar o acesso simultâneo às suas fontes e coletores.
Fatos conhecidos sobre ForEach
Foreach tem uma propriedade chamada batch count(n) onde o valor padrão é 20 e o máximo é 50.
A contagem de lote, n, é usada para construir n filas.
Cada fila é executada sequencialmente, mas você pode ter várias filas em paralelo.
As filas são pré-criadas. Isso significa que não há rebalanceamento das filas durante o tempo de execução.
A qualquer momento, você tem no máximo um item sendo processado por fila. Isso significa no máximo n itens sendo processados a qualquer momento.
O tempo total de processamento foreach é igual ao tempo de processamento da fila mais longa. Isso significa que a atividade foreach depende de como as filas são construídas.
Resolução
Você não deve usar a atividade SetVariable dentro de For Each que é executada em paralelo.
Levando em consideração a forma como as filas são construídas, o cliente pode melhorar o desempenho do foreach configurando múltiplos de foreach onde cada foreach terá itens com tempo de processamento semelhante.
Isso garantirá que as execuções longas sejam processadas em paralelo e não sequencialmente.
O status do pipeline está enfileirado ou travado por muito tempo
Causa
Isso pode acontecer por vários motivos, como atingir limites de simultaneidade, interrupções de serviço, falhas de rede e assim por diante.
Resolução
Limite de simultaneidade: se o pipeline tiver uma política de simultaneidade, verifique se não há execuções de pipeline antigas em andamento.
Limites de monitoramento : vá para a tela de criação do ADF, selecione seu pipeline e determine se ele tem uma propriedade de simultaneidade atribuída a ele. Se isso acontecer, vá para a exibição Monitoramento e verifique se não há nada em andamento nos últimos 45 dias. Se houver algo em andamento, você poderá cancelá-lo e a nova execução do pipeline deverá ser iniciada.
Problemas transitórios: é possível que sua execução tenha sido afetada por um problema transitório de rede, falhas de credenciais, interrupções de serviços etc. Se isso acontecer, o Azure Data Factory tem um processo de recuperação interno que monitora todas as execuções e as inicia quando percebe que algo deu errado . Você pode executar novamente pipelines e atividades conforme descrito aqui. . Você pode executar novamente as atividades se tiver cancelado a atividade ou tiver uma falha conforme Reexecutar a partir de falhas de atividade. Esse processo acontece a cada hora, portanto, se sua execução ficar travada por mais de uma hora, crie um caso de suporte.
Tempos de inicialização mais longos para atividades no ADF Copy e Data Flow
Causa
Isso pode acontecer se você não implementou o recurso de tempo de vida para o fluxo de dados ou SHIR otimizado.
Resolução
Se cada atividade de cópia estiver demorando até 2 minutos para iniciar e o problema ocorrer principalmente em uma junção de VNet (vs. Azure IR), isso pode ser um problema de desempenho de cópia. Para revisar as etapas de solução de problemas, vá para Melhoria do desempenho da cópia.
Você pode usar o recurso time to live para diminuir o tempo de inicialização do cluster para atividades de fluxo de dados. Revise o Data Flow Integration Runtime.
Atingindo problemas de capacidade no SHIR (Self-Hosted Integration Runtime)
Causa
Isso pode acontecer se você não tiver ampliado o SHIR de acordo com sua carga de trabalho.
Resolução
Se você encontrar um problema de capacidade do SHIR, atualize a VM para aumentar o nó para equilibrar as atividades. Se você receber uma mensagem de erro sobre uma falha ou erro geral de IR auto-hospedado, uma atualização de IR auto-hospedado ou problemas de conectividade de IR auto-hospedado, que podem gerar uma longa fila, vá para Solucionar problemas de tempo de execução de integração auto-hospedado.
Mensagens de erro devido a longas filas para cópia do ADF e fluxo de dados
Causa
Mensagens de erro longas relacionadas a filas podem aparecer por vários motivos.
Resolução
Se você receber uma mensagem de erro de qualquer origem ou destino por meio de conectores, o que pode gerar uma longa fila, acesse o Guia de solução de problemas do conector.
Se você receber uma mensagem de erro sobre o Mapeamento do Fluxo de Dados, que pode gerar uma longa fila, acesse o Guia de Solução de Problemas de Fluxos de Dados.
Se você receber uma mensagem de erro sobre outras atividades, como Databricks, atividades personalizadas ou HDI, que podem gerar uma longa fila, acesse Guia de solução de problemas de atividade.
Se você receber uma mensagem de erro sobre a execução de pacotes SSIS, que podem gerar uma longa fila, acesse o Guia de solução de problemas de execução de pacote Azure-SSIS e o Guia de solução de problemas de gerenciamento de tempo de execução de integração.
Mensagem de erro - "code":"BadRequest", "message":"null"
Causa
É um erro do usuário porque a carga JSON que atinge management.azure.com está corrompida. Nenhum log será armazenado porque a chamada do usuário não alcançou a camada de serviço ADF.
Resolução
Execute o rastreamento de rede de sua chamada de API do portal ADF usando as ferramentas de desenvolvedor do navegador Edge/Chrome . Você verá a carga útil JSON ofensiva, que pode ser devido a um caractere especial (por exemplo $), espaços e outros tipos de entrada do usuário. Depois de corrigir a expressão de string, você continuará com o restante das chamadas de uso do ADF no navegador.
As atividades ForEach não são executadas no modo paralelo
Causa
Você está executando o ADF no modo de depuração.
Resolução
Execute o pipeline no modo de gatilho.
Não é possível publicar porque a conta está bloqueada
Causa
Você fez alterações na ramificação de colaboração para remover o acionador de evento de armazenamento. Você está tentando publicar e encontrar Trigger deactivation errormensagem.
Resolução
Isso ocorre porque a conta de armazenamento, usada para o gatilho de evento, está sendo bloqueada. Desbloqueie a conta.
O construtor de expressões não carrega
Causa
O construtor de expressão pode falhar ao carregar devido a problemas de rede ou cache com o navegador da web.
Resolução
Atualize o navegador da Web para a versão mais recente de um navegador compatível, limpe os cookies do site e atualize a página.
"Code":"BadRequest","message":"ErrorCode=FlowRunSizeLimitExceeded
Causa
Você encadeou muitas atividades.
Resolução
Você pode dividir seus pipelines em subpipelines e juntá-los com a atividade ExecutePipeline .
Como otimizar o pipeline com fluxos de dados de mapeamento para evitar erros internos do servidor, erros de simultaneidade etc. durante a execução
Causa
Você não otimizou o fluxo de dados de mapeamento.
Resolução
Use computação otimizada para memória ao lidar com uma grande quantidade de dados e transformações.
Reduza o tamanho do lote em caso de a para cada atividade.
Amplie seus bancos de dados e armazéns para corresponder ao desempenho de seu ADF.
Use um IR (tempo de execução de integração) separado para atividades executadas em paralelo.
Ajuste as partições na origem e afunde de acordo.
Revise as otimizações de fluxo de dados
Código de erro "BadRequest" ao passar parâmetros para pipelines filhos
Causa
O tipo de falha é um problema de configuração do usuário. String de parâmetros, em vez de Array, é passado para o pipeline filho.
Resolução
Insira a atividade de pipeline de execução para o parâmetro de pipeline como @createArray('a','b') , por exemplo, se você quiser passar os parâmetros 'a' e 'b'. Se você quiser passar números, por exemplo, use @createArray(1,2,3) . Use a função createArray para forçar a passagem de parâmetros como uma matriz.
Ao projetar um modelo de esquema em estrela para conjuntos de dados de pequeno ou médio porte, você pode usar seu banco de dados preferido, como Azure SQL. Para conjuntos de dados maiores, você pode se beneficiar da implementação de seu data warehouse no Azure Synapse Analytics em vez do SQL Server. É importante entender algumas diferenças importantes ao criar tabelas no Synapse Analytics.
No Synapse, você não tem chaves estrangeiras e restrições de valor exclusivo como no SQL Server. Como essas regras não são impostas na camada de banco de dados, os trabalhos usados ​​para carregar dados têm mais responsabilidade de manter a integridade dos dados. Você ainda tem a opção de usar índices clusterizados, mas para a maioria das tabelas de dimensão no Synapse, você se beneficiará do uso de um índice de armazenamento de colunas clusterizadas (CCI). Neste exemplo, algumas tabelas têm tipos de dados que não podem ser incluídos em um índice de armazenamento de colunas clusterizadas, portanto, um índice clusterizado foi usado.
Como o Synapse Analytics é um  sistema de processamento massivamente paralelo  (MPP), você deve considerar como os dados são distribuídos em seu design de tabela, em oposição aos sistemas de multiprocessamento simétrico (SMP), como bancos de dados OLTP, como o Banco de Dados SQL do Azure. A categoria da tabela geralmente determina qual opção escolher para distribuir a tabela.
Categoria da tabela
	Opção de distribuição recomendada
	Facto
	Use distribuição de hash com índice de armazenamento de colunas clusterizadas. O desempenho melhora quando duas tabelas de hash são unidas na mesma coluna de distribuição.


	Dimensão
	Use replicado para tabelas menores. Se as tabelas forem muito grandes para serem armazenadas em cada nó Compute, use distribuição por hash.
	Encenação
	Use round-robin para a mesa de encenação. A carga com CTAS é rápida. Quando os dados estiverem na tabela de preparo, use INSERT...SELECT para mover os dados para as tabelas de produção.
	No caso das tabelas de dimensão neste exercício, a quantidade de dados armazenados por tabela está dentro dos critérios para usar uma distribuição replicada.
Armazenamento de dados


Um data warehouse é um repositório centralizado de dados integrados de uma ou mais fontes diferentes. Os data warehouses armazenam dados atuais e históricos e são usados para geração de relatórios e análise dos dados.
 
Para mover dados para um data warehouse, os dados são extraídos periodicamente de várias fontes que contêm informações comerciais importantes. À medida que os dados são movidos, eles podem ser formatados, limpos, validados, resumidos e reorganizados. Alternativamente, os dados podem ser armazenados no nível mais baixo de detalhes, com visualizações agregadas fornecidas no warehouse para relatórios. Em ambos os casos, o data warehouse torna-se um armazenamento de dados permanente para relatórios, análises e business intelligence (BI).
Arquiteturas de armazenamento de dados
As seguintes arquiteturas de referência mostram arquiteturas de data warehouse de ponta a ponta no Azure:
Enterprise BI no Azure com Azure Synapse Analytics . Essa arquitetura de referência implementa um pipeline de extração, carregamento e transformação (ELT) que move dados de um banco de dados SQL Server local para o Azure Synapse.
BI empresarial automatizado com Azure Synapse e Azure Data Factory . Essa arquitetura de referência mostra um pipeline ELT com carregamento incremental, automatizado usando o Azure Data Factory.
Quando usar esta solução
Escolha um data warehouse quando precisar transformar grandes quantidades de dados de sistemas operacionais em um formato fácil de entender. Os data warehouses não precisam seguir a mesma estrutura de dados concisa que você pode estar usando em seus bancos de dados OLTP. Você pode usar nomes de coluna que façam sentido para usuários e analistas de negócios, reestruturar o esquema para simplificar os relacionamentos e consolidar várias tabelas em uma. Essas etapas ajudam a orientar os usuários que precisam criar relatórios e analisar os dados em sistemas de BI, sem a ajuda de um administrador de banco de dados (DBA) ou desenvolvedor de dados.
Considere usar um data warehouse quando precisar manter os dados históricos separados dos sistemas de transações de origem por motivos de desempenho. Os data warehouses facilitam o acesso a dados históricos de vários locais, fornecendo um local centralizado usando formatos, chaves e modelos de dados comuns.
Como os data warehouses são otimizados para acesso de leitura, gerar relatórios é mais rápido do que usar o sistema de transações de origem para relatórios.
Outros benefícios incluem:
O data warehouse pode armazenar dados históricos de várias fontes, representando uma única fonte de verdade.
Você pode melhorar a qualidade dos dados limpando os dados à medida que são importados para o data warehouse.
As ferramentas de relatório não competem com os sistemas transacionais para ciclos de processamento de consultas. Um data warehouse permite que o sistema transacional se concentre no tratamento de gravações, enquanto o data warehouse satisfaz a maioria das solicitações de leitura.
Um data warehouse pode consolidar dados de diferentes softwares.
As ferramentas de mineração de dados podem encontrar padrões ocultos nos dados usando metodologias automáticas.
Os data warehouses facilitam o acesso seguro a usuários autorizados, enquanto restringem o acesso a outros. Os usuários corporativos não precisam acessar os dados de origem, removendo um possível vetor de ataque.
Os data warehouses facilitam a criação de soluções de business intelligence, como cubos OLAP .
Desafios
A configuração adequada de um data warehouse para atender às necessidades do seu negócio pode trazer alguns dos seguintes desafios:
Comprometer o tempo necessário para modelar adequadamente seus conceitos de negócios. Os data warehouses são orientados por informações. Você deve padronizar os termos relacionados aos negócios e os formatos comuns, como moeda e datas. Você também precisa reestruturar o esquema de uma maneira que faça sentido para os usuários de negócios, mas ainda garanta a precisão dos agregados e relacionamentos de dados.
Planejando e configurando sua orquestração de dados. Considere como copiar dados do sistema transacional de origem para o data warehouse e quando mover dados históricos de armazenamentos de dados operacionais para o warehouse.
Manter ou melhorar a qualidade dos dados limpando os dados à medida que são importados para o warehouse.
Armazenamento de dados no Azure
Você pode ter uma ou mais fontes de dados, seja de transações de clientes ou aplicativos de negócios. Esses dados são tradicionalmente armazenados em um ou mais OLTPbancos de dados. Os dados podem ser persistidos em outras mídias de armazenamento, como compartilhamentos de rede, Azure Storage Blobs ou um data lake. Os dados também podem ser armazenados pelo próprio data warehouse ou em um banco de dados relacional, como o Banco de Dados SQL do Azure. A finalidade da camada de armazenamento de dados analíticos é atender às consultas emitidas por ferramentas de análise e relatórios no data warehouse. No Azure, esse recurso de armazenamento analítico pode ser atendido com o Azure Synapse ou com o Azure HDInsight usando Hive ou Consulta Interativa. Além disso, você precisará de algum nível de orquestração para mover ou copiar dados do armazenamento de dados para o data warehouse, o que pode ser feito usando Azure Data Factory ou Oozie no Azure HDInsight.
Existem várias opções para implementar um data warehouse no Azure, dependendo das suas necessidades. As listas a seguir são divididas em duas categorias, multiprocessamento simétrico (SMP) e processamento paralelo massivo (MPP).
SMP:
Banco de Dados SQL do Azure
SQL Server em uma máquina virtual
MPP:
Azure Synapse Analytics (anteriormente Azure Data Warehouse)
Apache Hive no HDInsight
Consulta interativa (Hive LLAP) no HDInsight
Como regra geral, os armazéns baseados em SMP são mais adequados para conjuntos de dados pequenos e médios (até 4-100 TB), enquanto o MPP é frequentemente usado para big data. A delimitação entre pequenos/médios e grandes volumes de dados tem a ver, em parte, com a definição e a infraestrutura de suporte da sua organização. (Consulte Escolhendo um armazenamento de dados OLTP .)
Além do tamanho dos dados, o tipo de padrão de carga de trabalho provavelmente será um fator determinante maior. Por exemplo, consultas complexas podem ser muito lentas para uma solução SMP e, em vez disso, exigir uma solução MPP. Os sistemas baseados em MPP geralmente têm uma penalidade de desempenho com tamanhos de dados pequenos, devido à forma como os trabalhos são distribuídos e consolidados entre os nós. Se o tamanho dos seus dados já excede 1 TB e espera-se que cresça continuamente, considere selecionar uma solução MPP. No entanto, se os tamanhos dos seus dados forem menores, mas suas cargas de trabalho estiverem excedendo os recursos disponíveis de sua solução SMP, o MPP também pode ser sua melhor opção.
Os dados acessados ou armazenados por seu data warehouse podem vir de várias fontes de dados, incluindo um data lake, como Azure Data Lake Storage . Para uma sessão de vídeo que compara os diferentes pontos fortes dos serviços MPP que podem usar o Azure Data Lake, consulte Azure Data Lake e Azure Data Warehouse: aplicando práticas modernas ao seu aplicativo .
Os sistemas SMP são caracterizados por uma única instância de um sistema de gerenciamento de banco de dados relacional compartilhando todos os recursos (CPU/memória/disco). Você pode escalar um sistema SMP. Para o SQL Server em execução em uma VM, você pode aumentar o tamanho da VM. Para o Banco de Dados SQL do Azure, você pode escalar verticalmente selecionando uma camada de serviço diferente.
Os sistemas MPP podem ser expandidos adicionando mais nós de computação (que têm seus próprios subsistemas de CPU, memória e E/S). Existem limitações físicas para dimensionar um servidor, ponto em que a expansão é mais desejável, dependendo da carga de trabalho. No entanto, as diferenças na consulta, modelagem e particionamento de dados significam que as soluções MPP exigem um conjunto de habilidades diferente.
Ao decidir qual solução SMP usar, confira Uma visão mais detalhada do Banco de Dados SQL do Azure e do SQL Server em VMs do Azure .
O Azure Synapse (anteriormente Azure SQL Data Warehouse) também pode ser usado para conjuntos de dados pequenos e médios, onde a carga de trabalho é intensiva em computação e memória. Leia mais sobre padrões do Azure Synapse e cenários comuns:
Padrões e antipadrões de carga de trabalho do Azure SQL Data Warehouse
Padrões e estratégias de carregamento do Azure SQL Data Warehouse
Migrando dados para o Azure SQL Data Warehouse na prática
Padrões de aplicativos ISV comuns usando o Azure SQL Data Warehouse
Principais critérios de seleção
Para restringir as opções, comece respondendo a estas perguntas:
Você quer um serviço gerenciado em vez de gerenciar seus próprios servidores?
Você está trabalhando com conjuntos de dados extremamente grandes ou consultas altamente complexas e de longa duração? Se sim, considere uma opção MPP.
Para um grande conjunto de dados, a fonte de dados é estruturada ou não estruturada? Dados não estruturados podem precisar ser processados em um ambiente de big data, como Spark no HDInsight, Azure Databricks, Hive LLAP no HDInsight ou Azure Data Lake Analytics. Todos eles podem servir como motores ELT (Extrair, Carregar, Transformar) e ETL (Extrair, Transformar, Carregar). Eles podem produzir os dados processados em dados estruturados, facilitando o carregamento no Azure Synapse ou em uma das outras opções. Para dados estruturados, o Azure Synapse tem um nível de desempenho chamado Otimizado para Computação, para cargas de trabalho com uso intensivo de computação que exigem desempenho ultra-alto.
Deseja separar seus dados históricos de seus dados operacionais atuais? Em caso afirmativo, selecione uma das opções em que a orquestração é necessária. Esses são armazéns autônomos otimizados para acesso de leitura pesado e são mais adequados como um armazenamento de dados históricos separado.
Você precisa integrar dados de várias fontes, além do armazenamento de dados OLTP? Em caso afirmativo, considere opções que integrem facilmente várias fontes de dados.
Você tem um requisito de multilocação? Nesse caso, o Azure Synapse não é ideal para esse requisito. Para obter mais informações, consulte Padrões e antipadrões do Azure Synapse .
Você prefere um armazenamento de dados relacional? Nesse caso, escolha uma opção com um armazenamento de dados relacional, mas observe também que você pode usar uma ferramenta como o PolyBase para consultar armazenamentos de dados não relacionais, se necessário. Se você decidir usar o PolyBase, no entanto, execute testes de desempenho em seus conjuntos de dados não estruturados para sua carga de trabalho.
Você tem requisitos de relatórios em tempo real? Se você precisar de tempos de resposta de consulta rápidos em grandes volumes de inserções singleton, escolha uma opção que suporte relatórios em tempo real.
Você precisa dar suporte a um grande número de usuários e conexões simultâneos? A capacidade de oferecer suporte a vários usuários/conexões simultâneos depende de vários fatores.
Para o Banco de Dados SQL do Azure, consulte os limites de recursos documentados com base na sua camada de serviço.
O SQL Server permite um máximo de 32.767 conexões de usuário. Ao executar em uma VM, o desempenho dependerá do tamanho da VM e de outros fatores.
O Azure Synapse tem limites para consultas e conexões simultâneas. Para obter mais informações, consulte Simultaneidade e gerenciamento de carga de trabalho no Azure Synapse . Considere usar serviços complementares, como Azure Analysis Services , para superar os limites no Azure Synapse.
Que tipo de carga de trabalho você tem? Em geral, as soluções de armazém baseadas em MPP são mais adequadas para cargas de trabalho analíticas orientadas a lotes. Se suas cargas de trabalho são transacionais por natureza, com muitas pequenas operações de leitura/gravação ou várias operações de linha por linha, considere usar uma das opções de SMP. Uma exceção a essa diretriz é ao usar o processamento de fluxo em um cluster HDInsight, como o Spark Streaming, e armazenar os dados em uma tabela Hive.
Matriz de Capacidade
As tabelas a seguir resumem as principais diferenças nos recursos.
Recursos gerais
RECURSOS GERAIS
Capacidade Banco de Dados SQL do Azure SQL Server (VM) Azure Synapse Apache Hive no HDInsight Hive LLAP no HDInsight
É serviço gerenciado sim Não sim Sim 1 Sim 1
Requer orquestração de dados (contém cópia de dados/dados históricos) Não Não sim sim sim
Integre facilmente várias fontes de dados Não Não sim sim sim
Suporta pausa de computação Não Não sim Nº 2 Nº 2
Armazenamento de dados relacionais sim sim sim Não Não
Relatórios em tempo real sim sim Não Não sim
Pontos de restauração de backup flexíveis sim sim Nº 3 Sim 4 Sim 4
SMP/MPP SMP SMP MPP MPP MPP
[1] Configuração e dimensionamento manual.
[2] Os clusters HDInsight podem ser excluídos quando não forem necessários e, em seguida, recriados. Anexe um armazenamento de dados externo ao seu cluster para que seus dados sejam retidos quando você excluir seu cluster. Você pode usar o Azure Data Factory para automatizar o ciclo de vida do cluster criando um cluster HDInsight sob demanda para processar sua carga de trabalho e excluí-lo assim que o processamento for concluído.
[3] Com o Azure Synapse, você pode restaurar um banco de dados para qualquer ponto de restauração disponível nos últimos sete dias. Os instantâneos começam a cada quatro a oito horas e ficam disponíveis por sete dias. Quando um instantâneo tem mais de sete dias, ele expira e seu ponto de restauração não está mais disponível.
[4] Considere usar um metastore Hive externo que pode ser copiado e restaurado conforme necessário. As opções padrão de backup e restauração que se aplicam ao Blob Storage ou Data Lake Storage podem ser usadas para os dados, ou soluções de backup e restauração HDInsight de terceiros, como Imanis Data , podem ser usadas para maior flexibilidade e facilidade de uso.
Recursos de escalabilidade
RECURSOS DE ESCALABILIDADE
Capacidade Banco de Dados SQL do Azure SQL Server (VM) Azure Synapse Apache Hive no HDInsight Hive LLAP no HDInsight
Servidores regionais redundantes para alta disponibilidade sim sim sim Não Não
Suporta expansão de consulta (consultas distribuídas) Não Não sim sim sim
Escalabilidade dinâmica sim Não Sim 1 Não Não
Suporta cache de dados na memória sim sim sim sim sim
[1] O Azure Synapse permite que você aumente ou diminua a escala ajustando o número de unidades de data warehouse (DWUs). Consulte Gerenciar o poder de computação no Azure Synapse .
Recursos de segurança
RECURSOS DE SEGURANÇA
Capacidade Banco de Dados SQL do Azure SQL Server em uma máquina virtual Azure Synapse Apache Hive no HDInsight Hive LLAP no HDInsight
Autenticação SQL/Azure Active Directory (Azure AD) SQL / Azure AD / Active Directory SQL/Azure AD local/Azure AD 1 local/Azure AD 1
Autorização sim sim sim sim Sim 1
Auditoria sim sim sim sim Sim 1
Criptografia de dados em repouso Sim 2 Sim 2 Sim 2 Sim 2 Sim 1
Segurança em nível de linha sim sim sim Não Sim 1
Suporta firewalls sim sim sim sim Sim 3
Mascaramento de dados dinâmicos sim sim sim Não

Structured data
In relational database systems like Microsoft SQL Server, Azure SQL Database, and Azure SQL Data Warehouse, data structure is defined at design time. Data structure is designed in the form of tables. This means it's designed before any information is loaded into the system. The data structure includes the relational model, table structure, column width, and data types.
Relational systems react slowly to changes in data requirements because the structural database needs to change every time a data requirement changes. When new columns are added, you might need to bulk-update all existing records to populate the new column throughout the table.
Relational systems typically use a querying language such as Transact-SQL (T-SQL).
Nonstructured data
Examples of nonstructured data include binary, audio, and image files. Nonstructured data is stored in nonrelational systems, commonly called unstructured or NoSQL systems. In nonrelational systems, the data structure isn't defined at design time, and data is typically loaded in its raw format. The data structure is defined only when the data is read. The difference in the definition point gives you flexibility to use the same source data for different outputs. Nonrelational systems can also support semistructured data such as JSON file formats.
The open-source world offers four types of NoSQL databases:
Key-value store: Stores key-value pairs of data in a table structure.
Document database: Stores documents that are tagged with metadata to aid document searches.
Graph database: Finds relationships between data points by using a structure that's composed of vertices and edges.
Column database: Stores data based on columns rather than rows. Columns can be defined at the query's runtime, allowing flexibility in the data that's returned performantly.
Now that we've reviewed data types, let's look at common data platform technologies that facilitate the storage, processing, and querying of these data types.

Compreender o armazenamento de dados no Armazenamento do Azure
Concluído
100 EXP
5 minutos
As contas de armazenamento do Azure são o tipo de armazenamento básico no Azure. O Armazenamento do Azure oferece um armazenamento de objetos escalável para objetos de dados e serviços de sistema de arquivos na nuvem. Ele também pode fornecer um repositório de mensagens para mensagens confiáveis ​​ou pode atuar como um repositório NoSQL.

O Armazenamento do Azure oferece quatro opções de configuração:

Blob do Azure : um armazenamento de objeto escalonável para texto e dados binários
Arquivos do Azure : compartilhamentos de arquivos gerenciados para implantações na nuvem ou locais
Fila do Azure : um repositório de mensagens para mensagens confiáveis ​​entre os componentes do aplicativo
Tabela do Azure : um armazenamento NoSQL para armazenamento sem esquema de dados estruturados
Você pode usar o Armazenamento do Azure como base de armazenamento ao provisionar uma tecnologia de plataforma de dados, como Azure Data Lake Storage e HDInsight. Mas você também pode provisionar o Armazenamento do Azure para uso autônomo. Por exemplo, você provisiona um armazenamento de Blobs do Azure como armazenamento padrão na forma de armazenamento em disco magnético ou como armazenamento premium na forma de unidades de estado sólido (SSDs).

As definições a seguir se concentram no armazenamento de Blobs do Azure.

Quando usar o armazenamento de blobs
Se você precisar provisionar um armazenamento de dados que armazenará, mas não consultará dados, sua opção mais barata será configurar uma conta de armazenamento como um armazenamento de Blobs. O armazenamento de blobs funciona bem com imagens e dados não estruturados e é a maneira mais barata de armazenar dados no Azure.

Características principais
As contas de armazenamento do Azure são escalonáveis ​​e seguras, duráveis ​​e altamente disponíveis. O Azure lida com a manutenção de hardware, atualizações e problemas críticos. Ele também fornece APIs REST e SDKs para Armazenamento do Azure em vários idiomas. As linguagens, ambientes de tempo de execução e plataformas de desenvolvedor compatíveis incluem .NET, Node.js, Java, Python, PHP, Ruby e Go. O Armazenamento do Azure também dá suporte a scripts no Azure PowerShell e na CLI do Azure.

Ingestão de dados
Para ingerir dados em seu sistema, use Azure Data Factory, Gerenciador de Armazenamento, a ferramenta AzCopy, PowerShell ou Visual Studio. Se você usar o recurso de upload de arquivo para importar tamanhos de arquivo acima de 2 GB, use o PowerShell ou o Visual Studio. AzCopy oferece suporte a um tamanho máximo de arquivo de 1 TB e divide automaticamente arquivos de dados que excedem 200 GB.

Segurança de dados
O Armazenamento do Azure criptografa todos os dados gravados nele. O Armazenamento do Azure também fornece controle refinado sobre quem tem acesso aos seus dados. Você protegerá os dados usando chaves ou assinaturas de acesso compartilhado.

O Azure Resource Manager fornece um modelo de permissões que usa o controle de acesso baseado em função (RBAC). Use essa funcionalidade para definir permissões e atribuir funções a usuários, grupos ou aplicativos.


Compreender o armazenamento de dados no Azure Data Lake Storage
Concluído
100 EXP
5 minutos
O Azure Data Lake Storage é um repositório de dados compatível com Hadoop que pode armazenar qualquer tamanho ou tipo de dados. Este serviço de armazenamento está disponível como Geração 1 (Gen1) ou Geração 2 (Gen2). Os usuários do Data Lake Storage Gen1 não precisam atualizar para Gen2, mas renunciam a alguns benefícios.

Os usuários do Data Lake Storage Gen2 aproveitam o armazenamento de BLOBs do Azure, um sistema de arquivos hierárquico e o ajuste de desempenho que os ajuda a processar soluções de análise de big data. Na Gen2, os desenvolvedores podem acessar dados por meio da API Blob ou da API de arquivo Data Lake. Gen2 também pode atuar como uma camada de armazenamento para uma ampla variedade de plataformas de computação, incluindo Azure Databricks, Hadoop e Azure HDInsight, mas os dados não precisam ser carregados nas plataformas.

Onde usar o Data Lake Storage Gen2
O Data Lake Storage foi projetado para armazenar grandes quantidades de dados para análises de big data. Por exemplo, a Contoso Life Sciences é um centro de pesquisa de câncer que analisa petabytes de dados genéticos, dados de pacientes e registros de dados de amostras relacionadas. O Data Lake Storage Gen2 reduz os tempos de computação, tornando a pesquisa mais rápida e menos cara.

O aspecto de computação que fica acima desse armazenamento pode variar. O aspecto pode incluir plataformas como HDInsight, Hadoop e Azure Databricks.

Características principais
Aqui estão os principais recursos do Data Lake Storage:

Escalabilidade ilimitada
Compatibilidade com Hadoop
Suporte de segurança para ambas as listas de controle de acesso (ACLs)
Conformidade POSIX
Um driver otimizado do Azure Blob File System (ABFS) projetado para análise de big data
Armazenamento com redundância de zona
Armazenamento com redundância geográfica
Ingestão de dados
Para ingerir dados em seu sistema, use Azure Data Factory, Apache Sqoop, Gerenciador de Armazenamento do Azure, a ferramenta AzCopy, PowerShell ou Visual Studio. Para usar o recurso de upload de arquivo para importar tamanhos de arquivo acima de 2 GB, use o PowerShell ou o Visual Studio. AzCopy oferece suporte a um tamanho máximo de arquivo de 1 TB e divide automaticamente arquivos de dados que excedem 200 GB.

Consultas
No Data Lake Storage Gen1, os engenheiros de dados consultam dados usando a linguagem U-SQL. Na geração 2, use a API do Armazenamento de Blobs do Azure ou a API do Azure Data Lake System (ADLS).

Segurança de dados
Como o Data Lake Storage dá suporte a ACLs do Azure Active Directory, os administradores de segurança podem controlar o acesso a dados usando os conhecidos Grupos de Segurança do Active Directory. O controle de acesso baseado em função (RBAC) está disponível em Gen1 e Gen2. Os grupos de segurança integrados incluem ReadOnlyUsers, WriteAccessUsers e FullAccessUsers.

Habilite o firewall para limitar o tráfego apenas aos serviços do Azure. O Data Lake Storage criptografa automaticamente os dados em repouso, protegendo a privacidade dos dados.

Compreender o Azure Cosmos DB
Concluído
100 EXP
5 minutos
O Azure Cosmos DB é um banco de dados multimodelo distribuído globalmente. Você pode implantá-lo usando vários modelos de API:

API SQL
API do MongoDB
API do Cassandra
API Gremlin
API de tabela
Devido à arquitetura multimodelo do Azure Cosmos DB, você se beneficia dos recursos inerentes de cada modelo. Por exemplo, você pode usar MongoDB para dados semiestruturados, Cassandra para colunas largas ou Gremlin para bancos de dados gráficos. Quando você move seus dados do SQL, MongoDB ou Cassandra para o Azure Cosmos DB, os aplicativos criados usando as APIs do SQL, MongoDB ou Cassandra continuarão a operar.

 Observação

Para obter mais informações sobre as APIs disponíveis no Azure Cosmos DB, consulte Escolher a API apropriada para armazenamento do Azure Cosmos DB .

Quando usar o Azure Cosmos DB
Implante o Azure Cosmos DB quando precisar de um banco de dados NoSQL do modelo de API com suporte, em escala planetária e com desempenho de baixa latência. Atualmente, o Azure Cosmos DB dá suporte ao tempo de atividade de cinco noves (99,999%). Ele pode suportar tempos de resposta abaixo de 10 ms quando for provisionado corretamente.

Considere este exemplo em que o Azure Cosmos DB ajuda a resolver um problema comercial. A Contoso é uma varejista de comércio eletrônico com sede em Manchester, Reino Unido. A empresa vende brinquedos infantis. Depois de analisar os relatórios do Power BI, os gerentes da Contoso notaram uma diminuição significativa nas vendas na Austrália. Os gerentes analisam casos de atendimento ao cliente no Dynamics 365 e veem muitas reclamações de clientes australianos de que o carrinho de compras do site está expirando.

O gerente de operações de rede da Contoso confirma o problema. É que o único data center da empresa está localizado em Londres. A distância física para a Austrália está causando atrasos. A Contoso aplica uma solução que usa o datacenter Microsoft Australia East para fornecer uma versão local dos dados aos usuários na Austrália. A Contoso migra seu banco de dados SQL local para o Azure Cosmos DB usando a API SQL. Esta solução melhora o desempenho para usuários australianos. Os dados podem ser armazenados no Reino Unido e replicados para a Austrália para melhorar os tempos de processamento.

Características principais
O Azure Cosmos DB dá suporte a 99,999% de tempo de atividade. Você pode invocar um failover regional usando a programação ou o portal do Azure. Um banco de dados do Azure Cosmos DB fará failover automaticamente se houver um desastre regional.

Ao usar a replicação multimestre, o Azure Cosmos DB garante um tempo de resposta inferior a 10 ms para leituras e gravações.

Para manter a consistência dos dados no Azure Cosmos DB, sua equipe de engenharia deve apresentar um novo conjunto de níveis de consistência que atendam aos desafios exclusivos das soluções em escala planetária. Os níveis de consistência no Azure Cosmos DB incluem:

Forte
Inatividade limitada
Sessão
Prefixo consistente
Eventual
Ingestão de dados
Para ingerir dados no Azure Cosmos DB, use o Azure Data Factory, crie um aplicativo que grave dados no Azure Cosmos DB por meio de sua API, carregue documentos JSON ou edite diretamente o documento.

Consultas
Como engenheiro de dados, você pode criar procedimentos armazenados, gatilhos e funções definidas pelo usuário (UDFs). Ou use a API de consulta JavaScript. Você também encontrará outros métodos para consultar as outras APIs no Azure Cosmos DB. Por exemplo, no componente Data Explorer, você pode usar o painel de visualização de gráfico.

Segurança de dados
O Azure Cosmos DB dá suporte à criptografia de dados, configurações de firewall IP e acesso de redes virtuais. Os dados são criptografados automaticamente. A autenticação do usuário é baseada em tokens e o Azure Active Directory fornece segurança baseada em função.

O Azure Cosmos DB atende a muitas certificações de conformidade de segurança, incluindo HIPAA, FedRAMP, SOX e HITRUST.

Próxima unidade: Entender o Banco de Dados SQL do Azure

Entenda o Banco de Dados SQL do Azure
Concluído
100 EXP
5 minutos
O Banco de Dados SQL do Azure é um serviço de banco de dados relacional gerenciado. Ele suporta estruturas como dados relacionais e formatos não estruturados, como dados espaciais e XML. O Banco de Dados SQL fornece processamento de transações online (OLTP) que pode ser dimensionado sob demanda. Você também encontrará a segurança e a disponibilidade abrangentes que você aprecia nos serviços de banco de dados do Azure.

 Importante

Esta unidade se concentra no Banco de Dados SQL do Azure, a oferta de banco de dados de plataforma como serviço (PaaS). Aqui, não abordamos instâncias do Microsoft SQL Server instaladas localmente ou em uma máquina virtual (VM) do Azure. Embora sejam semelhantes, a configuração e os benefícios do Microsoft SQL Server diferem dos do Banco de Dados SQL do Azure.

Quando usar o Banco de Dados SQL
Use o Banco de Dados SQL quando precisar aumentar e diminuir sistemas OLTP sob demanda. O Banco de Dados SQL é uma boa solução quando sua organização deseja aproveitar os recursos de segurança e disponibilidade do Azure. As organizações que escolhem o Banco de Dados SQL também evitam os riscos de gastos de capital e de aumentar os gastos operacionais em sistemas locais complexos.

O Banco de Dados SQL pode ser mais flexível do que uma solução SQL Server local porque você pode provisioná-lo e configurá-lo em minutos. Ainda mais, o Banco de Dados SQL é apoiado pelo contrato de nível de serviço (SLA) do Azure.

Características principais
O Banco de Dados SQL oferece desempenho previsível para vários tipos de recursos, camadas de serviço e tamanhos de computação. Quase sem necessidade de administração, oferece escalabilidade dinâmica sem tempo de inatividade, otimização inteligente integrada, escalabilidade e disponibilidade globais e opções avançadas de segurança. Esses recursos permitem que você se concentre no desenvolvimento rápido de aplicativos e na aceleração do tempo de lançamento no mercado. Você não precisa mais dedicar tempo e recursos preciosos ao gerenciamento de máquinas virtuais e infraestrutura.

Ingestão e processamento de dados
O Banco de Dados SQL pode ingerir dados por meio da integração de aplicativos de uma ampla variedade de SDKs de desenvolvedor. As linguagens de programação suportadas, ambientes de tempo de execução e plataformas de desenvolvedor incluem .NET, Node.js, Python e Java. Além dos aplicativos, você também pode ingerir dados por meio de técnicas Transact-SQL (T-SQL) e da movimentação de dados usando o Azure Data Factory.

Consultas
Use T-SQL para consultar o conteúdo de um Banco de Dados SQL. Esse método se beneficia de uma ampla variedade de recursos SQL padrão para filtrar, ordenar e projetar os dados no formato que você precisa.

Segurança de dados
O Banco de Dados SQL fornece uma variedade de recursos internos de segurança e conformidade. Esses recursos ajudam seu aplicativo a atender a requisitos de segurança e conformidade como estes:

Proteção avançada contra ameaças
Auditoria de banco de dados SQL
Criptografia de dados
Autenticação do Azure Active Directory
Autenticação multifator
Certificação de conformidade
Próxima unidade: Entenda o Azure Synapse Analytics

Entenda o Azure Synapse Analytics
Concluído
100 EXP
5 minutos
O Azure Synapse Analytics é uma plataforma de dados baseada em nuvem que reúne armazenamento de dados corporativos e análise de Big Data. Ele pode processar grandes quantidades de dados e responder a questões de negócios complexas com escala ilimitada.

Quando usar o Azure Synapse Analytics
As cargas de dados podem aumentar o tempo de processamento para soluções analíticas descritivas de armazenamento de dados no local. As organizações que enfrentam esse problema podem procurar uma alternativa baseada em nuvem para reduzir o tempo de processamento e liberar relatórios de business intelligence mais rapidamente. Mas muitas organizações consideram primeiro a expansão de servidores locais. À medida que essa abordagem atinge seus limites físicos, eles procuram uma solução em uma escala de petabytes que não envolva instalações e configurações complexas. O recurso SQL Pools do Azure Synapse Analytics pode atender a essa necessidade.

O volume e a variedade de dados que estão sendo gerados estão oferecendo oportunidades para realizar diferentes tipos de análise nos dados. Isso pode incluir técnicas como análise exploratória de dados para identificar padrões iniciais ou significados nos dados. Também pode incluir a realização de análises preditivas para previsão ou segmentação de dados. O recurso Big Data Analytics do Azure Synapse Analytics acomodará isso.

Características principais
Os Pools SQL usam o processamento paralelo em massa (MPP) para executar consultas rapidamente em petabytes de dados. Como o armazenamento é separado dos nós de computação, você pode dimensionar os nós de computação de forma independente para atender a qualquer demanda a qualquer momento.

No Azure Synapse Analytics, o Data Movement Service (DMS) coordena e transporta dados entre nós de computação conforme necessário. Mas você pode usar uma tabela replicada para reduzir a movimentação de dados e melhorar o desempenho. O Azure Synapse Analytics dá suporte a três tipos de tabelas distribuídas: hash, round-robin e replicado. Use essas tabelas para ajustar o desempenho.

É importante ressaltar que o Azure Synapse Analytics também pode pausar e retomar a camada de computação. Isso significa que você paga apenas pelo cálculo que usar. Esse recurso é útil no armazenamento de dados.

Ingestão e processamento de dados
O Azure Synapse Analytics usa a abordagem de extração, carregamento e transformação (ELT) para dados em massa. Os profissionais de SQL já estão familiarizados com ferramentas de cópia em massa, como bcp e a API SQLBulkCopy. Os engenheiros de dados que trabalham com o Azure Synapse Analytics aprenderão em breve com que rapidez o PolyBase pode carregar dados.

PolyBase é uma tecnologia que elimina a complexidade para engenheiros de dados. Eles aproveitam as técnicas de ingestão e processamento de big data transferindo cálculos complexos para a nuvem. Os desenvolvedores usam o PolyBase para aplicar procedimentos armazenados, rótulos, exibições e SQL em seus aplicativos. Você também pode usar o Azure Data Factory para ingerir e processar dados usando o PolyBase também.

Consultas
Como engenheiro de dados, você pode usar o Transact-SQL familiar para consultar o conteúdo do Azure Synapse Analytics. Esse método tira proveito de uma ampla variedade de recursos, incluindo as cláusulas WHERE, ORDER BY e GROUP BY. Carregue dados rapidamente usando o PolyBase com construções Transact-SQL adicionais, como CREATE TABLE e SELECT.

Segurança de dados
O Azure Synapse Analytics dá suporte à autenticação do SQL Server e ao Azure Active Directory. Para ambientes de alta segurança, configure a autenticação multifator. De uma perspectiva de dados, o Azure Synapse Analytics dá suporte à segurança no nível de colunas e linhas.

Próxima unidade: Entenda o Azure Stream Analytics

Entenda o Azure Stream Analytics
Concluído
100 EXP
8 minutos
Aplicativos, sensores, dispositivos de monitoramento e gateways transmitem dados de eventos contínuos conhecidos como fluxos de dados . Os dados de streaming são de alto volume e têm uma carga útil mais leve do que os sistemas sem streaming.

Os engenheiros de dados usam o Azure Stream Analytics para processar dados de streaming e responder a anomalias de dados em tempo real. Você pode usar o Stream Analytics para monitoramento da Internet das Coisas (IoT), logs da Web, monitoramento remoto de pacientes e sistemas de ponto de venda (POS).

Quando usar o Stream Analytics
Se sua organização precisa responder a eventos de dados em tempo real ou analisar grandes lotes de dados em um fluxo contínuo com limite de tempo, o Stream Analytics é uma boa solução. Sua organização deve decidir se trabalha com dados de streaming ou dados em lote.

Em tempo real, os dados são ingeridos de aplicativos ou dispositivos e gateways IoT em um hub de eventos ou hub IoT. O hub de eventos ou hub IoT transmite os dados para o Stream Analytics para análise em tempo real.

Os sistemas em lote processam grupos de dados armazenados em um repositório de Blobs do Azure. Eles fazem isso em um único trabalho executado em um intervalo predefinido. Não use sistemas em lote para sistemas de business intelligence que não podem tolerar o intervalo predefinido. Por exemplo, um veículo autônomo não pode esperar que um sistema em lote ajuste sua direção. Da mesma forma, um sistema de detecção de fraudes deve recusar uma transação financeira questionável em tempo real.

Ingestão de dados
Como engenheiro de dados, configure a ingestão de dados no Stream Analytics configurando entradas de dados de fontes de integração de primeira classe. Essas fontes incluem Hubs de Eventos do Azure, Hub IoT do Azure e armazenamento de Blobs do Azure.

Um hub IoT é o gateway de nuvem que conecta dispositivos IoT. Os hubs IoT coletam dados para gerar insights e automação de negócios.

Os recursos do Hub IoT do Azure enriquecem a relação entre seus dispositivos e seus sistemas de back-end. Os recursos de comunicação bidirecional significam que, enquanto você recebe dados de dispositivos, também pode enviar comandos e políticas de volta aos dispositivos. Aproveite essa capacidade, por exemplo, para atualizar propriedades ou invocar ações de gerenciamento de dispositivos. O Hub IoT do Azure também pode autenticar o acesso entre o dispositivo IoT e o hub IoT.

Os Hubs de Eventos do Azure fornecem serviços de streaming de big data. Ele foi projetado para alta taxa de transferência de dados, permitindo que os clientes enviem bilhões de solicitações por dia. Os Hubs de Eventos usam um modelo de consumidor particionado para dimensionar seu fluxo de dados. Este serviço está integrado aos serviços de big data e análise do Azure. Isso inclui Databricks, Stream Analytics, Azure Data Lake Storage e HDInsight. Os Hubs de Eventos fornecem autenticação por meio de uma chave compartilhada.

Você pode usar o Armazenamento do Azure para armazenar dados antes de processá-los em lotes.

Processamento de dados
Para processar dados de streaming, configure trabalhos do Stream Analytics com pipelines de entrada e saída. As entradas são fornecidas pelos Hubs de Eventos, Hubs IoT ou Armazenamento do Azure. O Stream Analytics pode rotear a saída do trabalho para muitos sistemas de armazenamento. Esses sistemas incluem Azure Blob, Azure SQL Database, Azure Data Lake Storage e Azure Cosmos DB.

Depois de armazenar os dados, execute a análise em lote no Azure HDInsight. Ou envie a saída para um serviço como Hubs de Eventos para consumo. Ou use a API de streaming do Power BI para enviar a saída ao Power BI para visualização em tempo real.

Consultas
Para definir transformações de trabalho, use uma linguagem de consulta simples e declarativa do Stream Analytics. A linguagem deve permitir que você use construções SQL simples para escrever análises e consultas temporais complexas.

A linguagem de consulta do Stream Analytics é consistente com a linguagem SQL. Se você estiver familiarizado com a linguagem SQL, poderá começar a criar trabalhos.

Segurança de dados
O Stream Analytics lida com a segurança na camada de transporte entre o dispositivo e o Hub IoT do Azure. Os dados de streaming geralmente são descartados após a conclusão das operações de janela. Os Hubs de Eventos usam uma chave compartilhada para proteger a transferência de dados. Se você deseja armazenar os dados, seu dispositivo de armazenamento fornecerá segurança.

Próxima unidade: entender o Azure HDInsight

Entenda o Azure HDInsight
Concluído
100 EXP
5 minutos
O Azure HDInsight fornece tecnologias para ajudá-lo a ingerir, processar e analisar big data. Ele suporta processamento em lote, armazenamento de dados, IoT e ciência de dados.

Características principais
O HDInsight é uma solução de nuvem de baixo custo. Inclui Apache Hadoop, Spark, Kafka, HBase, Storm e Interactive Query.

O Hadoop inclui Apache Hive, HBase, Spark e Kafka. O Hadoop armazena dados em um sistema de arquivos (HDFS). O Spark armazena dados na memória. Essa diferença no armazenamento torna o Spark cerca de 100 vezes mais rápido.
HBase é um banco de dados NoSQL construído no Hadoop. É comumente usado para motores de busca. O HBase oferece failover automático.
Storm é uma solução de análise de streaming em tempo real distribuída.
Kafka é uma plataforma de código aberto usada para compor pipelines de dados. Ele oferece a funcionalidade de fila de mensagens, que permite que os usuários publiquem ou assinem fluxos de dados em tempo real.
Como ingerir dados
Como engenheiro de dados, use o Hive para executar operações de ETL nos dados que você está ingerindo. Ou orquestre consultas do Hive no Azure Data Factory.

Processamento de dados
No Hadoop, use Java e Python para processar big data. O Mapper consome e analisa os dados de entrada. Em seguida, ele emite tuplas que o Reducer pode analisar. O Reducer executa operações de resumo para criar um conjunto de resultados combinado menor.

O Spark processa os fluxos usando o Spark Streaming. Para aprendizado de máquina, use as 200 bibliotecas Anaconda pré-carregadas com Python. Use GraphX ​​para cálculos de gráfico.

Os desenvolvedores podem enviar e monitorar remotamente trabalhos do Spark. Storm suporta linguagens de programação comuns como Java, C# e Python.

Consultas
O Hadoop suporta as linguagens Pig e HiveQL. No Spark, os engenheiros de dados usam o Spark SQL.

Segurança de dados
O Hadoop dá suporte à criptografia, Secure Shell (SSH), assinaturas de acesso compartilhado e segurança do Azure Active Directory.

Próxima unidade: entender outros serviços de dados do Azure

Entenda outros serviços de dados do Azure
Concluído
100 EXP
5 minutos
Para completar sua compreensão das ofertas na plataforma de dados do Azure, considere Azure Databricks, Data Factory e Azure Purview.

Databricks
Databricks é uma plataforma de análise de dados otimizada para o Azure. Ele fornece configuração com um clique, fluxos de trabalho simplificados e um espaço de trabalho interativo para aplicativos baseados em Spark.

O Databricks adiciona recursos ao Apache Spark, incluindo clusters Spark totalmente gerenciados e um espaço de trabalho interativo. Você pode usar APIs REST para programar clusters.

Nos notebooks Databricks, você usará ferramentas de programação familiares, como R, Python, Scala e SQL. A segurança baseada em função no Azure Active Directory e Databricks fornece segurança de nível empresarial.

Fábrica de dados
Data Factory é um serviço de integração em nuvem. Ele orquestra o movimento de dados entre vários armazenamentos de dados.

Como engenheiro de dados, você pode criar fluxos de trabalho orientados por dados na nuvem para orquestrar e automatizar a movimentação e a transformação de dados. Use o Data Factory para criar e agendar fluxos de trabalho controlados por dados (chamados pipelines) que podem ingerir dados de armazenamentos de dados.

O Data Factory processa e transforma dados usando serviços de computação como Azure HDInsight, Hadoop, Spark e Azure Machine Learning. Publique dados de saída em repositórios de dados, como o Azure Synapse Analytics, para que os aplicativos de business intelligence possam consumir os dados. Por fim, você usa o Data Factory para organizar dados brutos em armazenamentos de dados e data lakes significativos para que sua organização possa tomar melhores decisões de negócios.

Área do Azure
O Azure Purview é um serviço unificado de governança de dados que ajuda você a gerenciar e controlar seus dados locais, multicloud e software como serviço (SaaS). Crie facilmente um mapa holístico e atualizado de seu cenário de dados com descoberta de dados automatizada, classificação de dados confidenciais e linhagem de dados de ponta a ponta. Capacite os consumidores de dados a encontrar dados valiosos e confiáveis.

Introdução
Concluído
100 EXP
5 minutos
Os engenheiros de dados que trabalham em projetos de nível empresarial fornecem dados para uma variedade de profissionais, desde profissionais da informação até cientistas de dados. Muitas organizações que buscam projetos de transformação de dados ainda usam funções de trabalho tradicionais, como analistas de negócios. Mas à medida que os tipos de dados evoluem e os métodos de análise mudam, surgem novas funções.

Suponha que sua organização esteja planejando um projeto de transformação digital em que os dados desempenham um papel fundamental. A organização incorporará o melhor de seus sistemas atuais, como o processamento de transações online (OLTP) e sistemas de armazenamento de dados. Mas a organização também quer usar a análise preditiva para aproveitar as oportunidades de vendas e melhorar os gastos operacionais.

Como administrador de banco de dados de sua empresa, seu trabalho é mover os dados existentes para novos sistemas e conectá-los. Onde você começa? Seu trabalho está obsoleto? A resposta curta é não. Você não tem nada a perder, porque suas habilidades ainda são essenciais. E você tem tudo a ganhar aprendendo novas habilidades em tecnologias que desbloqueiam o potencial das técnicas modernas de engenharia de dados.

Objetivos de aprendizado
Neste módulo você irá:

Liste as funções de trabalho envolvidas em projetos de dados modernos.
Descreva as práticas de engenharia de dados.
Explore o processo de alto nível para projetar um projeto de engenharia de dados.
Próxima unidade: Identificar funções de trabalho

Identificar funções de trabalho
Concluído
100 EXP
5 minutos
Grandes projetos de dados podem ser complexos. Os projetos geralmente envolvem centenas de decisões. Normalmente, várias pessoas estão envolvidas, e cada pessoa ajuda a levar o projeto do design à produção.

Funções como partes interessadas de negócios, analistas de negócios e desenvolvedores de inteligência de negócios são bem conhecidas e ainda valiosas. À medida que as técnicas de processamento de dados mudam com a tecnologia, novos papéis começam a aparecer. Essas funções fornecem habilidades especializadas para ajudar a simplificar o processo de engenharia de dados.

Em particular, três funções estão começando a se tornar comuns em projetos de dados modernos:

Engenheiro de dados
Cientista de dados
Engenheiro de inteligência artificial (IA)
Engenheiro de dados
Os engenheiros de dados provisionam e configuram tecnologias de plataforma de dados no local e na nuvem. Eles gerenciam e protegem o fluxo de dados estruturados e não estruturados de várias fontes. As plataformas de dados que eles usam podem incluir bancos de dados relacionais, bancos de dados não relacionais, fluxos de dados e armazenamentos de arquivos. Os engenheiros de dados também garantem que os serviços de dados se integrem com segurança e perfeição a outras tecnologias de plataforma de dados ou serviços de aplicativos, como Serviços Cognitivos do Azure, Pesquisa do Azure ou até bots.

O engenheiro de dados do Azure se concentra em tarefas relacionadas a dados no Azure. As principais responsabilidades incluem o uso de serviços e ferramentas para ingerir, sair e transformar dados de várias fontes. Os engenheiros de dados do Azure colaboram com as partes interessadas nos negócios para identificar e atender aos requisitos de dados. Eles projetam e implementam soluções. Eles também gerenciam, monitoram e garantem a segurança e a privacidade dos dados para atender às necessidades dos negócios.

A função de engenheiro de dados é diferente da função de administrador de banco de dados. O escopo de trabalho de um engenheiro de dados vai muito além de cuidar de um banco de dados e do servidor onde ele está hospedado. Os engenheiros de dados também devem obter, ingerir, transformar, validar e limpar dados para atender aos requisitos de negócios. Esse processo é chamado de disputa de dados .

Um engenheiro de dados agrega enorme valor aos projetos de inteligência de negócios e ciência de dados. A disputa de dados pode consumir muito tempo. Quando o engenheiro de dados lida com dados, os projetos avançam mais rapidamente porque os cientistas de dados podem se concentrar em suas próprias áreas de trabalho.

Tanto os administradores de banco de dados quanto os profissionais de inteligência de negócios podem fazer a transição facilmente para uma função de engenheiro de dados. Eles só precisam aprender as ferramentas e a tecnologia que são usadas para processar grandes quantidades de dados.

Cientista de dados
Os cientistas de dados realizam análises avançadas para extrair valor dos dados. Seu trabalho pode variar de análises descritivas a análises preditivas. A análise descritiva avalia os dados por meio de um processo conhecido como análise exploratória de dados (EDA). A análise preditiva é usada no aprendizado de máquina para aplicar técnicas de modelagem que podem detectar anomalias ou padrões. Estes são uma parte importante dos modelos de previsão.

A análise descritiva e preditiva é apenas um aspecto do trabalho dos cientistas de dados. Alguns cientistas de dados podem até trabalhar no campo do aprendizado profundo, experimentando iterativamente para resolver um problema de dados complexo usando algoritmos personalizados.

Evidências anedóticas sugerem que a maior parte do trabalho em um projeto de ciência de dados é gasto em disputa de dados e engenharia de recursos. Os cientistas de dados podem acelerar o processo de experimentação quando os engenheiros de dados usam suas habilidades para organizar dados com sucesso.

engenheiro de IA
Os engenheiros de IA trabalham com serviços de IA, como Serviços Cognitivos, Pesquisa Cognitiva e Bot Framework. Os Serviços Cognitivos incluem Visão Computacional, Análise de Texto, Pesquisa do Bing e Compreensão da Linguagem (LUIS).

Em vez de criar modelos, os engenheiros de IA aplicam os recursos pré-criados das APIs de Serviços Cognitivos. Os engenheiros de IA incorporam esses recursos em um aplicativo ou bot novo ou existente. Os engenheiros de IA contam com a experiência dos engenheiros de dados para armazenar informações geradas pela IA.

Por exemplo, um engenheiro de IA pode estar trabalhando em um aplicativo de Visão Computacional que processa imagens. Esse engenheiro de IA pediria a um engenheiro de dados para provisionar uma instância do Azure Cosmos DB para armazenar os metadados e as marcas que o aplicativo Computer Vision gera.

Diferenças de função
As funções do engenheiro de dados, engenheiro de IA e cientista de dados diferem. Cada função resolve um problema diferente.

Os engenheiros de dados provisionam principalmente armazenamentos de dados. Eles garantem que grandes quantidades de dados sejam extraídas, carregadas e transformadas de forma segura e econômica.

Os engenheiros de IA adicionam os recursos inteligentes de visão, voz, linguagem e conhecimento aos aplicativos. Para fazer isso, eles usam as ofertas de Serviços Cognitivos que estão disponíveis imediatamente.

Quando um aplicativo de Serviços Cognitivos atinge sua capacidade, os engenheiros de IA chamam os cientistas de dados. Os cientistas de dados desenvolvem modelos de aprendizado de máquina e personalizam componentes para o aplicativo de um engenheiro de IA.

Cada função da tecnologia de dados é distinta e cada uma contribui com uma parte importante para os projetos de transformação digital.

Próxima unidade: Identificar processos de engenharia de dados

Identifique os processos de engenharia de dados
Concluído
100 EXP
5 minutos
Aqui estão algumas das tarefas de um engenheiro de dados do Azure:

Projetar e desenvolver soluções de armazenamento e processamento de dados para a empresa.
Configure e implante serviços de dados baseados em nuvem, como serviços de blob, bancos de dados e análises.
Proteja a plataforma e os dados armazenados. Certifique-se de que apenas os usuários necessários possam acessar os dados.
Garanta a continuidade dos negócios em condições incomuns usando técnicas de alta disponibilidade e recuperação de desastres.
Monitore para garantir que os sistemas funcionem corretamente e sejam econômicos.
Como um engenheiro de dados difere de um administrador de banco de dados

A função do engenheiro de dados se sobrepõe à função do administrador de banco de dados (DBA) em termos de tarefas amplas. As diferenças estão no escopo e no foco. Os engenheiros de dados trabalham com mais do que apenas bancos de dados e se concentram em implementações de nuvem em vez de servidores locais.

Mova os dados
Como engenheiro de dados, você pode transferir e mover dados de várias maneiras. Uma maneira é iniciar um processo de extração, transformação e carregamento (ETL) .

As origens de extração podem incluir bancos de dados, arquivos e fluxos. Cada fonte tem formatos de dados exclusivos que podem ser estruturados, semiestruturados ou não estruturados. No Azure, as fontes de dados incluem Azure Cosmos DB, Azure Data Lake, arquivos e armazenamento de BLOBs do Azure.

Extrair
Durante o processo de extração, os engenheiros de dados definem os dados e sua origem:

Defina a fonte de dados : Identifique os detalhes da fonte, como o grupo de recursos, a assinatura e as informações de identidade, como uma chave ou segredo.

Defina os dados : Identifique os dados a serem extraídos. Defina dados usando uma consulta de banco de dados, um conjunto de arquivos ou um nome de armazenamento de BLOBs do Azure para armazenamento de blobs.

Transformar
Defina a transformação de dados : As operações de transformação de dados podem incluir dividir, combinar, derivar, adicionar, remover ou dinamizar colunas. Campos de mapa entre a fonte de dados e o destino de dados. Você também pode precisar agregar ou mesclar dados.

Carregar
Defina o destino : durante um carregamento, muitos destinos do Azure podem aceitar dados formatados como JavaScript Object Notation (JSON), arquivo ou blob. Pode ser necessário escrever código para interagir com as APIs do aplicativo.

O Azure Data Factory oferece suporte interno para Azure Functions. Você também encontrará suporte para muitas linguagens de programação, incluindo Node.js, .NET, Python e Java. Embora a Extensible Markup Language (XML) fosse comum no passado, a maioria dos sistemas migrou para JSON devido à sua flexibilidade como um tipo de dados semiestruturado.

Iniciar o trabalho : Teste o trabalho ETL em um ambiente de desenvolvimento ou teste. Em seguida, migre o trabalho para um ambiente de produção para carregar o sistema de produção.

Monitore o trabalho : as operações de ETL podem envolver muitos processos complexos. Configure um sistema de monitoramento proativo e reativo para fornecer informações quando as coisas derem errado. Configure o registro de acordo com a tecnologia que o utilizará.

Ferramentas ETL
Como engenheiro de dados, você usará várias ferramentas para ETL. A ferramenta mais comum é o Azure Data Factory, que fornece recursos robustos e quase 100 conectores corporativos. O Data Factory também permite transformar dados usando uma ampla variedade de linguagens.

Você pode descobrir que também precisa de um repositório para manter informações sobre as fontes de dados e os dicionários de sua organização. O Azure Purview pode armazenar essas informações centralmente.

Evolução do ETL
O Azure abriu o caminho para tecnologias que podem lidar com dados não estruturados em uma escala ilimitada. Essa mudança mudou o paradigma para carregar e transformar dados de ETL para extrair, carregar e transformar (ELT).

O benefício do ELT é que você pode armazenar dados em seu formato original, seja JSON, XML, PDF ou imagens. No ELT, você define a estrutura dos dados durante a fase de transformação, para que possa usar os dados de origem em vários sistemas downstream.

Em um processo ELT, os dados são extraídos e carregados em seu formato nativo. Essa alteração reduz o tempo necessário para carregar os dados em um sistema de destino. A mudança também limita a contenção de recursos nas fontes de dados.

As etapas do processo ELT são as mesmas do processo ETL. Eles apenas seguem uma ordem diferente.

Outro processo como o ELT é chamado extrair, carregar, transformar e carregar (ELTL). A diferença com o ELTL é que ele tem uma carga final em um sistema de destino.

Engenharia de dados holística
As organizações estão mudando seus tipos de análise para incorporar análises preditivas e preventivas. Devido a essas mudanças, como engenheiro de dados, você deve analisar os projetos de dados de forma holística. Os profissionais de dados costumavam se concentrar em ETL, mas os desenvolvimentos em tecnologias de plataforma de dados se prestam a uma abordagem ELT.

Projete projetos de dados em fases que reflitam a abordagem ELT:

Fonte : Identifique os sistemas de origem dos quais extrair.
Ingest : Identifique a tecnologia e o método para carregar os dados.
Preparar : Identifique a tecnologia e o método para transformar ou preparar os dados.
Considere também as tecnologias que você usará para analisar e consumir os dados no projeto. Estas são as próximas duas fases do processo:

Analisar : Identifique a tecnologia e o método para analisar os dados.
Consumir : Identifique a tecnologia e o método para consumir e apresentar os dados.
Em projetos tradicionais de análise descritiva, você pode ter transformado dados no Azure Analysis Services e usado o Power BI para consumir os dados analisados. Novas tecnologias de IA, como os serviços Azure Machine Learning e Azure Synapse Analytics, fornecem uma gama mais ampla de tecnologias para automatizar algumas das análises necessárias.

Essas fases do projeto não precisam necessariamente fluir linearmente. Por exemplo, como a experimentação de aprendizado de máquina é iterativa, a fase de análise às vezes revela problemas como dados de origem ausentes ou etapas de transformação. Para obter os resultados de que você precisa, talvez seja necessário repetir as fases anteriores.

Para apreciar totalmente esse processo, vamos examiná-lo usando um exemplo de arquitetura de alto nível.

Próxima unidade: Analisar as tarefas de um engenheiro de dados



Analisar as tarefas de um engenheiro de dados
Concluído
100 EXP
5 minutos
Aqui está um exemplo de como projetar holisticamente um projeto de engenharia de dados seguindo as cinco fases: origem, ingestão, preparação, análise e consumo.

A Contoso Health Network implantou recentemente dispositivos IoT em sua unidade de terapia intensiva (UTI). Eis os objetivos do projeto:

Capture dados sobre o monitoramento biométrico do paciente em tempo real para ajudar os médicos a tratar seus pacientes.
Armazene os dados biométricos para que o centro de pesquisa da Contoso possa analisá-los ainda mais no futuro.
Use o Azure Machine Learning para entender quais tratamentos melhoram a qualidade do atendimento e reduzem a probabilidade de um paciente ser readmitido no hospital.
Crie uma visualização do histórico dos dados para o diretor médico da Contoso.
Depois de analisar o caso de negócios, o arquiteto técnico da Contoso propõe as seguintes tecnologias:

Hub IoT do Azure para capturar dados em tempo real dos dispositivos IoT da ICU.
Azure Stream Analytics para transmitir e enriquecer os dados de IoT, criar janelas e agregações e integrar o Azure Machine Learning.
Azure Data Lake Storage Gen2 para armazenar os dados biométricos em alta velocidade.
Azure Data Factory para executar o processo de extração, carregamento, transformação e carregamento (ELTL) para mover os dados do armazenamento do data lake para o Azure SQL Data Warehouse.
Azure Synapse Analytics para fornecer serviços de armazenamento de dados e engenharia de big data para dar suporte às necessidades do diretor médico.
Power BI para criar o painel do paciente. Parte do painel mostrará telemetria em tempo real sobre a condição do paciente. A outra parte mostrará o histórico recente do paciente.
Azure Machine Learning para processar dados brutos e agregados. Os pesquisadores usarão isso para realizar análises preditivas sobre a readmissão do paciente.
O engenheiro de dados da Contoso cria um plano de trabalho para implementar as operações ELTL. O plano inclui um fluxo de trabalho de provisionamento e um fluxo de trabalho holístico.

O fluxo de trabalho de provisionamento:

Provisione o Azure Data Lake Storage Gen2.
Provisione o Azure Synapse Analytics.
Provisione o Hub IoT do Azure.
Provisione o Azure Stream Analytics.
Provisione o Azure Machine Learning.
Provisione o Azure Data Factory.
Provisione o Power BI.
O fluxo de trabalho holístico:

Configure o Hub IoT do Azure para capturar dados dos dispositivos IoT ICU.
Conecte o Hub IoT do Azure ao Azure Stream Analytics. Configure as funções de criação de janela para os dados do ICU. As funções agregarão os dados para cada janela. Ao mesmo tempo, configure o Hub IoT para mover os dados de streaming para o Azure Data Lake Storage usando o Azure Functions.
Configure o Azure Functions para armazenar os agregados do Azure Stream Analytics no Azure Data Lake Storage Gen2.
Use o Azure Data Factory para carregar dados do data lake no Azure Synapse Analytics para dar suporte às necessidades do diretor médico. Depois que os dados são carregados, as transformações podem ocorrer no Azure Synapse Analytics.
Em paralelo, conecte o serviço Azure Machine Learning ao Azure Data Lake Storage para realizar análises preditivas.
Conecte o Power BI ao Stream Analytics para extrair as agregações em tempo real dos dados do paciente. Conecte o Azure Synapse Analytics para extrair os dados históricos para criar um painel combinado.

Introdução
Concluído
100 EXP
1 minuto
Imagine que você está criando um aplicativo de compartilhamento de fotos. Todos os dias, milhares de usuários tiram fotos e confiam em seu aplicativo para manter suas fotos seguras e, ao mesmo tempo, torná-las acessíveis em todos os seus dispositivos. Armazenar essas fotos é fundamental para seus negócios e você deseja garantir aos usuários que o sistema usado em seu aplicativo é rápido, confiável e seguro. Idealmente, isso seria feito sem que você precisasse criar todos esses aspectos no aplicativo.

O Armazenamento do Azure fornece esse poder, resiliência e desempenho na nuvem. Este módulo fornece instruções sobre como usar o poder do Armazenamento do Azure para obter a confiabilidade e o desempenho de que você precisa em seu aplicativo. Você pode então aplicar esse conhecimento a qualquer aplicativo.

Objetivos de aprendizado
Neste módulo, você irá:

Explore os tipos de dados e serviços suportados por uma conta de armazenamento do Azure.
Crie uma nova conta de armazenamento do Azure usando o portal do Azure.
Crie um novo aplicativo e conecte o aplicativo a uma conta de armazenamento do Azure.
Pré-requisitos
Alguma experiência de programação em C# ou JavaScript é assumida.

Próxima unidade: Explore os serviços de armazenamento do Azure

Explore os serviços de armazenamento do Azure
Concluído
100 EXP
5 minutos
Vamos começar dando uma olhada rápida nos serviços de armazenamento do Azure, estilos de dados e contas.

O Armazenamento do Microsoft Azure é um serviço gerenciado que fornece armazenamento durável , seguro e escalável na nuvem. Vamos quebrar esses termos.

Prazo	Descrição
Gerenciou	O Microsoft Azure lida com a manutenção e quaisquer problemas críticos para você.
Durável	A redundância garante que seus dados estejam seguros em caso de falhas transitórias de hardware. Você também pode replicar dados em datacenters ou regiões geográficas para proteção extra contra catástrofes locais ou desastres naturais. Os dados replicados dessa maneira permanecem altamente disponíveis no caso de uma interrupção inesperada.
Seguro	Todos os dados gravados no Armazenamento do Azure são criptografados pelo serviço. O Armazenamento do Azure fornece controle detalhado sobre quem tem acesso aos seus dados.
Escalável	O Armazenamento do Azure foi projetado para ser massivamente escalável para atender às necessidades de armazenamento de dados e desempenho dos aplicativos atuais.
Uma única assinatura do Azure pode hospedar até 250 contas de armazenamento por região, cada uma com capacidade máxima de conta de armazenamento de 5 PiB.

Tipos de dados nos serviços de armazenamento do Azure
O armazenamento do Azure inclui quatro tipos de dados:

Blobs : Um armazenamento de objetos massivamente escalável para texto e dados binários. Pode incluir suporte para Azure Data Lake Storage Gen2.
Arquivos : compartilhamentos de arquivos gerenciados para implantações na nuvem ou no local.
Filas : Um repositório de mensagens para mensagens confiáveis entre componentes de aplicativos.
Armazenamento de tabela : um armazenamento NoSQL para armazenamento de dados estruturados sem esquema. O armazenamento de tabelas não é abordado neste módulo.
Todos esses tipos de dados no Armazenamento do Azure podem ser acessados de qualquer lugar do mundo por HTTP ou HTTPS. A Microsoft fornece SDKs para serviços de armazenamento do Azure em várias linguagens de programação e em uma API REST. Você também pode explorar visualmente seus dados diretamente no portal do Azure.

Bolhas
O Armazenamento de Blobs do Azure é uma solução de armazenamento de objetos otimizada para armazenar grandes quantidades de dados não estruturados, como texto ou dados binários. O Armazenamento de Blobs é ideal para:

Exibição de imagens ou documentos diretamente em um navegador, incluindo sites estáticos completos.
Armazenando arquivos para acesso distribuído.
Transmissão de vídeo e áudio.
Armazenar dados para backup e restauração, recuperação de desastres e arquivamento.
Armazenar dados para análise por um serviço local ou hospedado no Azure.
O Armazenamento do Azure dá suporte a três tipos de blobs:

Tipo de blob	Descrição
Blobs de blocos	Blobs de blocos são usados para armazenar arquivos de texto ou binários de até ~5 TB (50.000 blocos de 100 MB) de tamanho. O principal caso de uso para blobs de blocos é o armazenamento de arquivos que são lidos do início ao fim, como arquivos de mídia ou arquivos de imagem para sites. Eles são denominados blobs de blocos porque os arquivos maiores que 100 MB devem ser carregados como pequenos blocos. Esses blocos são então consolidados (ou confirmados) no blob final.
Blobs de página	Os blobs de páginas são usados para armazenar arquivos de acesso aleatório de até 8 TB de tamanho. Os blobs de páginas são usados principalmente como armazenamento de apoio para os VHDs usados para fornecer discos duráveis para Máquinas Virtuais do Azure (VMs do Azure). Eles são blobs de páginas nomeados porque fornecem acesso aleatório de leitura/gravação a páginas de 512 bytes.
Anexar blobs	Os blobs de acréscimo são compostos de blocos como blobs de bloco, mas são otimizados para operações de acréscimo. Esses blobs são frequentemente usados para registrar informações de uma ou mais fontes no mesmo blob. Por exemplo, você pode gravar todo o log de rastreamento no mesmo blob de acréscimo para um aplicativo executado em várias VMs. Um único blob de acréscimo pode ter até 195 GB.
arquivos
O armazenamento de arquivos do Azure permite que você configure compartilhamentos de arquivos de rede altamente disponíveis que podem ser acessados usando o protocolo SMB (Server Message Block) padrão. Isso significa que várias VMs podem compartilhar os mesmos arquivos com acesso de leitura e gravação. Você também pode ler os arquivos usando a interface REST ou as bibliotecas do cliente de armazenamento. Você também pode associar um URL exclusivo a qualquer arquivo para permitir acesso refinado a um arquivo privado por um determinado período de tempo. Os compartilhamentos de arquivos podem ser usados para muitos cenários comuns:

Armazenar arquivos de configuração compartilhados para VMs, ferramentas ou utilitários para que todos usem a mesma versão.
Arquivos de log, como diagnósticos, métricas e despejos de memória.
Dados compartilhados entre aplicativos locais e VMs do Azure para permitir a migração de aplicativos para a nuvem durante um período de tempo.
Filas
O Armazenamento de Filas do Azure é usado para armazenar e recuperar mensagens. As mensagens de fila podem ter até 64 KB de tamanho e uma fila pode conter milhões de mensagens. As filas são usadas para armazenar listas de mensagens a serem processadas de forma assíncrona.

Você pode usar filas para conectar livremente diferentes partes do seu aplicativo. Por exemplo, poderíamos realizar o processamento de imagens nas fotos enviadas por nossos usuários. Talvez queiramos fornecer algum tipo de detecção de rosto ou capacidade de marcação, para que as pessoas possam pesquisar todas as imagens armazenadas em nosso serviço. Poderíamos usar filas para passar mensagens para nosso serviço de processamento de imagens para que ele saiba que novas imagens foram carregadas e estão prontas para processamento. Esse tipo de arquitetura permitiria desenvolver e atualizar cada parte do serviço de forma independente.

Armazenamento de mesa
Para obter mais informações sobre o armazenamento de tabelas do Azure, consulte visão geral do armazenamento de tabelas

Contas de armazenamento do Azure
Para acessar qualquer um desses serviços de um aplicativo, você precisa criar uma conta de armazenamento . A conta de armazenamento fornece um namespace exclusivo no Azure para armazenar e acessar seus objetos de dados. Uma conta de armazenamento contém todos os blobs, arquivos, filas, tabelas e discos de VM que você cria nessa conta.

Criar uma conta de armazenamento
Você pode criar uma conta de armazenamento do Azure usando o portal do Azure, Azure PowerShell ou CLI do Azure. O Armazenamento do Azure oferece três opções de conta distintas com diferentes preços e recursos suportados.

Tipo de conta	Descrição
V2 de uso geral (GPv2)	As contas de uso geral v2 (GPv2) são contas de armazenamento que oferecem suporte a todos os recursos mais recentes para blobs, arquivos, filas e tabelas. O preço das contas GPv2 foi projetado para oferecer os preços mais baixos por gigabyte.
v1 de uso geral (GPv1)	As contas de uso geral v1 (GPv1) fornecem acesso a todos os serviços de armazenamento do Azure, mas podem não ter os recursos mais recentes ou o menor preço por gigabyte. Por exemplo, armazenamento legal e armazenamento de arquivo não são suportados no GPv1. O preço é menor para transações GPv1, portanto, cargas de trabalho com alta rotatividade ou altas taxas de leitura podem se beneficiar desse tipo de conta.
Contas de armazenamento de blobs	Um tipo de conta herdada, as contas de armazenamento de blobs dão suporte a todos os mesmos recursos de blob de blocos que o GPv2, mas estão limitados a dar suporte apenas a blobs de bloco e anexo. O preço é bastante semelhante ao preço de contas v2 de uso geral.
Se você estiver interessado em saber mais sobre como criar contas de armazenamento, certifique-se de acessar o módulo Criar uma conta de armazenamento do Azure no portal de aprendizado.

Próxima unidade: Exercício - Criar um novo aplicativo para trabalhar com o armazenamento do Azure

Interagir com as APIs de Armazenamento do Azure
Concluído
100 EXP
5 minutos
O Armazenamento do Azure fornece uma API REST para trabalhar com os contêineres e dados armazenados em cada conta. Cada tipo de dados que você pode armazenar tem sua própria API independente. Lembre-se de que temos quatro tipos de dados específicos:

Blobs para dados não estruturados, como arquivos binários e de texto.
Filas para mensagens persistentes.
Tabelas para armazenamento estruturado de chaves/valores.
Arquivos para compartilhamentos de arquivos SMB tradicionais.
Usar a API REST
As APIs REST de armazenamento podem ser acessadas de qualquer lugar na Internet por qualquer aplicativo que possa enviar uma solicitação HTTP/HTTPS e receber uma resposta HTTP/HTTPS.

Conecte-se à sua conta de armazenamento do Azure
Concluído
100 EXP
10 minutos
Você adicionou as bibliotecas de cliente necessárias ao seu aplicativo e está pronto para se conectar à sua conta de armazenamento do Azure.

Para trabalhar com dados em uma conta de armazenamento, seu aplicativo precisará de dois dados:

Chave de acesso
Ponto de extremidade da API REST
Chaves de acesso de segurança
Cada conta de armazenamento tem duas chaves de acesso exclusivas que são usadas para proteger a conta de armazenamento. Se seu aplicativo precisar se conectar a várias contas de armazenamento, ele exigirá uma chave de acesso para cada conta de armazenamento.

Ponto de extremidade da API REST
Além das chaves de acesso para autenticação em contas de armazenamento, seu aplicativo precisará conhecer os pontos de extremidade do serviço de armazenamento para emitir as solicitações REST.

O ponto de extremidade REST é uma combinação do nome da sua conta de armazenamento , o tipo de dados e um domínio conhecido. Por exemplo:

Tipo de dados	Exemplo de endpoint
Bolhas	https://[name].blob.core.windows.net/
Filas	https://[name].queue.core.windows.net/
Mesa	https://[name].table.core.windows.net/
arquivos	https://[name].file.core.windows.net/
Se você tiver um domínio personalizado vinculado ao Azure, também poderá criar uma URL de domínio personalizado para o ponto de extremidade.

Cadeias de conexão
A maneira mais simples de lidar com chaves de acesso e URLs de ponto de extremidade em aplicativos é usar cadeias de conexão de conta de armazenamento . Uma cadeia de conexão fornece todas as informações de conectividade necessárias em uma única cadeia de texto.

As cadeias de conexão do Armazenamento do Azure são semelhantes ao exemplo a seguir, mas com a chave de acesso e o nome da conta de sua conta de armazenamento específica:


cópia de
DefaultEndpointsProtocol=https;AccountName={your-storage};
   AccountKey={your-access-key};
   EndpointSuffix=core.windows.net
Segurança
As chaves de acesso são essenciais para fornecer acesso à sua conta de armazenamento e, como resultado, não devem ser fornecidas a nenhum sistema ou pessoa que você não deseja que tenha acesso à sua conta de armazenamento. As chaves de acesso são o equivalente a um nome de usuário e senha para acessar seu computador.

Normalmente, as informações de conectividade da conta de armazenamento são armazenadas em uma variável de ambiente, banco de dados ou arquivo de configuração.

 Importante

Armazenar essas informações em um arquivo de configuração pode ser perigoso se você incluir esse arquivo no controle do código-fonte e armazená-lo em um repositório público. Esse é um erro comum e significa que qualquer pessoa pode navegar pelo seu código-fonte no repositório público e ver as informações de conexão da sua conta de armazenamento.

Cada conta de armazenamento tem duas chaves de acesso. A razão para isso é permitir que as chaves sejam rotacionadas (regeneradas) periodicamente como parte das melhores práticas de segurança para manter sua conta de armazenamento segura. Esse processo pode ser feito no portal do Azure ou na ferramenta de linha de comando Azure CLI/PowerShell.

A rotação de uma chave invalidará imediatamente o valor da chave original e revogará o acesso de qualquer pessoa que a tenha obtido de forma inadequada. Com suporte para duas chaves, você pode girar as chaves sem causar tempo de inatividade nos aplicativos que as utilizam. Seu aplicativo pode alternar para usar a chave de acesso alternativa enquanto a outra chave é gerada novamente. Se você tiver vários aplicativos usando essa conta de armazenamento, todos eles devem usar a mesma chave para dar suporte a essa técnica. Segue a ideia básica:

Atualize as cadeias de conexão em seu código de aplicativo para fazer referência à chave de acesso secundária da conta de armazenamento.
Regenere a chave de acesso primária para sua conta de armazenamento usando o portal do Azure ou a ferramenta de linha de comando.
Atualize as cadeias de conexão em seu código para fazer referência à nova chave de acesso primária.
Regenere a chave de acesso secundária da mesma maneira.
 Dica

É altamente recomendável que você alterne periodicamente suas chaves de acesso para garantir que elas permaneçam privadas, assim como alterar suas senhas. Se você estiver usando a chave em um aplicativo de servidor, poderá usar um Azure Key Vault para armazenar a chave de acesso para você. Os cofres de chaves incluem suporte para sincronizar diretamente com a conta de armazenamento e alternar automaticamente as chaves periodicamente. O uso de um Key Vault fornece uma camada adicional de segurança, para que seu aplicativo nunca precise trabalhar diretamente com uma chave de acesso.

Assinaturas de acesso compartilhado (SAS)
As chaves de acesso são a abordagem mais fácil para autenticar o acesso a uma conta de armazenamento. No entanto, eles fornecem acesso total a qualquer coisa na conta de armazenamento, semelhante a uma senha de root em um computador.

As contas de armazenamento oferecem um mecanismo de autenticação separado chamado assinaturas de acesso compartilhado que dão suporte à expiração e a permissões limitadas para cenários em que você precisa conceder acesso limitado. Você deve usar essa abordagem ao permitir que outros usuários leiam e gravem dados em sua conta de armazenamento. Há links para nossa documentação sobre este tópico avançado no final do módulo.

Próxima unidade: Exercício - Adicionar configuração de armazenamento do Azure ao seu aplicativo



Integração de dados em escala com Azure Data Factory ou pipeline Azure Synapse

Requisitos anteriores
Os seguintes pré-requisitos devem ser preenchidos:

Entrada bem-sucedida no portal do Azure
Descrição das opções de armazenamento do Azure
Descrição das opções de processamento do Azure


Introdução
Preenchidas
100 EXP
2 minutos
Com a grande variedade de armazenamentos de dados disponíveis no Azure, há a necessidade de gerenciar e organizar a movimentação de dados entre eles. Na verdade, você pode querer automatizar um processo normal de movimentação de dados como parte de uma solução de análise de negócios mais ampla. Os pipelines do Azure Data Factory e do Azure Synapse atendem a essa necessidade, e esta seção apresentará essa tecnologia, seus componentes, o processo do Azure Data Factory e a segurança necessária para provisionar e gerenciar o serviço.

Objetivos de aprendizado
Neste módulo, você aprenderá a:

Conheça o Azure Data Factory
Descrição dos padrões de integração de dados
Listar o processo do Data Factory
Descrever os componentes do Azure Data Factory
Gerenciar a segurança do Azure Data Factory
Configurar o Azure Data Factory
Criar serviços vinculados
Criação de conjuntos de dados
Criar atividades e pipelines
Gerenciar um ambiente de tempo de execução de integração
Próxima unidade: Conheça o Azure Data Factory

Conheça o Azure Data Factory
Preenchidas
100 EXP
4 minutos
A necessidade de acionar a movimentação de dados em lote ou configurar uma programação regular é um requisito para a maioria das soluções de análise. O Azure Data Factory (ADF) é o serviço que pode ser usado para atender a esse requisito. O ADF fornece um serviço de integração de dados baseado em nuvem que orquestra o movimento e a transformação de dados em vários armazenamentos de dados e recursos de computação.

O Azure Data Factory é o serviço de integração de dados e ETL baseado em nuvem que permite criar fluxos de trabalho controlados por dados para orquestrar a movimentação de dados e transformar dados em escala. Com o Azure Data Factory, você pode criar e agendar fluxos de trabalho controlados por dados (chamados pipelines) que podem ingerir dados de diferentes armazenamentos de dados. Você pode criar processos ETL complexos que transformam dados visualmente com fluxos de dados ou usando serviços de computação como Azure HDInsight Hadoop, Azure Databricks e Azure Synapse Analytics.

Grande parte da funcionalidade do Azure Data Factory aparece no Azure Synapse Analytics como um recurso chamado pipelines, que permite integrar pipelines de dados entre pools SQL, pools Spark e SQL sem servidor, fornecendo um armazenamento de ponto único para todas as suas necessidades de análise.

O que significa orquestração?
Para usar uma analogia, pense em uma orquestra sinfônica. O principal membro da orquestra é o maestro. O maestro não toca os instrumentos, ele simplesmente conduz os membros da orquestra através de toda a música que eles executam. Os músicos usam seu próprio conhecimento para gerar sons específicos em diferentes partes da sinfonia, para que possam aprender apenas algumas partes da peça. O maestro conduz a música inteira e, portanto, está ciente da partitura completa que está sendo tocada. Você também usará movimentos de braço específicos que fornecem instruções aos músicos sobre como uma peça musical deve ser tocada.

O ADF pode usar uma abordagem semelhante, embora tenha funcionalidade nativa para ingestão e transformação de dados, às vezes ele informa a outro serviço para fazer o trabalho real necessário em seu nome, como o Databricks para executar uma consulta de transformação. Portanto, neste caso, seria o Databricks fazendo o trabalho, não o ADF. O ADF simplesmente orquestra a execução da consulta e fornece os pipelines para mover os dados para a próxima etapa ou destino.

Ele também fornece ótimas visualizações para mostrar a linhagem e as dependências entre seus pipelines de dados e permitir que você monitore todos os seus pipelines de dados a partir de uma visão unificada, facilitando a identificação de problemas e a configuração de alertas de monitoramento.

Descrição dos padrões de integração de dados
Preenchidas
100 EXP
6 minutos
O Microsoft Azure fornece uma variedade de serviços de plataforma de dados que permitem realizar diferentes tipos de análise. Seja uma solução de análise descritiva em um data warehouse, por meio de análise preditiva no HDInsight, Azure Databricks ou Serviços de Machine Learning. Existe a necessidade de um serviço para lidar com o aspecto importante da integração de dados.

Primeiro, a integração de dados envolve a coleta de dados de uma ou várias fontes. Opcionalmente, normalmente inclui um processo onde os dados podem ser limpos e transformados, ou talvez aumentados com dados adicionais e prontos. Por fim, os dados mesclados são armazenados em um serviço de plataforma de dados que controla o tipo de análise que você deseja realizar. Esse processo pode ser automatizado usando o Azure Data Factory em um padrão conhecido como extrair, transformar e carregar (ETL).

Extração
Durante o processo de extração, os engenheiros de dados definem os dados e sua origem:

Definição da fonte de dados – os detalhes da fonte são identificados, como o grupo de recursos, a assinatura e as informações de identidade, como uma chave ou segredo.

Definição de dados : os dados a serem extraídos são identificados. Uma consulta de banco de dados, um conjunto de arquivos ou um nome de armazenamento de BLOBs do Azure para armazenamento de BLOBs é usado para definir os dados.

Transformação
Definição de transformação de dados – As operações de transformação de dados, como dividir, mesclar, derivar, adicionar, excluir ou dinamizar em colunas, são definidas. Os campos são mapeados entre a origem e o destino dos dados. Os dados também podem precisar ser agregados ou combinados.
Fardo
Definição de destino – durante um carregamento, muitos destinos do Azure podem aceitar dados nos formatos JavaScript Object Notation (JSON), arquivo ou blob. Também pode ser necessário escrever código para interagir com as APIs do aplicativo.

O Azure Data Factory oferece suporte interno para Azure Functions. Você também encontrará suporte para várias linguagens de programação, incluindo Node.js, .NET, Python e Java. Embora a Extensible Markup Language (XML) fosse comum no passado, a maioria dos sistemas migrou para JSON devido à sua flexibilidade como um tipo de dados semiestruturado.

Iniciando o trabalho - O trabalho ETL é testado em um ambiente de desenvolvimento ou teste. Em seguida, você migra para um ambiente de produção para carregar o sistema de produção.

Monitoramento de Trabalho – As operações de ETL podem envolver muitos processos complexos. Ao configurar um sistema de monitoramento proativo e reativo, as informações podem ser obtidas quando algo dá errado. É necessário configurar o registro de acordo com a tecnologia a ser utilizada.

Ferramentas ETL
Como engenheiro de dados, existem várias ferramentas para extrair, transformar e carregar dados. O Azure Data Factory fornece quase 100 conectores corporativos e recursos robustos para usuários de código e usuários baseados em código para atender às suas necessidades de movimentação e transformação de dados.

Evolução do ETL
O Azure abriu o caminho para tecnologias que podem lidar com dados não estruturados em escala ilimitada. Essa mudança levou a uma evolução do paradigma de carregamento e transformação de dados de ETL para ELT (extrair, carregar e transformar).

A vantagem do ELT é que os dados podem ser armazenados em seu formato original, seja JSON, XML, PDF ou imagens. No ELT, a estrutura dos dados é definida durante a fase de transformação, para que os dados de origem possam ser usados em vários sistemas de nível inferior.

Em um processo ELT, os dados são extraídos e carregados em seu formato nativo. Essa alteração reduz o tempo necessário para carregar os dados em um sistema de destino. Isso também limita a contenção de recursos nas fontes de dados.

As etapas do processo ETL são as mesmas do processo ETL. Eles apenas seguem uma ordem diferente.

Um processo semelhante ao ELT é chamado ELTL (extrair, carregar, transformar e carregar). A diferença do ELTL é que ele possui uma carga final em um sistema de destino.

Existem dois tipos comuns de padrões de integração de dados que podem ter suporte no Azure Data Factory.

Cargas de trabalho modernas de data warehouse:
Um data warehouse moderno é um data warehouse centralizado que fornece análises descritivas e serviços de suporte a decisões em toda a empresa usando fontes de dados estruturadas, não estruturadas ou de streaming. Os dados fluem para o armazenamento de vários sistemas transacionais, bancos de dados relacionais e outras fontes de dados regularmente. Os dados armazenados são usados para relatórios históricos e análise de tendências. O data warehouse atua como um repositório central para muitas áreas temáticas e é a única fonte de dados.

O Azure Data Factory é normalmente usado para automatizar o processo de extração, transformação e carregamento de dados por meio de processamento em lote em fontes de dados estruturadas e não estruturadas.

Cargas de trabalho analíticas avançadas
A análise avançada na forma de análise preemptiva ou preditiva pode ser realizada usando uma variedade de serviços da Plataforma de Dados do Azure. O Azure Data Factory fornece a integração de sistemas de origem em um data lake warehouse e pode iniciar recursos de computação, como Azure Databricks ou HDInsight, para usar os dados para realizar trabalho analítico avançado.

Próxima Unidade: Explicando o Processo de Data Factory

Explicação do processo de Data Factory
Preenchidas
100 EXP
4 minutos
Fluxos de trabalho orientados a dados

Conecte e colete
A primeira etapa na criação de um sistema de orquestração é definir e conectar todas as fontes de dados necessárias, como bancos de dados, compartilhamentos e serviços web FTP. A próxima etapa é ingerir os dados conforme necessário para um local centralizado para processamento adicional.

transformar e enriquecer
Serviços de computação, como Databricks e Machine Learning, podem ser usados para preparar ou gerar dados transformados em um cronograma controlado e monitorado para transformar dados limpos e transformados em ambientes de produção. Em alguns casos, você pode aumentar ainda mais os dados de origem com dados adicionais para ajudar na análise ou consolidá-los por meio de um processo de normalização para ser usado em um experimento de aprendizado de máquina como exemplo.

Publicar
Depois que os dados brutos forem refinados em um formato pronto para negócios com suporte da fase de transformação e enriquecimento, você poderá carregar os dados no Azure Data Warehouse, no Banco de Dados SQL do Azure, no Azure Cosmos DB ou na análise de mecanismo para a qual seus usuários de negócios podem direcionar de suas ferramentas de inteligência de negócios.

Supervisão
O Azure Data Factory tem suporte interno para monitoramento de pipelines por meio da API do Azure Monitor, PowerShell, logs do Azure Monitor e painéis de integridade no portal do Azure, para monitorar atividades e pipelines agendados para taxas de sucesso e falha.

Próxima unidade: Saiba mais sobre os componentes do Azure Data Factory
Continuar

O Data Factory oferece suporte a uma ampla variedade de fontes de dados às quais você pode se conectar criando um objeto conhecido como serviço vinculado , permitindo que você ingira dados de uma fonte de dados em preparação para preparar os dados para transformação ou análise. Além disso, os serviços vinculados podem ativar os serviços de computação sob demanda. Por exemplo, você pode ter um requisito para iniciar um cluster HDInsight sob demanda para processar apenas dados usando uma consulta do Hive. Assim, os serviços vinculados permitem que você defina fontes de dados ou calcule os recursos necessários para ingerir e preparar os dados.

Com o serviço vinculado definido, o Azure Data Factory leva em consideração os conjuntos de valores a serem usados ​​criando um objeto datasets . Os conjuntos de dados representam as estruturas de dados do armazenamento de dados referenciado pelo objeto de serviço vinculado. Os conjuntos de dados também podem ser usados ​​por um objeto ADF conhecido como atividade.

atividades _eles normalmente contêm lógica de transformação ou comandos de análise do trabalho do Azure Data Factory. As atividades incluem atividade de cópia que pode ser usada para ingerir dados de várias fontes de dados. Eles também podem incluir o fluxo de dados de mapeamento para realizar transformações de dados sem código. Também pode incluir a execução de um procedimento armazenado, consulta Hive ou script Pig para transformar os dados. Você pode alimentar dados em um modelo de aprendizado de máquina para realizar análises. Não é incomum que ocorram várias atividades, que podem incluir a transformação de dados usando um procedimento armazenado SQL e, em seguida, a execução de análises usando Databricks. Nesse caso, várias atividades podem ser agrupadas logicamente com um objeto chamadopipeline , que pode ser agendado para execução, ou você pode definir um gatilho que determina quando uma execução de pipeline precisa ser iniciada. Existem diferentes tipos de gatilhos para diferentes tipos de eventos.

O fluxo de controle é uma orquestração de atividades de pipeline que inclui atividades de encadeamento em uma sequência, ramificação, definição de parâmetros de nível de pipeline e passagem de argumentos enquanto o pipeline é invocado sob demanda ou a partir de um gatilho. Ele também inclui contêineres de passagem e loop de estado personalizados e iteradores Foreach.

Os parâmetros são pares de valores-chave de configuração somente leitura. Os parâmetros são definidos no pipeline. Os argumentos de parâmetro definidos são passados ​​em tempo de execução a partir do contexto de execução criado por um gatilho ou um pipeline executado manualmente. As atividades no pipeline consomem os valores dos parâmetros.

O Azure Data Factory tem um tempo de execução de integração que permite vincular entre atividade e objetos de serviço vinculados. Ele é referenciado pelo serviço vinculado e fornece o ambiente de computação de onde a atividade é executada ou despachada. Desta forma, a atividade pode ser realizada na região mais próxima possível. Existem três tipos de ambiente Integration Runtime, incluindo Azure, auto-hospedado e Azure-SSIS.

Depois que todo o trabalho estiver concluído, você poderá usar o Data Factory para publicar o conjunto de alterações final em outro serviço vinculado, que pode consumir tecnologias como Power BI ou Machine Learning.

Próxima unidade: segurança do Azure Data Factory

Segurança do Azure Data Factory
Preenchidas
100 EXP
3 minutos
Para criar instâncias do Data Factory, a conta de usuário que você usa para entrar no Azure deve ser um membro das funções Contributor , Owner ou Azure Subscription Administrator .

Para criar e gerenciar objetos do Data Factory, incluindo conjuntos de dados, serviços vinculados, pipelines, gatilhos e tempos de execução de integração, os seguintes requisitos devem ser atendidos:

Para criar e gerenciar recursos filho no portal do Azure, você deve ser membro da função Colaborador de Data Factory no nível do grupo de recursos ou superior.
Para criar e gerenciar recursos com o PowerShell ou o SDK, a função Colaborador no nível do recurso ou superior é suficiente.
Função de Colaborador do Data Factory
Quando adicionado como membro desta função, você tem as seguintes permissões:

Crie, edite e exclua fábricas de dados e recursos filho, incluindo conjuntos de dados, serviços vinculados, pipelines, gatilhos e tempos de execução de integração.
Implante modelos do Resource Manager. O método de implantação usado pelo Data Factory no portal do Azure é a implantação do Resource Manager.
Gerencie alertas do App Insights para uma fábrica de dados.
No nível do grupo de recursos ou acima, permite que os usuários implantem modelos do Resource Manager.
Crie tíquetes de suporte.
Se a função de colaborador do Data Factory não atender aos seus requisitos, você poderá criar sua própria função personalizada .

Próxima unidade: Configurar Azure Data Factory

Configurar o Azure Data Factory
Preenchidas
100 EXP
4 minutos
É fácil configurar o Azure Data Factory a partir do portal do Azure, você só precisa das seguintes informações:

Nome - nome da instância do Azure Data Factory.
Assinatura - Assinatura na qual a instância do ADF é criada.
Grupo de recursos – o grupo de recursos no qual a instância do ADF residirá.
Versão - Selecione V2 para os recursos mais recentes.
Localização – a localização do datacenter onde a instância está armazenada.
Habilitar o Git fornece a capacidade de integrar o código que você compila com um repositório Git que permite controlar o código-fonte que será compilado. Defina a URL do Git, o nome do repositório, o nome da ramificação e o diretório raiz.

Próxima unidade: criar serviços vinculados

Criar serviços vinculados
Preenchidas
100 EXP
5 minutos
Antes de criar um conjunto de dados, você deve criar um serviço vinculado para vincular seu data warehouse ao data factory. Os serviços vinculados são muito semelhantes às cadeias de conexão que definem as informações de conexão necessárias para que o Data Factory se conecte a recursos externos. Existem mais de 100 conectores que podem ser usados para definir um serviço vinculado.

Um serviço vinculado ao Data Factory pode ser definido usando a atividade Copiar Dados no designer do ADF ou você pode criá-los independentemente para apontar para um armazenamento de dados ou recursos de computação. A atividade de cópia copia dados entre a origem e o destino e, ao executar essa atividade, você é solicitado a definir um serviço vinculado como parte da definição da atividade de cópia

Como alternativa, você pode definir programaticamente um serviço vinculado no formato JSON para ser usado por meio das APIs REST ou do SDK, usando a seguinte notação:

A tabela a seguir descreve as propriedades do JSON acima:

Propriedade	Descrição	Obrigatoriedade
inhame	Nome do serviço vinculado.	Sim
modelo	Tipo de serviço vinculado. Por exemplo: AzureStorage (armazenamento de dados) ou AzureBatch (processo). Consulte a descrição de typeProperties.	Sim
typePropriedades	As propriedades de tipo são diferentes para cada armazenamento de dados ou processo. Para tipos de armazenamento de dados compatíveis e suas propriedades de tipo, consulte a tabela Tipo de conjunto de dados . Acesse o artigo do conector de armazenamento de dados para obter informações sobre as propriedades de tipo específico de um armazenamento de dados.	Sim
conectarVia	O ambiente de tempo de execução de integração a ser usado para se conectar ao armazenamento de dados. Você pode usar o Integration Runtime (auto-hospedado) (se o armazenamento de dados estiver em uma rede privada) ou o Azure Integration Runtime. Se não for especificado, o Azure Integration Runtime será usado.	Não

Exemplo de um serviço vinculado
Banco de Dados SQL do Azure
O exemplo a seguir cria um serviço vinculado chamado "AzureSqlLinkedService" que se conecta a um Banco de Dados SQL do Azure chamado "ctosqldb" com o id . nome de usuário "ctesta-oneill" e senha "P@ssw0rd".

Armazenamento de Blobs do Azure
O exemplo a seguir cria um serviço vinculado chamado "StorageLinkedService" que se conecta a um repositório de blobs do Azure chamado "ctostorageaccount" com a chave da conta de armazenamento usada para se conectar ao repositório de dados.

Criação de conjuntos de dados
Preenchidas
100 EXP
6 minutos
Um conjunto de dados é uma exibição nomeada de dados que simplesmente aponta ou faz referência aos dados que você deseja usar em suas atividades como entradas e saídas. Os conjuntos de dados identificam dados em diferentes armazenamentos de dados, como tabelas, arquivos, pastas e documentos. Por exemplo, um conjunto de dados de blob do Azure especifica o contêiner de blob e a pasta de armazenamento de blob da qual a atividade deve ler os dados.

Um Data Factory Information Set pode ser definido como um objeto dentro da atividade Copy Data, como um objeto autônomo ou em um formato JSON para criação programática da seguinte forma:

A tabela a seguir descreve as propriedades do JSON acima:

Propriedade	Descrição	Obrigatoriedade
inhame	Nome do conjunto de dados.	Sim
modelo	Tipo de conjunto de dados. Especifique um dos tipos suportados pelo Data Factory (por exemplo: AzureBlob, AzureSqlTable).	Sim
esquema	Esquema do conjunto de dados.	Não
typePropriedades	As propriedades de tipo são diferentes para cada tipo (por exemplo: Azure Blob, Azure SQL Table).	Sim
Exemplo de um conjunto de dados
Blobs do Azure
Neste procedimento, você criará dois conjuntos de dados: InputDataset e OutputDataset. Esses conjuntos de dados são do tipo Binário. Eles fazem referência ao serviço vinculado do Armazenamento do Azure chamado AzureStorageLinkedService. O conjunto de dados de entrada representa os dados de origem na pasta de entrada. Na definição do conjunto de dados de entrada, você especifica o contêiner de blob (adftutorial), a pasta (entrada) e o arquivo (emp.txt) que contém os dados de origem. O conjunto de dados de saída representa os dados que são copiados para o destino. Na definição do conjunto de dados de saída, você especifica o contêiner de blob (adftutorial), a pasta (saída) e o arquivo para o qual copiar os dados.

Na área de trabalho, crie uma pasta chamada ADFv2QuickStartPSH na unidade C.

Crie um arquivo JSON chamado InputDataset.json na pasta C:\ADFv2QuickStartPSH com o seguinte conteúdo:

Próxima unidade: Criar atividades e pipelines de data factory

Criar atividades e pipelines de data factory
Preenchidas
100 EXP
9 minutos
As atividades dentro do Azure Data Factory definem as ações a serem executadas nos dados e, para isso, existem três categorias, incluindo:

Atividades de movimentação de dados
Atividades de transformação de dados
atividades de controle
Atividades de movimentação de dados
As atividades de movimentação de dados simplesmente movem dados de um armazenamento de dados para outro. Você pode usar a atividade de cópia para realizar atividades de movimentação de dados ou via JSON. Há uma ampla variedade de armazenamentos de dados com suporte como fonte e como coletor. Esta lista está crescendo o tempo todo e você pode encontrar as informações mais recentes aqui .

Atividades de transformação de dados
As atividades de transformação de dados podem ser executadas nativamente na ferramenta de criação do Azure Data Factory usando o fluxo de dados de mapeamento. Como alternativa, você pode chamar um recurso de computação para alterar ou aprimorar os dados por meio da transformação ou realizar análises nos dados. Isso inclui tecnologias de computação como Azure Databricks, Azure Batch, SQL Database e Azure Synapse Analytics, Machine Learning Services, Azure Virtual Machines e HDInsight. Você pode usar qualquer pacote existente do SQL Server Integration Services (SSIS) armazenado em um catálogo para ser executado no Azure.

Como esta lista está sempre evoluindo, você pode obter as informações mais recentes aqui .

atividades de controle
Ao criar soluções ADF graficamente, você pode usar o fluxo de controle em seu design para orquestrar atividades de pipeline que incluem atividades de encadeamento em uma sequência, ramificação, definição de parâmetros de nível de pipeline e argumentos de etapas ao invocar o pipeline sob demanda ou a partir de um gatilho. Os recursos atuais incluem:

atividade de monitoramento	Descrição
Atividade de execução do pipeline	A atividade de execução do pipeline permite que um pipeline do Data Factory invoque outro pipeline.
Para cada atividade	A atividade ForEach define um fluxo repetido de controle no pipeline. Essa atividade é usada para iterar em uma coleção e executar as atividades especificadas em um loop. A implementação de loop dessa atividade é semelhante à estrutura de loop ForEach das linguagens de programação.
atividade na web	A atividade da Web pode ser usada para chamar um endpoint REST personalizado de um pipeline do Data Factory. Você pode passar conjuntos de dados e serviços vinculados que a atividade consumirá e acessará.
atividade de pesquisa	A atividade de pesquisa pode ser usada para ler ou pesquisar um registro, nome de tabela ou valor de qualquer fonte externa. Além disso, as atividades subsequentes podem fazer referência a essa saída.
Obter atividade de metadados	A atividade GetMetadata pode ser usada para recuperar metadados de qualquer dado no Azure Data Factory.
Até Atividade	Ele implementa o loop Do-Until, que é semelhante à estrutura do loop Do-Until das linguagens de programação. Executa um conjunto de atividades em um loop até que a condição associada à atividade seja avaliada como verdadeira. Você pode especificar um valor de tempo limite para a atividade Until no Data Factory.
Atividade Se Condição	A condição If pode ser usada para criar uma ramificação com base em uma condição avaliada como verdadeira ou falsa. A atividade de condição IF fornece a mesma funcionalidade que uma instrução If fornece em linguagens de programação. Avalia um conjunto de atividades quando a condição é avaliada como verdadeira e outro conjunto de atividades quando a condição é avaliada como falsa.
esperar atividade	Quando você usa uma atividade Wait em um pipeline, ela aguarda o tempo especificado antes de continuar a executar atividades sucessivas.
Você pode obter as informações mais recentes aqui .

atividades e tubulações
Definir atividades
Ao usar a notação JSON, a seção de atividades pode ter uma ou mais atividades definidas nela. Existem dois tipos principais de atividades: atividades de execução e de controle. As atividades de execução (também conhecidas como processamento) incluem atividades de movimentação e transformação de dados. Eles têm a seguinte estrutura de nível superior:

atividades e tubulações
Definir atividades
Ao usar a notação JSON, a seção de atividades pode ter uma ou mais atividades definidas nela. Existem dois tipos principais de atividades: atividades de execução e de controle. As atividades de execução (também conhecidas como processamento) incluem atividades de movimentação e transformação de dados. Eles têm a seguinte estrutura de nível superior:

JSON

cópia de
{
    "name": "Execution Activity Name",
    "description": "description",
    "type": "<ActivityType>",
    "typeProperties":
    {
    },
    "linkedServiceName": "MyLinkedService",
    "policy":
    {
    },
    "dependsOn":
    {
    }
}
A tabela a seguir descreve as propriedades do JSON acima:

Propriedade	Descrição	Obrigatoriedade
inhame	Nome da atividade.	Sim
Descrição	Texto que descreve para que a atividade é usada.	Não
modelo	Define o tipo da atividade.	Sim
linkedServiceName	Nome do serviço vinculado usado pela atividade.	Sim para HDInsight, atividade de pontuação em lote de aprendizado de máquina e atividade de procedimento armazenado.
typePropriedades	As propriedades na seção typeProperties dependem de cada tipo de atividade.	Não
políticas	Diretivas que afetam o comportamento de tempo de execução da atividade. Essa propriedade inclui o comportamento de tempo limite e de repetição.	Não
depende de	Essa propriedade é usada para definir dependências de atividade e como as atividades subsequentes dependem das atividades anteriores.	Não
Definir atividades de controle

Gerenciando ambientes de tempo de execução de integração
Preenchidas
100 EXP
6 minutos
No Data Factory, uma atividade define a ação a ser executada. Um serviço vinculado define um armazenamento de dados de destino ou serviço de computação. Um ambiente de tempo de execução de integração fornece a infraestrutura para a atividade e os serviços vinculados.

A atividade ou serviço vinculado faz referência ao Integration Runtime e fornece o ambiente de computação de onde a atividade é executada ou despachada. Dessa forma, a atividade pode ser executada na região mais próxima possível do serviço de computação ou armazenamento de dados de destino da maneira mais eficiente, atendendo às necessidades de segurança e conformidade.

Em suma, o Integration Runtime (IR) é a infraestrutura de computação usada pelo Azure Data Factory. Ele fornece os seguintes recursos de integração de dados em vários ambientes de rede, incluindo:

Fluxo de dados - execute uma instância de fluxo de dados no ambiente de computação do Azure gerenciado.
Movimentação de dados : Copie dados entre armazenamentos de dados em redes públicas e armazenamentos de dados em redes privadas (rede privada local ou virtual). Fornece suporte para conectores integrados, conversão de formato, mapeamento de coluna e transferência de dados escalável e de alto desempenho.
Expedição de atividades – despache e monitore atividades de transformação em execução em uma variedade de serviços de computação, como Azure Databricks, Azure HDInsight, Azure Machine Learning, Banco de Dados SQL do Azure, SQL Server e muito mais.
Executando pacotes SSIS - execute nativamente pacotes do SQL Server Integration Services (SSIS) em um ambiente de computação gerenciado do Azure.
Sempre que um Azure Data Factory é criado, é criado um ambiente de tempo de execução de integração padrão que dá suporte a operações em armazenamentos de dados em nuvem e serviços de computação na rede pública. Ele pode ser visto quando o tempo de execução da integração é configurado para resolução automática.

Tipos de instâncias de tempo de execução de integração
O Data Factory oferece três tipos de instâncias do Integration Runtime e você deve escolher o tipo que melhor atende aos recursos de integração de dados e às necessidades do ambiente de rede que você está procurando. Esses três tipos são:

Azure
auto-hospedado
Azure SSIS
Você pode definir explicitamente o valor do Integration Runtime na propriedade connectVia , se não for definido, o tempo de execução de integração padrão será usado com a propriedade definida para resolução automática.

A tabela a seguir descreve os recursos e o suporte de rede para cada um dos tipos de instância do Integration Runtime:

Tipo de RI	rede pública	Rede privada
Azure	Enviar atividade de movimentação de dados do fluxo de dados	
auto-hospedado	Enviar atividade de movimentação de dados	Enviar atividade de movimentação de dados
Azure SSIS	Execução de pacotes SSIS	Execução de pacotes SSIS
Determinar qual tempo de execução de integração usar
Há vários fatores que afetam o ambiente do Integration Runtime a ser usado. O seguinte é um guia para ajudá-lo a selecionar o IR correto

copiar atividade
Para atividade de cópia, você precisa de serviços vinculados à origem e ao coletor para definir a direção do fluxo de dados. A lógica a seguir é usada para determinar qual instância do Integration Runtime é usada para executar a cópia:

Copie entre duas fontes de dados na nuvem: se o serviço vinculado de origem e coletor usar o Azure IR, o ADF usará o Azure IR regional, se você o especificou, ou determinará automaticamente um local para o Azure IR se você optar por usar a opção Azure IR. resolução automática de IR (a opção padrão) conforme descrito na seção Colocação do Integration Runtime.

Copiando dados entre uma fonte de dados na nuvem e uma fonte de dados em uma rede privada: se a fonte ou o serviço vinculado ao coletor apontar para uma instância de IR auto-hospedada, a atividade de cópia será executada nessa instância do Integration Runtime auto-hospedada.

Copiar entre duas fontes de dados em uma rede privada: o serviço vinculado de origem e coletor deve apontar para a mesma instância de tempo de execução de integração e esse ambiente é usado para executar a atividade de cópia.

Pesquisa de atividade e GetMetadata
A atividade Lookup e GetMetadata é executada no ambiente de tempo de execução de integração associado ao serviço vinculado ao data warehouse.

atividade de transformação
Cada atividade de transformação tem um destino de serviço vinculado de computação de destino, que aponta para uma instância do Integration Runtime. Essa instância do Integration Runtime é de onde a atividade de transformação é despachada.

Atividade de fluxo de dados
A atividade de fluxo de dados é executada no ambiente de tempo de execução de integração associado a ela.

Próxima Unidade: Verificação de Conhecimento


Obtención de soluciones analíticas integradas con Azure Synapse Analytics

Introdução
Preenchidas
100 EXP
3 minutos
Saiba como o Azure Synapse Analytics resolve o problema de ter um único serviço para atender à ampla variedade de requisitos de análise que as organizações enfrentam hoje.

Nesta lição, você aprenderá o seguinte:

O que é o Azure Synapse Analytics
Como funciona o Azure Synapse Analytics
Quando usar o Azure Synapse Analytics
Antes de concluir esta lição, é recomendável que o aluno seja capaz de fazer o seguinte:

Entre no portal do Azure.
Explicar e criar grupos de recursos.
Próxima unidade: O que é o Azure Synapse Analytics

O que é o Azure Synapse Analytics
Preenchidas
100 EXP
5 minutos
O Azure Synapse Analytics é uma plataforma de análise integrada que combina armazenamento de dados, análise de big data, integração de dados e visualização em um único ambiente. O Azure Synapse Analytics permite que os usuários obtenham acesso rápido e insights sobre todos os seus dados, independentemente de seus recursos, trazendo um novo nível de desempenho e escalabilidade.

O Gartner define vários tipos analíticos aos quais o Azure Synapse Analytics dá suporte, incluindo o seguinte:

Análise descritiva
A análise descritiva responde à pergunta "O que está acontecendo no meu negócio?". Os dados para responder a essa pergunta geralmente são obtidos pela criação de um data warehouse. O Azure Synapse Analytics aproveita a funcionalidade de pool SQL dedicado, que permite criar um armazenamento de dados persistente para realizar esse tipo de análise. Você também pode usar o pool SQL sem servidor para preparar dados de arquivos armazenados em um data lake para criar interativamente um data warehouse.

análise diagnóstica
A análise diagnóstica se concentra em responder à pergunta "Por que isso está acontecendo?" Isso pode envolver a exploração de informações que já existem em um data warehouse, mas normalmente envolve uma pesquisa mais ampla do estado dos dados para encontrar mais dados que suportem esse tipo de análise.

Você pode usar a mesma funcionalidade de pool de SQL sem servidor no Azure Synapse Analytics, que permite explorar dados interativamente em um data lake. Os pools de SQL sem servidor permitem que um usuário encontre rapidamente dados adicionais que podem ajudá-lo a entender por que algo está acontecendo.

análise preditiva
O Azure Synapse Analytics também permite que você responda à pergunta "O que provavelmente acontecerá no futuro com base em tendências e padrões anteriores?" usando o mecanismo interno do Apache Spark. Os pools do Azure Synapse Spark podem ser usados com outros serviços, como Azure Machine Learning Services ou Azure Databricks.

análise prescritiva
Esse tipo de análise examina a execução de ações com base na análise de dados em tempo real ou quase em tempo real, por meio do uso de análise preditiva. O Azure Synapse Analytics fornece essa funcionalidade por meio do Apache Spark e do Apache Synapse Link e por meio da integração de tecnologias de streaming, como o Azure Stream Analytics.

O Azure Synapse Analytics oferece aos usuários do serviço a liberdade de consultar dados em seus próprios termos, usando recursos dedicados ou sem servidor em escala. O Azure Synapse Analytics reúne esses dois mundos com uma experiência unificada de integração de dados para ingerir, preparar, gerenciar e fornecer dados usando pipelines do Azure Synapse. Além disso, você pode visualizar dados na forma de painéis e relatórios para análise imediata usando o Power BI, que também está integrado ao serviço.

Próxima unidade: Como funciona o Azure Synapse Analytics

Como funciona o Azure Synapse Analytics
Preenchidas
100 EXP
8 minutos
O Azure Synapse Analytics pode atuar como um único hub para todas as suas necessidades de análise em um ambiente integrado, caso você ainda não tenha um ambiente de análise. Para isso, ele fornece as seguintes funcionalidades:

Recursos de análise oferecidos por meio do Azure Synapse SQL usando pools SQL dedicados ou sem servidor
O Azure Synapse SQL é um sistema de consulPool do Apache Spark com suporte total para Scala, Python, SparkSQL e C#
Você pode desenvolver soluções de engenharia de aprendizado de máquina e big data com o Apache Spark para Azure Synapse. Aproveite o mecanismo de cálculo de big data para lidar com transformações de computação complexas que levariam muito tempo em um data warehouse. Para cargas de trabalho de aprendizado de máquina, você pode usar algoritmos SparkML e integração AzureML para Apache Spark 2.4 com suporte interno para Linux Foundation Delta Lake. Há um modelo simples que permite provisionar e dimensionar clusters Spark para atender às suas necessidades de computação, independentemente de quais operações você executa em seus dados.

Integração de dados com pipelines do Azure Synapse
Os pipelines do Azure Synapse aproveitam os recursos do Azure Data Factory e são um serviço de integração de dados e ETL baseado em nuvem que permite criar fluxos de trabalho controlados por dados para orquestrar a movimentação e a transformação de dados em larga escala. Com os pipelines do Azure Synapse, você pode criar e agendar fluxos de trabalho controlados por dados (chamados pipelines) que podem ingerir dados de diferentes armazenamentos de dados. Você pode criar processos ETL complexos que transformam dados visualmente com fluxos de dados ou por meio de serviços de computação, como Azure Databricks.

Executando análises operacionais com processamento transacional e analítico híbrido quase em tempo real com o Azure Synapse Link
O Azure Synapse Analytics permite que você se comunique com dados operacionais usando o Azure Synapse Link, sem afetar o desempenho de seu armazenamento de dados transacionais. Para que isso aconteça, você deve habilitar o recurso no Azure Synapse Analytics e no armazenamento de dados ao qual o Azure Synapse Analytics se conectará, como o Azure Cosmos DB. Para o Azure Cosmos DB, será criado um armazenamento de dados de análise. À medida que os dados são alterados no sistema transacional, os dados alterados são alimentados no repositório analítico em um formato de repositório de colunas, a partir do qual o Azure Synapse Link pode consultar perfeitamente o sistema de origem.
O Azure Synapse Studio é uma única interface de usuário da Web que permite:

Explore o estado dos dados.
Desenvolva notebooks e scripts TSQL para interagir com mecanismos analíticos.
Crie pipelines de integração de dados para gerenciar a movimentação de dados.
Monitore as cargas de trabalho dentro do serviço.
Gerenciar componentes de serviço.
Integração com várias tecnologias do Azure Data Platform
Para organizações que já possuem soluções de análise, o Azure Synapse Analytics pode ser integrado a muitas tecnologias para complementá-las. Por exemplo, se você já usa o Azure Data Factory para criar pipelines de integração de dados, eles podem ser usados para carregar dados no Azure Synapse Analytics. Você também pode integrar qualquer projeto de ciência de dados ou preparação de dados que tenha no Azure Databricks. A integração com muitos dos componentes de segurança do Azure também é fornecida, permitindo garantir que você atenda aos requisitos de segurança e conformidade da sua organização. Na implantação inicial do Azure Synapse Analytics, existem alguns recursos implantados junto com ele,

No portal do Azure, há links para configurar seu workspace, gerenciar o acesso usando o controle de acesso (IAM), firewalls, identidades gerenciadas e conexões de ponto de extremidade privado.

Ele também contém informações importantes sobre o ambiente Synapse Analytics, como o seguinte:

A URL da conta principal do ADLS Gen2 (1) , que identifica a conta de armazenamento principal do data lake.
SQL Endpoint e SQL On- Demand Endpoint (2) , usados ​​para integração com ferramentas externas, como SQL Server Management Studio (SSMS), Azure Data Studio e Power BI.
A URL da web do espaço de trabalho (3) , um link direto para o Synapse Studio para o espaço de trabalho.
Recursos disponíveis, como pools do SQL e pools do Apache Spark (4) .
Próxima unidade: Quando usar o Azure Synapse Analytics

Quando usar o Azure Synapse Analytics
Preenchidas
100 EXP
4 minutos
Em organizações e setores, casos de uso comuns para Azure Synapse Analytics são identificados pela necessidade do seguinte:

Armazenamento de dados moderno
Isso implica a capacidade de integrar todos os dados, incluindo big data, para raciocinar sobre dados para fins de análise e relatório de uma perspectiva analítica descritiva, independentemente de sua localização ou estrutura.

análise avançada
Ele permite que as organizações executem análises preditivas com os recursos nativos do Azure Synapse Analytics e integração com outras tecnologias, como Azure Databricks.

Exploração e descoberta de dados
A funcionalidade de pool de SQL sem servidor fornecida pelo Azure Synapse Analytics permite que analistas de dados, engenheiros de dados e cientistas de dados explorem a propriedade de dados. Essa funcionalidade oferece suporte à descoberta de dados, análise de diagnóstico e análise exploratória de dados.

análise em tempo real
O Azure Synapse Analytics pode capturar, armazenar e analisar dados em tempo real ou quase em tempo real com recursos como Azure Synapse Link ou integrando serviços como Azure Stream Analytics e Azure Data Explorer.

integração de dados
Os pipelines do Azure Synapse permitem ingerir, preparar, modelar e fornecer dados a serem usados por sistemas downstream. Os componentes do Azure Synapse Analytics podem usar isso exclusivamente.

Análise Integrada
Com a variedade de análises que podem ser realizadas nos dados à sua disposição, reunir serviços em uma solução coesa pode ser uma operação complexa. O Azure Synapse Analytics remove essa complexidade integrando o cenário de análise em um serviço. Assim, você pode gastar mais tempo trabalhando com dados para oferecer benefícios comerciais, em vez de gastar muito tempo provisionando e mantendo vários sistemas para obter os mesmos resultados.

Próxima Unidade: Verificação de Conhecimento
Introdução
Preenchidas
100 EXP
3 minutos
Nesta unidade, você aprenderá como o Azure Synapse Analytics permite criar data warehouses usando padrões de arquitetura modernos.

Nesta unidade, você aprenderá o seguinte:

Descreva um data warehouse moderno.
Defina uma arquitetura de data warehouse moderna.
Projete padrões de ingestão para armazenamento de dados moderno.
Entenda o armazenamento de dados para um data warehouse moderno.
Prepare e transforme dados com o Azure Synapse Analytics.
Sirva os dados para análise com o Azure Synapse Analytics.
Antes de concluir esta unidade, é recomendável que o aluno seja capaz de fazer o seguinte:

Entre no portal do Azure.
Explicar e criar grupos de recursos.
Próxima unidade: Descreva um data warehouse moderno.

Descreva um data warehouse moderno.
Preenchidas
100 EXP
4 minutos
Um data warehouse moderno permite reunir todos os seus dados em qualquer escala com facilidade, e isso significa que você pode obter insights por meio de painéis analíticos, relatórios operacionais ou análises avançadas para todos os usuários.

O ritmo da mudança nos recursos de tecnologia e a natureza elástica dos serviços em nuvem abriram novas oportunidades para desenvolver armazenamento de dados para gerenciar cargas de trabalho modernas, incluindo:

Maiores volumes de dados
Os Serviços do Microsoft Azure têm a capacidade de dimensionar a capacidade para atender às demandas que uma organização enfrenta à medida que os dados crescem. Em dados locais tradicionais, dimensionar servidores locais é uma tarefa não trivial que envolve custos, aquisição de hardware adicional, bem como possível interrupção dos negócios para atender à demanda. Com o Azure, serviços como o Azure Synapse Analytics podem ser dimensionados com o clique de um botão e podem até ser dimensionados automaticamente.

Novas variedades de dados
O Azure Data Lake Store Gen2 também simplifica a preparação de dados, pois pode armazenar uma ampla variedade de dados em seu formato bruto, facilitando muito o processo de ingestão de dados em um data warehouse.

No passado, os data warehouses tradicionais lutavam para lidar com certos tipos de dados. Por exemplo, extrapolar dados de fontes como arquivos PDF usando arquivos de som era muito complexo ou tinha um custo proibitivo. As melhorias nas tecnologias de IA, como o Reconhecimento de Formulários e a Fala em Texto dos Serviços Cognitivos, significam que esses tipos de fontes de dados agora podem ser passados por uma instância dos Serviços Cognitivos e enviados em um formato baseado em texto que pode ser armazenado no Azure Data Lake Store Gen2, juntamente com os próprios arquivos de origem.

taxas de dados
Os data warehouses locais tradicionais no primário foram desafiados com a movimentação de lotes de dados de forma programada. Algumas organizações podem querer criar um data warehouse em tempo real se a necessidade de negócios for atraente e a organização puder absorver o custo de implementação. O Azure tornou mais fácil e econômico fornecer serviços de streaming que podem interagir com uma ampla variedade de serviços para que um data warehouse moderno possa fornecer soluções em lote ou em tempo real sem que o custo seja um problema.

Próxima Unidade: Definindo uma Arquitetura Moderna de Data Warehouse

O fluxo de dados executa as seguintes funções:

Extraia dados da fonte de dados SAP HANA ( Selecione a etapa DatafromSAPHANA ).
Recupera apenas as linhas de uma atividade Upsert, em que o valor de ShipDate é maior que 2014-01-01 ( Selecione a etapa Last5YearsData ).
Executa transformações de tipo de dados nas colunas de origem usando uma atividade Coluna Derivada ( Selecione a atividade Coluna Derivada superior ).
No caminho superior do fluxo de dados, todas as colunas são selecionadas e, em seguida, os dados são carregados na tabela de grupos AggregatedSales_SAPHANANew Synapse ( Selecione a atividade Selectallcolumns e a atividade LoadtoAzureSynapse ).
No caminho inferior do fluxo de dados, selecionamos um subconjunto das colunas ( Select activity SelectRequiredColumns ).
Eles são então agrupados por quatro das colunas ( Selecione a atividade TotalSalesByYearMonthDay ) e crie agregados Sum e Average na coluna SalesAmount ( Selecione a opção Aggregates ).
Finalmente, os dados agregados são carregados na tabela do grupo AggregatedSales_SAPHANA Synapse ( Select LoadtoSynapse activity ).
Próxima Unidade: Entendendo o Armazenamento de Dados para um Data Warehouse Moderno


Visão geral do armazenamento de dados para um data warehouse moderno
Preenchidas
100 EXP
7 minutos
Embora você tenha a oportunidade de ingerir dados da origem diretamente em um data warehouse, é mais comum armazenar os dados de origem em uma área de preparo, também conhecida como zona de destino. Normalmente, essa é uma área de armazenamento neutra que fica entre os sistemas de origem e o armazenamento de dados. O principal motivo para adicionar uma área de preparação a uma arquitetura moderna de data warehouse é um dos seguintes motivos:

Para reduzir a contenção em sistemas de origem
Normalmente, os sistemas de origem desempenham um papel importante na condução de operações de negócios que trazem receita para uma organização ou fornecem uma função essencial para os negócios. Como resultado, a ingestão de dados desses sistemas deve minimizar o uso de recursos no sistema de origem para que não seja interrompido. Como resultado, algumas estratégias de projeto de data warehouse envolverão a busca de dados em uma fonte e o "despejo" dos dados em uma área de preparação.

Esta abordagem não envolve nenhuma transformação ou limpeza. Ele simplesmente captura os dados, reduzindo assim a contenção no sistema de origem. Isso também pode envolver a saída dos dados para arquivos de texto do sistema de origem, que são coletados pelo processo de extração, transformação e carregamento (ETL).

Permite lidar com a ingestão de sistemas de origem em horários diferentes.
Os ambientes de teste fornecem um ótimo local para armazenar dados de diferentes sistemas de origem, independentemente do agendamento em que os dados são ingeridos. Por exemplo, você pode buscar dados de alguns sistemas de origem no final da tarde porque é quando eles estão no nível mais baixo e, então, você pode não conseguir buscar dados de outro sistema até as primeiras horas da manhã, pois eles têm um processo de backup em execução que precisa ser concluído antes que eles possam ingerir os dados. Ter uma área de preparação permite que você gerencie esses diferentes horários.

Para combinar dados de diferentes sistemas de origem
Um ambiente de teste oferece a oportunidade de reunir uma única visualização de dados de diferentes sistemas de origem. Como a área de teste é independente dos sistemas de origem e do armazenamento de dados, você pode fazer qualquer trabalho que precisar sem afetar esses sistemas.

Você pode até criar tabelas adicionais que ajudam no processo de combinação de dados de diferentes sistemas de origem, chamadas tabelas de mapeamento. Nesse cenário, imagine que você tenha uma tabela de clientes em um sistema de origem com uma coluna chamada FirstName. Em um segundo sistema de origem, talvez executando um sistema AS400, você tem a tabela de clientes com uma coluna denominada FIRNAM que também representa o nome do cliente.

Você pode criar uma tabela separada que contém metadados que mapeiam dados de uma coluna de um sistema de origem, com outra coluna de outro sistema de origem que representa a mesma entidade comercial. Nesse caso, FirstName.

Para executar novamente carregamentos de data warehouse com falha de uma área de preparação
Nem todas as cargas de data warehouse serão concluídas com êxito, portanto, o data warehouse deve ser capaz de lidar com cenários em que o processo de ETL pode precisar ser reexecutado durante o horário comercial principal e deve ser executado sem interromper novamente os sistemas de origem. Ao manter os dados em teste, você pode executar novamente o processo ETL da área de teste, em vez do sistema de origem.

Em uma arquitetura de data warehouse moderna, os dados de origem podem variar. A variedade e o volume de dados gerados e analisados hoje estão aumentando. As empresas têm uma variedade de fontes de dados, de sites a sistemas de ponto de venda (POS) e, mais recentemente, de redes sociais a dispositivos de Internet das Coisas (IoT). Cada fonte fornece um aspecto essencial dos dados que precisam ser extraídos, analisados e potencialmente manipulados.

Com base nisso, o Azure Data Lake Storage Gen2 é a solução de armazenamento ideal para hospedar dados provisórios, pois contém um conjunto de recursos dedicados à análise de big data, conhecido como data lake. O termo "data lake" define um repositório de dados que é armazenado em seu formato natural, normalmente como blobs ou arquivos. O Azure Data Lake Storage é uma solução de data lake completa, escalável e econômica para análise de big data criada no Azure.

O Azure Data Lake Storage combina um sistema de arquivos com uma plataforma de armazenamento para ajudar a identificar rapidamente insights nos dados. O Azure Data Lake Storage Gen2 se baseia nos recursos do armazenamento de BLOBs do Azure para ser otimizado especificamente para cargas de trabalho de análise. Essa integração facilita o desempenho analítico, os recursos de gerenciamento do ciclo de vida de dados e a classificação em camadas do armazenamento de Blobs e os recursos de alta disponibilidade, segurança e durabilidade do Armazenamento do Azure.

Próxima Unidade: Informações sobre a Estrutura e Formatos de Arquivo do Armazenamento de Dados Moderno



Informações sobre a estrutura e os formatos de arquivo do armazenamento de dados moderno
Preenchidas
100 EXP
6 minutos
Ao carregar dados no data warehouse, os tipos de arquivo e os métodos para ingerir os dados variam de acordo com a origem. Por exemplo, carregar dados de sistemas de arquivos locais, armazenamentos de dados relacionais ou fontes de dados de streaming requer abordagens diferentes, desde a ingestão no data lake ou data warehouse de preparação, até o desembarque de dados limpos na camada de serviço. É importante entender os diferentes tipos de arquivos e quais usar para armazenamento bruto versus versões refinadas para consultas analíticas. Outras considerações de design incluem estruturas hierárquicas para otimizar consultas e atividades de carregamento de dados. Esta unidade descreve os tipos de arquivo e seus casos de uso ideais e como organizá-los melhor no data lake.

Formatos de arquivo suportados para ingestão de dados brutos em lote
Na engenharia de dados, a velocidade de upload de dados tende a ser descrita como uma das três latências:

Lote – Consultas ou programas que levam dezenas de minutos, horas ou dias para serem concluídos. As atividades podem incluir limpeza e transformação de dados, pipeline completo de ETL ou preparação para análise downstream.
Consulta interativa – Consulta em lote de dados em velocidades "humanas" interativas, o que, com a atual geração de tecnologias, significa que os resultados ficam prontos em períodos de tempo medidos em segundos ou minutos.
Tempo real/tempo quase real : Processando um fluxo de dados de entrada normalmente infinito (stream), o tempo para o qual os resultados ficam prontos é curto, medido em milissegundos ou segundos na maioria dos casos.
Quando se trata de ingerir dados brutos em lote de novas fontes de dados, o Synapse oferece suporte nativo a estes formatos de dados:

CSV
Parquet
ORC
JSON
No entanto, se você usar o Apache Spark com notebooks Synapse, poderá explorar e processar mais tipos de arquivos para seus dados brutos.

Ingestão de dados de streaming em pools SQL dedicados do Synapse
O processamento de dados que atingem a terceira latência (em tempo real ou quase em tempo real) também é conhecido como processamento de dados de streaming. As fontes de dados de streaming podem incluir dispositivos e sensores de IoT, transações financeiras, informações de fluxo de cliques, fábricas e dispositivos médicos, entre outros.

O Azure oferece serviços de ingestão de fluxo criados especificamente, como Hub IoT do Azure e Hubs de Eventos do Azure (com ou sem suporte Kafka) que são robustos, testados e eficientes. Em seu pipeline de dados, você deve coletar mensagens desses ou de serviços relacionados e processá-los usando Azure Stream Analytics , Azure Functions, Azure Databricks ou outros serviços que permitem ingerir e processar dados de streaming.

O objetivo pode ser trazer esses dados para o data lake, como a conta do Azure Data Lake Storage (ADLS) Gen2 associada ao espaço de trabalho Synapse Analytics e, em seguida, usar notebooks Synapse Spark ou consultas T-SQL com um SQL serverless de grupo para explorar e transformar dados. Nesse caso, você provavelmente salvará esses dados em um dos formatos brutos, como CSV ou JSON. O Azure Stream Analytics simplifica essa tarefa conectando-se ao Hub IoT ou Hubs de Eventos como uma fonte de entrada e enviando arquivos para sua conta de armazenamento ADLS Gen2 como saída. Você pode especificar um padrão de caminho para ajudá-lo a estruturar seus dados para cargas de trabalho analíticas ideais usando a remoção de dados baseada em caminho. Por exemplo, você pode definir o seguintepadrão de caminho na saída ADLS Gen2:. Outras opções permitem definir a serialização, como JSON; a codificação, como UTF-8; o número mínimo de linhas por arquivo, como 100; etc.

No entanto, se você deseja obter os dados de streaming em um pool SQL dedicado, algumas opções que você pode aplicar são primeiro processar e armazenar os dados no data lake, conforme descrito acima. Em seguida, carregue os dados em um pool SQL dedicado usando uma das várias técnicas de carregamento de dados, como pipelines Synapse, pools Synapse Spark ou instruções COPY. Isso permite que você use o data lake como uma área de teste ou mantenha o formato bruto no data lake além de uma versão mais refinada dos dados no pool SQL dedicado.

Outra opção é usar o Azure Stream Analytics com uma instância do Azure Synapse Analytics (pool SQL dedicado) como um coletor de saída para ingestão de dados de alto desempenho com trabalhos do Azure Stream Analytics.

Tipos de arquivo recomendados
dados não tratados
Para dados brutos, é recomendável armazenar os dados em seu formato nativo. Normalmente, os dados do banco de dados relacional devem ser armazenados no formato CSV. Este é o formato suportado pela maioria dos sistemas, por isso oferece mais flexibilidade.

Para dados de APIs da Web e bancos de dados NoSQL, JSON é o formato recomendado.

Versões refinadas para dados
Quando se trata de armazenar versões refinadas de dados para possíveis consultas, o formato de dados recomendado é Parquet .

Há um alinhamento do setor em torno do formato Parquet para compartilhamento de dados na camada de armazenamento (por exemplo, em cenários de mecanismo Hadoop, Databricks e SQL). Parquet é um formato orientado a colunas de alto desempenho otimizado para cenários de big data.

Formatos colunares, como o Parquet, têm vantagens de armazenamento e desempenho. Os valores são agrupados por coluna, portanto, a compactação é mais eficiente (para reduzir o espaço de armazenamento) e um mecanismo de consulta pode desativar as projeções de coluna (para reduzir a E/S de leitura de disco e rede). pulando colunas indesejadas), o que também é conhecido como queda de coluna. Tipos de dados semelhantes (para uma coluna) são armazenados juntos em arquivos Parquet, levando a esquemas de codificação e compactação de dados eficientes.

Parquet armazena o esquema de arquivo nos metadados do arquivo. Os arquivos CSV não armazenam metadados de arquivo, portanto, o esquema deve ser fornecido aos leitores ou o esquema deve ser inferido. Fornecer um esquema é tedioso e induzir um esquema é propenso a erros ou caro.

Organização da estrutura de arquivos para consultas analíticas
A primeira coisa a considerar ao ingerir dados no data lake é como estruturar ou organizar os dados no data lake. Você deve usar o Azure Data Lake Storage (ADLS) Gen2 (no portal do Azure, esta é uma conta de armazenamento do Azure com um namespace hierárquico habilitado).

Um mecanismo chave que permite que o ADLS Gen2 forneça desempenho de sistema de arquivos a preços e escala de armazenamento de objetos, além de um namespace hierárquico. Isso permite que a coleção de objetos ou arquivos em uma conta seja organizada em uma hierarquia de diretórios e subdiretórios aninhados da mesma forma que o sistema de arquivos em seu computador é organizado. Com um namespace hierárquico habilitado, uma conta de armazenamento é capaz de fornecer a escalabilidade e a economia do armazenamento de objetos, com semântica de sistema de arquivos com a qual as estruturas e mecanismos de análise estão familiarizados.

No ADLS Gen2, é uma prática recomendada ter uma conta de armazenamento dedicada para produção e uma conta de armazenamento separada para cargas de trabalho de desenvolvimento/teste. Isso garantirá que as cargas de trabalho de desenvolvimento ou teste nunca interfiram na produção.

Um método comum de estruturar pastas em um data lake é organizar os dados em pastas separadas com base no grau de sofisticação. Por exemplo, uma pasta bronze pode conter dados brutos, prata pode conter dados limpos, preparados e integrados e ouro pode conter dados prontos para dar suporte à análise, que pode incluir aprimoramentos finais, como agregados pré-computados. Se forem necessários mais níveis de refinamento, essa estrutura poderá ser modificada, conforme necessário, para incluir mais pastas.

Os dados brutos são armazenados na pasta bronze, os dados prontos para consulta são armazenados na pasta prata e os dados prontos para relatório são armazenados na pasta ouro.

Ao trabalhar com o Data Lake Storage Gen2, o seguinte deve ser considerado:

Quando os dados são armazenados no Data Lake Storage Gen2, o tamanho dos arquivos, seu número e a estrutura de pastas afetam o desempenho.
Se os dados forem armazenados em muitos arquivos pequenos, isso poderá afetar negativamente o desempenho. Em geral, organize os dados em arquivos maiores para melhorar o desempenho (tamanho de 256 MB a 100 GB).
Alguns mecanismos e aplicativos podem ter problemas para processar arquivos com mais de 100 GB com eficiência.
Os pipelines de dados às vezes têm controle limitado sobre dados brutos, que possuem um grande número de arquivos pequenos. É recomendável ter um processo de "cozinhar" que gere arquivos maiores para uso em aplicativos de download.
Próxima unidade: preparar e transformar dados com o Azure Synapse Analytics.



Prepare e transforme dados com o Azure Synapse Analytics.
Preenchidas
100 EXP
7 minutos
Você pode realizar transformações de dados nativamente com pipelines do Azure Synapse sem código usando a tarefa de fluxo de dados de mapeamento. O mapeamento de fluxos de dados fornece uma experiência completamente visual que não requer programação. Os fluxos de dados serão executados em seu próprio cluster de execução durante o processamento de dados em expansão. As atividades de fluxo de dados podem ser acionadas usando os recursos de agendamento, controle, fluxo e monitoramento existentes no Data Factory.

Ao criar fluxos de dados, você pode habilitar o modo de depuração, que ativa um pequeno cluster interativo do Spark. Para ativar o modo de depuração, alterne o controle deslizante na parte superior do módulo de autoria. Os clusters de depuração levam alguns minutos para serem preparados, mas podem ser usados para visualizar interativamente a saída de sua lógica de transformação.

Depois que o fluxo de dados de mapeamento for adicionado e o cluster Spark for executado, você poderá realizar a transformação e executar e visualizar os dados. Nenhum código é necessário, pois o Azure Data Factory lida com toda a tradução de código, otimização de caminho e execução de trabalhos de fluxo de dados.

Adicionando os dados de origem ao fluxo de dados de mapeamento
Abra a tela de fluxo de dados de mapeamento. Clique no botão Adicionar origem na tela do fluxo de dados. Na lista suspensa de conjuntos de dados de origem, selecione a fonte de dados. Neste exemplo, o conjunto de dados ADLS Gen2 é usado.



Há algumas coisas a ter em mente:

Se o conjunto de dados apontar para uma pasta que contém outros arquivos e você quiser usar apenas um arquivo, talvez seja necessário criar outro conjunto de dados ou usar a parametrização para garantir que apenas um arquivo específico seja lido.
Se você não importou o esquema para sua instância do ADLS, mas já ingeriu os dados, vá para a guia "Esquema" do conjunto de dados e clique em "Importar esquema" para que o fluxo de dados reconheça a projeção do esquema.
O fluxo de dados de mapeamento segue uma abordagem de extração, carregamento e transformação (ELT) e funciona com conjuntos de dados de preparo que estão todos no Azure. Atualmente, os seguintes conjuntos de dados podem ser usados em uma transformação de Origem:

Armazenamento de Blobs do Azure (JSON, Avro, Texto, Parquet)
Azure Data Lake Storage Gen1 (JSON, Avro, Texto, Parquet)
Azure Data Lake Storage Gen2 (JSON, Avro, Texto, Parquet)
Azure Synapse Analytics
Banco de Dados SQL do AzureUsando transformações no fluxo de dados de mapeamento
Agora que você moveu seus dados para o Azure Data Lake Store Gen2, está pronto para criar um fluxo de dados de mapeamento que transformará seus dados em escala usando um cluster Spark e, em seguida, carregá-los em um data warehouse.

As principais tarefas deste processo são as seguintes:

Preparação do ambiente

Adicionar uma fonte de dados

Usando a transformação de fluxo de dados de mapeamento

Gravando em um coletor de dados


Azure Cosmos DB
O Azure Data Factory tem acesso a mais de 80 conectores nativos. Para incluir dados dessas outras fontes em seu fluxo de dados, use a ferramenta de atividade de cópia para carregar esses dados em uma das áreas de preparo com suporte.

Depois que o cluster de depuração estiver preparado, verifique se os dados foram carregados corretamente usando a guia Visualização de dados. Ao clicar no botão Atualizar, o fluxo de dados de mapeamento exibirá um instantâneo da aparência dos dados em cada transformação.

Interface de usuário web única para acessar todos os recursos do Azure Synapse Analyticsta distribuída que permite implementar cenários de armazenamento de dados e virtualização de dados usando experiências T-SQL padrão com as quais os engenheiros de dados estão familiarizados. O Synapse SQL oferece modelos de recursos dedicados e sem servidor para trabalhar com cenários de diagnóstico e análise descritiva. Para custo e desempenho previsíveis, crie pools SQL dedicados para reservar poder de processamento para dados armazenados em tabelas SQL. Para cargas de trabalho ad hoc ou não planejadas, use o endpoint SQL sem servidor sempre disponível.

Execute a engenharia de dados com pools do Apache Spark no Azure Synapse

Introdução
Preenchidas
100 EXP
3 minutos
Saiba mais sobre os casos de uso de engenharia de dados com o Apache Spark no Azure Synapse Analytics.

Após concluir este módulo, você será capaz de:

Descrição do Apache Spark no Azure Synapse Analytics
Descrever como o Apache Spark funciona no Azure Synapse Analytics
Descubra quando usar o Apache Spark no Azure Synapse Analytics.
Pré-requisito
Antes de iniciar este módulo, é recomendável concluir os seguintes módulos:

Noções básicas de dados do Azure
Introdução ao Azure Data Factory
Introdução ao Azure Synapse Analytics
Próxima unidade: O que é um pool do Apache Spark no Azure Synapse Analytics

O que é um pool do Apache Spark no Azure Synapse Analytics
Preenchidas
100 EXP
3 minutos
O Apache Spark é uma plataforma de processamento paralelo que oferece suporte ao processamento na memória para melhorar o desempenho de aplicativos de análise de big data. O mecanismo principal do Apache Spark é um mecanismo de execução distribuído, cujos recursos são gerenciados pela camada Yet Another Resource Negotiator (YARN) para garantir o uso adequado do mecanismo distribuído para processar consultas e trabalhos do Spark. Você pode ter diferentes funcionalidades com base no mecanismo principal do Apache Spark que pode ser adicionado conforme sua análise exigir. Alguns exemplos são adicionar Apache Spark SQL para realizar consultas interativas para análise exploratória de dados, Spark MLib para aprendizado de máquina e GraphX para cálculos de gráficos. Os recursos permitem configurar várias cargas de trabalho em uma única plataforma.

Apache Spark no Azure Synapse Analytics é uma das implementações do Apache Spark na nuvem da Microsoft. O Azure Synapse Analytics facilita a criação e a configuração de um pool Apache Spark sem servidor no Azure. Os pools do Spark no Azure Synapse Analytics dão suporte ao Armazenamento do Azure e ao Azure Data Lake Storage Generation 2. Portanto, você pode usar os pools do Spark para processar dados armazenados no Azure.

Sua principal força é que ele reside na mesma plataforma que contém recursos para construir data warehouses com pools SQL, construir soluções de integração de dados com pipelines do Azure Synapse e integração com serviços como o Microsoft Power BI. Essa integração o torna uma opção atraente para usar o Azure Synapse Analytics, pois pode atuar como um local único para todas as suas soluções de análise.

Os benefícios de usar os pools do Apache Spark no Azure Synapse Analytics são os seguintes:

Velocidade e eficiência – Há um tempo de inicialização do nó rápido e desligamento automático quando as instâncias não são usadas por 5 minutos após o último trabalho, a menos que haja uma conexão de notebook ativa.
Facilidade de criação - a criação como um pool do Apache Spark pode ser feita usando o portal do Azure, PowerShell ou o .NET SDK para Azure Synapse Analytics.
Facilidade de uso - no espaço de trabalho do Azure Synapse Analytics, você pode se conectar diretamente ao pool do Apache Spark e interagir com a experiência de notebook interna ou usar notebooks personalizados derivados do Nteract. A integração do notebook ajuda você a desenvolver pipelines de processamento e visualização de dados interativos.
API REST – Para monitorar e enviar trabalhos remotamente, você pode usar o Apache Livy como uma API REST para o servidor de trabalho Spark. O Apache Livy é um serviço que permite a interação com um cluster Spark por meio de uma interface REST para habilitar o envio de trabalhos do Spark, trechos de código do Spark, recuperação de resultados síncrona ou assíncrona e gerenciamento de contexto do Spark.
Integração com ambientes de desenvolvimento integrado (IDEs) de terceiros – o Azure Synapse Analytics fornece um IDE para o IntelliJ criar e enviar aplicativos para o pool Apache Spark.
Bibliotecas Anaconda pré-carregadas - Mais de 200 bibliotecas Anaconda pré-instaladas no pool Spark no Azure Synapse Analytics.
Escalabilidade – Capacidade de dimensionamento automático, para que os clusters possam ser dimensionados para cima ou para baixo conforme necessário, adicionando ou removendo nós.
Próxima unidade: Como os pools do Apache Spark funcionam no Azure Synapse Analytics



Como os pools do Apache Spark funcionam no Azure Synapse Analytics
Preenchidas
100 EXP
3 minutos
Com o Azure Synapse Analytics, os aplicativos Apache Spark são executados como conjuntos de processos independentes em um pool, coordenados pelo objeto SparkContext no programa principal (chamado de programa de driver). SparkContext pode se conectar ao gerenciador de cluster, que aloca recursos entre aplicativos. O gerenciador de cluster é Apache Hadoop YARN. Uma vez conectado, o Spark adquire executores nos nós do pool, que são processos que executam cálculos e armazenam dados de aplicativos. Em seguida, ele envia o código do aplicativo (definido pelos arquivos JAR ou Python passados para o SparkContext) para os executores. Por fim, o SparkContext envia as tarefas aos executores para execução.

SparkContext executa a função principal do usuário e as várias operações paralelas nos nós. SparkContext então coleta os resultados das operações. Os nós lêem e gravam dados de e para o sistema de arquivos. Além disso, os nós armazenam em cache os dados transformados na memória como conjuntos de dados distribuídos resilientes (RDDs).

O SparkContext se conecta ao pool do Apache Spark e é responsável por transformar um aplicativo em um gráfico acíclico direcionado (DAG). O gráfico consiste em tarefas individuais que são executadas em um processo executor nos nós. Cada aplicativo obtém seus próprios processos executores, que permanecem durante toda a vida útil do aplicativo, e executam tarefas em vários threads.

Os processos são gerenciados automaticamente quando você cria um pool do Apache Spark no Azure Synapse Analytics. Para fazer isso, primeiro você precisaria criar um espaço de trabalho do Azure Synapse Analytics e, em seguida, criar um pool do Apache Spark, conforme mostrado nas etapas abaixo.

Próxima unidade: Quando os pools do Apache Spark são usados no Azure Synapse Analytics?

Quando os pools do Apache Spark são usados ​​no Azure Synapse Analytics?
Preenchidas
100 EXP
3 minutos
Com a variedade de serviços de dados do Apache Spark disponíveis no Azure, a tabela a seguir descreve onde os pools do Apache Spark do Azure Synapse Analytics estão localizados no ecossistema.

Apache Spark	Azure HDInsight	Azure Databricks	Apache Spark para Azure Synapse
o que	Sistema otimizado de memória de código aberto para gerenciar cargas de trabalho de big data.	A implementação da Microsoft do mecanismo Apache Spark de código aberto, gerenciado no domínio do Azure	Uma solução gerenciada de análise avançada Apache Spark como serviço	Funcionalidade do Apache Spark integrada ao Azure Synapse Analytics que reside na mesma plataforma que contém data warehouses e recursos de integração de dados, bem como integração com outros serviços do Azure.
quando	Quando você deseja aproveitar o Apache Spark para processamento de big data ou trabalho de ciência de dados sem acordos de nível de serviço (SLAs) de um fornecedor.	Quando você deseja aproveitar o software de código aberto do Spark com o Contrato de Nível de Serviço de um fornecedor.	Ele fornece engenharia de dados de ponta a ponta, uma solução de ciência de dados e uma plataforma de gerenciamento.	Ele permite que organizações sem implementações existentes do Apache Spark ativem um cluster Apache Spark para atender às necessidades de engenharia de dados sem a sobrecarga das outras plataformas Apache Spark mostradas.
Quem	Profissionais de código aberto	Profissionais de código aberto que desejam um Contrato de Nível de Serviço e especialistas na plataforma de dados da Microsoft	Engenheiros de dados e cientistas de dados que trabalham todos os dias em projetos de big data	Engenheiros de dados, cientistas de dados, especialistas em plataformas de dados e analistas de dados
Por que	Para superar as limitações dos sistemas de multiprocessamento simétrico (SMP) impostas às cargas de trabalho de big data.	Aproveitar a plataforma de análise de big data de software de código aberto com Acordo de Nível de Serviço para garantir a continuidade dos negócios.	Ele fornece a capacidade de criar e gerenciar um projeto de big data e ciência de dados de ponta a ponta usando uma plataforma.	Ele oferece a capacidade de dimensionar de forma eficaz com clusters Apache Spark em uma plataforma de análise centralizada para atender às suas necessidades.
Aqui estão os diferentes serviços de dados do Azure que têm implementações do Apache Spark, conforme mencionado na tabela:

Grupos do Apache Spark no Azure Synapse Analytics:
Os pools do Apache Spark no Azure Synapse Analytics têm recursos do Apache Spark integrados. Para organizações que ainda não têm implementações Apache Spark existentes, os clusters Apache Spark no Azure Synapse Analytics fornecem a funcionalidade de criar um cluster Apache Spark para atender às necessidades de engenharia de dados sem a sobrecarga de outras plataformas Apache Spark. Engenheiros de dados, cientistas de dados, especialistas em plataforma de dados e analistas de dados podem se reunir no Azure Synapse Analytics, onde o cluster Apache Spark é executado, para colaborar rapidamente em uma variedade de soluções de análise.

Apache Spark para Azure Synapse:
O Apache Spark é um sistema de código aberto e otimizado para memória para gerenciar cargas de trabalho de big data usado quando um mecanismo Spark é necessário para processamento de big data ou ciência de dados, independentemente de nenhum acordo de nível ser fornecido Serviço para manter os serviços em execução. Muitas vezes, é útil para profissionais de código aberto e seu papel é superar as limitações do que eram conhecidos como sistemas SMP para cargas de trabalho de big data.

Azure HDInsight (HDI):
O HDI é uma implementação da Microsoft do Apache Spark de código aberto, gerenciado na plataforma Azure. Você pode usar o HDI para um ambiente Apache Spark quando conhece os benefícios do Apache Spark em seu formato de código aberto, mas precisa de um Contrato de Nível de Serviço (SLA). Essa implementação geralmente é de interesse para profissionais de código aberto que precisam de um Contrato de Nível de Serviço e especialistas em plataforma de dados com experiência com produtos e serviços da Microsoft.

Azure Databricks:
O Azure Databricks é uma solução Apache Spark as a Service gerenciada proprietária que fornece uma plataforma de engenharia e ciência de dados de ponta a ponta. O Azure Databricks é do interesse de muitos cientistas e engenheiros de dados que trabalham atualmente em projetos de big data. Ele fornece a plataforma onde você pode criar e gerenciar seus projetos de big data e ciência de dados em uma plataforma.

Esses serviços não são mutuamente exclusivos. É comum encontrar clientes usando uma combinação dessas tecnologias trabalhando em conjunto.

Próxima Unidade: Teste de Conhecimento


Uso de soluciones de procesamiento analítico y transaccional híbridas mediante Azure Synapse Analytics

Introdução
Preenchidas
100 EXP
3 minutos
Saiba como o Link do Azure Synapse Analytics para Cosmos DB resolve os desafios de disponibilizar dados operacionais para consultas analíticas quase em tempo real.

Neste módulo, você aprenderá a:

Entenda os padrões de processamento analítico e transacional híbridos
Descrever as cargas de trabalho de análise compatíveis
Explicar os cenários HTAP no contexto do Synapse Link
Descreva o armazenamento analítico
Entendendo esquemas bem definidos
Esquemas de design que suportam o tipo de análise
Entenda os cenários de reescrita
Requisitos anteriores
Antes de começar com este módulo, é recomendável que você seja capaz de fazer o seguinte:

Ter uma boa compreensão dos recursos do Azure Cosmos DB
Ter conhecimento prático de SQL ou Spark
Próxima Unidade: Entendendo os Padrões Híbridos de Processamento Analítico e Transacional

Entendendo os padrões de processamento analítico e transacional híbridos
Preenchidas
100 EXP
6 minutos
Muitas arquiteturas de aplicativos corporativos separam o processamento analítico e transacional em sistemas separados com dados armazenados e processados em infraestruturas separadas. Essas infraestruturas são comumente conhecidas como sistemas OLTP (Online Transaction Processing), que trabalham com dados operacionais, e sistemas OLAP (Online Analytical Processing), que trabalham com dados históricos. Cada sistema é otimizado para sua tarefa específica.

Os sistemas OLTP são otimizados para lidar imediatamente com solicitações separadas de usuários ou sistemas e respondê-las o mais rápido possível.

Os sistemas OLAP são otimizados para processamento analítico, ingestão, síntese e gerenciamento de grandes conjuntos de dados históricos. Os dados processados por sistemas OLAP se originam em grande parte de sistemas OLTP e devem ser carregados em sistemas OLAP usando processos em lote comumente conhecidos como tarefas ETL (extração, transformação e carregamento de dados).

Devido à sua complexidade e à necessidade de copiar fisicamente grandes quantidades de dados, há um atraso na disponibilidade de dados para fornecer insights por meio de sistemas OLAP.

Quanto mais as empresas adotarem processos digitais, mais será reconhecido o valor de poder responder às oportunidades tomando decisões mais rápidas e informadas. O HTAP (Hybrid Analytical and Transactional Processing) permite que as empresas executem análises avançadas quase em tempo real em dados armazenados e processados por sistemas OLTP.

Link do Azure Synapse para Azure Cosmos DB
O Azure Synapse Link para Azure Cosmos DB é um recurso HTAP nativo da nuvem que permite executar análises quase em tempo real em dados operacionais armazenados no Azure Cosmos DB. O Azure Synapse Link cria uma integração perfeita e perfeita entre o Azure Cosmos DB e o Azure Synapse Analytics.

O Azure Cosmos DB fornece um repositório transacional otimizado para cargas de trabalho transacionais e um repositório analítico otimizado para cargas de trabalho analíticas, bem como um processo de sincronização totalmente gerenciado e totalmente automatizado para manter os dados nesses repositórios sincronizados.

O Azure Synapse Analytics fornece um mecanismo de consulta sem servidor SQL para consultar o repositório analítico usando T-SQL e um mecanismo de consulta Apache Spark popular para aproveitar o repositório analítico com Scala, Java, Python ou SQL, além de um notebook fácil de usar

A combinação do Azure Cosmos DB e do Synapse Analytics permite que as organizações gerem e consumam insights de seus dados operacionais quase em tempo real usando as ferramentas de consulta e análise de sua escolha. Tudo isso é obtido sem a necessidade de pipelines ETL complexos e sem afetar o desempenho dos sistemas OLTP usando o Azure Cosmos DB.

Próxima unidade: descrever as cargas de trabalho de análise compatíveis

Descrever as cargas de trabalho de análise compatíveis
Preenchidas
100 EXP
5 minutos
Veja a seguir casos de uso típicos para o recurso Azure Synapse Link para Azure Cosmos DB para atender às necessidades de negócios do mundo real por meio de análise operacional:

Análise, previsões e relatórios da cadeia de suprimentos
À medida que as cadeias de suprimentos geram volumes crescentes de dados operacionais para pedidos, remessas e transações de vendas a cada minuto, fabricantes e distribuidores precisam de um banco de dados operacional que possa ser dimensionado para controlar os volumes de dados, bem como uma plataforma analítica para alcançar um nível de inteligência contextual em tempo real. tempo que está à frente dos acontecimentos.

O Azure Synapse Link para Cosmos DB permite que essas organizações armazenem dados de seus sistemas de vendas, ingerem dados de telemetria em tempo real de sistemas de veículos e integrem dados de seus sistemas ERP em um armazenamento operacional comum do Azure Cosmos DB para , em seguida, aproveite os dados do Synapse Analytics para habilite cenários de análise preditiva, como monitoramento de estoque e gerenciamento de gargalos da cadeia de suprimentos (1), bem como habilite relatórios operacionais diretamente em seus dados operacionais com ferramentas de relatórios padrão como Power BI (2).

Personalização de negócios em tempo real
No varejo, muitos distribuidores baseados na web realizam análises de carrinho em tempo real para recomendar produtos a clientes que estão prestes a comprar outros. A análise da cesta em tempo real pode aumentar a receita das organizações, pois as sugestões direcionadas podem levar a outras compras de produtos normalmente adquiridos em conjunto.

Manutenção preditiva por meio da detecção de anomalias com IoT
As inovações em IoT industrial reduziram drasticamente o tempo de inatividade das máquinas e aumentaram a eficiência geral em todo o setor. Uma dessas inovações é a análise de manutenção preditiva para máquinas na borda da nuvem.

A arquitetura a seguir aplica os recursos HTAP nativos da nuvem do Azure Synapse Link para Azure Cosmos DB para manutenção preditiva de IoT:

Próxima Unidade: Explicar Cenários HTAP no Contexto do Link Synapse
Explicar os cenários HTAP no contexto do Synapse Link
Preenchidas
100 EXP
5 minutos
O processamento analítico e transacional híbrido (HTAP) permite que as empresas executem análises em sistemas de banco de dados que fornecem recursos transacionais de alto desempenho sem afetar o desempenho desses sistemas. O HTAP permite que as organizações usem um banco de dados para atender às necessidades transacionais e analíticas e dar suporte à análise quase em tempo real de dados operacionais para tomar decisões oportunas.

Por exemplo, a Adventure Works usa o Azure Cosmos DB para armazenar dados de pedido de venda e perfil de cliente de seu site de comércio eletrônico. O armazenamento de documentos NoSQL fornecido pelo Azure Cosmos DB fornece informações sobre como gerenciar seus dados usando a sintaxe SQL, permitindo que você leia e grave seus arquivos em uma escala global massiva.

Embora a Adventure Works esteja satisfeita com os recursos e o desempenho do Azure Cosmos DB, eles estão preocupados com o custo de execução do alto volume de consultas analíticas complexas necessárias para atender aos requisitos de relatórios operacionais. Eles desejam acessar com eficiência todos os seus dados operacionais armazenados no Cosmos DB sem aumentar o desempenho do Azure Cosmos DB e o custo associado. Eles exploraram opções para extrair dados de seus contêineres para o data lake conforme ele muda, usando o mecanismo de feed de alterações do Azure Cosmos DB. O problema com essa abordagem são as dependências adicionais de serviço e código, bem como a manutenção de longo prazo da solução. Eles poderiam fazer exportações em massa de um oleoduto Synapse,

Eles decidem habilitar o Azure Synapse Link para Cosmos DB e o repositório analítico em seus contêineres do Azure Cosmos DB. Com essa configuração, todos os dados transacionais são armazenados automaticamente em um columnstore totalmente isolado. Esse armazenamento permite análises em larga escala de dados operacionais no Azure Cosmos DB, sem afetar as cargas de trabalho transacionais. O Azure Synapse Link para Cosmos DB cria uma integração completa entre o Azure Cosmos DB e o Azure Synapse Analytics, permitindo que a Adventure Works execute análises quase em tempo real em seus dados operacionais sem ETL e com isolamento completo de desempenho de cargas de trabalho transacionais.

Ao combinar a escala distribuída do processamento transacional e armazenamento analítico integrado do Cosmos DB com o poder de computação do Azure Synapse Analytics, o Azure Synapse Link permite uma arquitetura híbrida de processamento transacional e analítico (HTAP) para simplificar os processos de negócios corporativos. Essa integração elimina os processos de ETL, permitindo que analistas de negócios, engenheiros e cientistas de dados façam autoatendimento e executem pipelines de business intelligence, análise e aprendizado de máquina nesses dados operacionais quase em tempo real.

Próxima Unidade: Descreva o Armazém Analítico

Descreva o armazenamento analítico
Preenchidas
100 EXP
7 minutos
O Armazenamento Analítico do Azure Cosmos DB é um armazenamento de colunas totalmente isolado para permitir análises em larga escala em dados operacionais de sua instância do Azure Cosmos DB, sem afetar as cargas de trabalho transacionais.

Armazenamento transacional orientado a linhas
Os dados operacionais de um contêiner do Azure Cosmos DB são armazenados internamente em um "armazenamento transacional" baseado em linha indexado. O formato rowstore e seu índice de árvore B associado são projetados para permitir leituras e gravações transacionais rápidas com tempos de resposta de milissegundos de um dígito e consultas operacionais de alto desempenho. À medida que o conjunto de resultados cresce, consultas analíticas complexas podem se tornar caras, pois usam mais recursos de desempenho provisionados. O aumento do consumo da taxa de transferência provisionada, por sua vez, afeta a taxa de transferência das cargas de trabalho transacionais.

Armazenamento analítico orientado a colunas
O repositório analítico do Azure Cosmos DB aborda os desafios de complexidade e latência que acompanham os pipelines ETL tradicionais. O repositório analítico do Azure Cosmos DB pode sincronizar automaticamente os dados operacionais com um repositório de colunas separado. O formato columnstore é adequado para que consultas analíticas em larga escala sejam realizadas de maneira otimizada, o que aumenta a latência da consulta.

Recursos do Armazém Analítico
Ao criar um contêiner do Azure Cosmos DB, você tem a opção de habilitar o repositório analítico, no qual é criada uma nova estrutura de repositório de colunas que espelha os dados do repositório transacional. Esses dados de estrutura columnstore são armazenados separadamente do armazenamento transacional orientado a linhas com inserções, atualizações e exclusões realizadas no armazenamento transacional sendo copiadas de forma transparente por meio de um processo de sincronização automática interno totalmente gerenciado para o armazenamento.

 Observação

Você só pode habilitar o repositório analítico ao criar um novo contêiner.

 Observação

O Azure Synapse Link dá suporte à API Azure Cosmos DB SQL (Core) e à API Azure Cosmos DB para MongoDB.

Normalmente, os dados são sincronizados automaticamente entre o armazenamento transacional e o armazenamento analítico em menos de 2 minutos, graças ao processo de sincronização automática. No entanto, em algumas circunstâncias, e particularmente em situações em que o banco de dados de desempenho compartilhado tem muitos contêineres, a latência da sincronização automática pode levar até 5 minutos.

Devido ao fato de que o armazenamento transacional e analítico são mantidos e consultados separadamente, as cargas de trabalho associadas a esses armazenamentos são isoladas umas das outras; ou seja, as consultas feitas no repositório analítico (ou no processo de sincronização automática) não afetarão o desempenho ou os recursos (taxa de transferência ou unidades de solicitação) provisionados para o repositório transacional e as operações realizadas nele não afetarão a sincronização automática latência.

 Observação

As transações (leitura e gravação) e os custos de armazenamento do armazenamento analítico são cobrados separadamente do armazenamento e da taxa de transferência do armazenamento transacional.

O processo de sincronização automática também lida com atualizações de esquema no armazenamento analítico de esquema automaticamente, à medida que novas propriedades exclusivas são adicionadas a elementos em seu contêiner ao longo do tempo. Isso permite que você aproveite os benefícios de desempenho que a estrutura de tópicos oferece sem nenhum esforço de sua parte. Veremos como o esquema de armazenamento analítico é gerenciado e exposto aos recursos de consulta do Synapse Analytics na próxima unidade.

Você pode configurar a propriedade de tempo de vida padrão (TTL) para registros armazenados no repositório transacional e no repositório analítico independentemente um do outro. O valor TTL de um registro define quando ele será removido automaticamente da loja. Ao definir o valor de TTL padrão para ambos os armazenamentos, você pode gerenciar o ciclo de vida dos dados e definir por quanto tempo eles serão mantidos em cada armazenamento. Você pode substituir o valor TTL padrão (no nível do item) para o armazenamento transacional; no entanto, o valor TTL padrão sempre será aplicado aos dados na análise e não pode ser substituído no nível do item.

O Azure Cosmos DB dá suporte a contas distribuídas globalmente que replicam de forma transparente seus contêineres nas regiões do Azure de sua escolha. Se ativado em um contêiner, o armazenamento analítico será configurado automaticamente em todas as regiões de distribuição global escolhidas; você não pode escolher seletivamente em quais regiões deseja implementar um armazenamento analítico. Também é recomendável escolher e configurar as regiões de distribuição global em sua conta antes de habilitar o repositório analítico em um contêiner.

Próxima unidade: Entendendo esquemas bem definidos

Entendendo esquemas bem definidos
Preenchidas
100 EXP
9 minutos
Há duas restrições que se aplicam à inferência de esquema realizada pelo processo de sincronização automática, que mantém de forma transparente o esquema no repositório analítico com base nos itens adicionados ou atualizados no repositório transacional:

Você pode ter no máximo 1.000 propriedades em qualquer nível de aninhamento em itens armazenados em um repositório transacional. As propriedades acima dele e seus valores associados não serão incluídos no repositório analítico.
Os nomes de propriedade devem ser exclusivos quando comparados sem distinção entre maiúsculas e minúsculas. Por exemplo, as propriedades {"name": "Franklin Ye"} e {"Name": "Franklin Ye"} não podem sair no mesmo nível de aninhamento de elementos iguais ou diferentes em um contêiner, pois "name " e "Name " não são únicos quando comparados sem distinção entre maiúsculas e minúsculas.
No repositório analítico, há duas maneiras de representar o esquema de seus dados armazenados. Esses modos têm vantagens em termos de simplicidade de renderização colunar, controle de esquemas polimórficos e simplicidade da experiência de consulta:

Representação esquemática bem definida
Representação de esquemas com total fidelidade
Para contas de API SQL (Core), quando o armazenamento analítico está ativado, a representação de esquema padrão nele é a representação bem definida. Considerando que, no caso da conta da API do Azure Cosmos DB para MongoDB, a representação de esquema padrão no repositório analítico é uma representação de esquema de fidelidade total. (Se você tiver cenários que exigem uma representação de esquema diferente do padrão para cada uma dessas APIs, entre em contato com a equipe do Azure Cosmos DB para habilitá-lo.)

Representação esquemática bem definida
A representação de esquema bem definida cria uma representação tabular simples dos dados independentes de esquema no armazenamento transacional conforme você os copia para o armazenamento analítico.

O snippet de código a seguir é um exemplo de documento JSON que representa um registro de perfil de cliente:


A representação de esquema bem definida tem as propriedades de nível superior dos documentos expostos como colunas quando consultados no Synapse SQL e no Synapse Spark, juntamente com os valores das colunas que representam os valores das propriedades. Exceto se esses valores forem do tipo objeto ou array, nesse caso uma representação JSON dos valores da propriedade é mapeada para os valores da coluna, e eles possuem as seguintes considerações adicionais:

As propriedades sempre têm o mesmo tipo entre os elementos.

Por exemplo, {"postcode":98065}e {"postcode": "CR30AA"}não tem um esquema bem definido porque "código postal" às vezes é uma string e às vezes um número. Nesse caso, o armazenamento analítico registra o tipo de dados do atributo "código postal" no armazenamento analítico, como o tipo de dados da propriedade "código postal" na primeira ocorrência no tempo de vida do contêiner; itens em que o tipo de dados do atributo "código postal" difere deste valor registrado não serão incluídos no armazenamento analítico.

Essa condição não se aplica a propriedades NULL.

Por exemplo, {"postcode":98065} {"postcode":null} ainda está bem definido.

Os tipos de matriz devem conter um único tipo repetido.

Por exemplo, {"postcode": [98065, “CR30AA”]} não é um esquema bem definido, porque a matriz contém uma mistura de tipos inteiros e de string.

É importante ressaltar que quando o repositório analítico do Azure Cosmos DB usa um modo de representação de esquema bem definido dos dados armazenados nos documentos adicionados ao contêiner que viola as regras acima, esses documentos não são incluídos no repositório analítico.


epresentação esquemática totalmente fiel
A representação de esquema totalmente fiel cria uma representação tabular mais complexa dos dados independentes de esquema no armazenamento transacional à medida que você os copia para o armazenamento analítico. A representação de esquema totalmente fiel tem as propriedades de nível superior dos documentos expostos como colunas quando consultados no Synapse SQL e no Synapse Spark, juntamente com uma representação JSON dos valores de propriedade que contém como valores de coluna. Isso é estendido para incluir o tipo de dados das propriedades junto com seus valores de propriedade e, como tal, pode controlar melhor os esquemas polimórficos de operações. Com essa representação de esquema, nenhum item é substituído do armazenamento analítico devido à necessidade de aderir a regras de esquema bem definidas.

Esquemas de design que suportam o tipo de análise
Preenchidas
100 EXP
7 minutos
Agora que você entende como os dados são transformados e representados no armazenamento analítico, há algumas considerações adicionais que você deve levar em consideração ao modelar seus dados para que possa suportar consultas analíticas do armazenamento analítico com mais conforto.

Tipos de entidade mista por contêiner
Você pode querer misturar diferentes tipos de entidade de documento (entidades) no mesmo contêiner, o que é útil para recuperar dados de forma eficiente de ambas as entidades usando uma única consulta. Por exemplo, você pode colocar o perfil do cliente e os dados do pedido de venda no mesmo contêiner e partição usando customerId. Nesta situação, normalmente é adicionado um campo aos documentos que identifica o tipo de entidade de cada um para diferenciá-los no momento da consulta.

Por sua vez, essa abordagem de modelagem é potencialmente útil para consultas do Cosmos DB de armazenamento transacional. Todos os documentos em um único contêiner são mapeados para um único repositório analítico, levando a repositórios de colunas que são gerados esparsamente com diferentes tipos de dados que precisam ser mais independentes ao executar uma consulta analítica.

Recomendação: Assim como em muitas decisões de design, há uma compensação entre a eficiência de consultar o armazenamento transacional e a facilidade de consultar o armazenamento analítico. Avalie cuidadosamente a utilidade de armazenar uma combinação de diferentes tipos de entidade de documento no mesmo contêiner como cargas de trabalho transacionais. Se você optar por fazer isso, será solicitado a filtrar a propriedade do tipo de entidade de propriedade que você selecionou.

Inserindo entidades de matriz
Ao otimizar modelos de dados transacionais, optamos por inserir entidades em uma matriz de um documento, especialmente para cargas de trabalho de leitura intensa em que:

Existem relacionamentos contidos entre entidades.
Existem relacionamentos um-para-algum entre entidades.
Há dados incorporados que mudam com pouca frequência.
Existem dados incorporados que não crescerão sem limites.
Há dados incorporados que são frequentemente consultados juntos.
Devido ao fato de haver entre um e muitos relacionamentos entre as entidades incorporadas que são representadas em um único documento, elas são mapeadas para uma única coluna de uma única linha no repositório analítico. A matriz inteira de entidades incorporadas residirá em um único valor de coluna e deve ser convertida de sua representação JSON no momento da consulta para recuperar os valores da entidade incorporada, independentemente de qual dos dois modos de representação de esquema é usado.

Recomendação: Um equilíbrio precisa ser alcançado novamente entre a utilidade da injeção de entidade de aplicativo transacional e a complexidade adicional de escrever consultas nos documentos JSON incorporados do aplicativo.

Particionamento de dados
Todos os dados em um contêiner do Azure Cosmos DB são particionados com base na chave de partição e aplicados ao repositório transacional e ao repositório analítico. Os limites de paralelização de carga de trabalho são baseados nessa chave de partição.

A disciplina associada no momento em que os dados aparecem no repositório analítico para uma consulta só é garantida em uma partição. Por exemplo, quando os documentos (1) (2) (3) são inseridos no repositório transacional em uma única partição, é garantido que eles serão encontrados no repositório analítico na ordem em que foram inseridos.

Próxima Unidade: Desenhar Cenários de Gravação Preguiçosa

Cenários de escrita preguiçosa de design
Preenchidas
100 EXP
2 minutos
O repositório analítico do Azure Cosmos DB é somente leitura do ponto de vista das cargas de trabalho analíticas; no entanto, há cenários em que os clientes que usam o repositório transacional do Cosmos DB devem recuperar os resultados de consultas analíticas.

Para esses requisitos, os resultados das consultas analíticas executadas em um repositório analítico do Cosmos DB podem ser gravados de volta no repositório transacional do Cosmos DB usando o Azure Synapse Apache Spark.

Próxima Unidade: Verificação de Conhecimento

Resumo
Preenchidas
100 EXP
3 minutos
Você viu como o Azure Synapse Analytics Link para Cosmos DB resolve os desafios de disponibilizar dados operacionais para consultas analíticas quase em tempo real por meio de processamento analítico e transacional híbrido.

Agora que você concluiu este módulo, você aprendeu a:

Entenda os padrões de processamento analítico e transacional híbridos
Descrever as cargas de trabalho de análise compatíveis
Explicar os cenários HTAP no contexto do Synapse Link
Descreva o armazenamento analítico
Entendendo esquemas bem definidos
Esquemas de design que suportam o tipo de análise
Cenários de escrita preguiçosa de design

Ingeniería de datos con Azure Databricks

Introdução
Preenchidas
100 EXP
4 minutos
O Azure Databricks é uma versão totalmente gerenciada do mecanismo de análise e processamento de dados Apache Spark de software livre . O Azure Databricks é uma plataforma de aprendizado de máquina e big data segura, de nível empresarial e baseada em nuvem.

O Databricks fornece um ambiente de ambiente de trabalho como serviço Apache Spark orientado a notebook, facilitando o gerenciamento de clusters e o exame interativo de dados.

Objetivos de aprendizado
Neste módulo, você aprenderá a:

Descrever a plataforma Azure Databricks
Crie seu próprio espaço de trabalho do Azure Databricks
Crie um notebook dentro da pasta principal do Databricks
Entenda os fundamentos dos notebooks Apache Spark
Criar ou conectar-se a um cluster Spark
Identifique os tipos de tarefas mais apropriados para o mecanismo de análise unificada do Apache Spark

Azure Databricks explicado
Preenchidas
100 EXP
10 minutos
O Azure Databricks é uma plataforma de big data e aprendizado de máquina totalmente gerenciada e baseada em nuvem que permite que os desenvolvedores acelerem a IA e a inovação simplificando o processo de criação de aplicativos de dados de produção de nível empresarial. Criado como uma iniciativa conjunta da equipe que iniciou o Apache Spark e a Microsoft, o Azure Databricks fornece às equipes de ciência de dados e engenharia uma plataforma única para processamento de big data e aprendizado de máquina.

Ao combinar o poder do Databricks, uma plataforma Apache Spark gerenciada de ponta a ponta e otimizada para nuvem, com a segurança e a escala corporativa da plataforma Azure da Microsoft, o Azure Databricks facilita a execução de cargas de trabalho do Spark em grande escala.

ambiente otimizado
Para resolver problemas encontrados em outras plataformas de big data, o Azure Databricks foi otimizado desde o início com foco no desempenho e na relação custo-benefício na nuvem. O tempo de execução do Databricks adiciona vários recursos importantes às cargas de trabalho do Apache Spark que podem aumentar o desempenho e reduzir os custos em um fator de 10 a 100 ao executar no Azure, incluindo:

Conectores de alta velocidade para serviços de armazenamento do Azure, como Azure Blob Store e Azure Data Lake
Dimensionamento automático e encerramento automático de clusters Spark para minimizar custos
cache
indexação
Otimização avançada de consultas
Ao fornecer um ambiente simplificado que é fácil de provisionar e configurar, o Azure Databricks oferece aos desenvolvedores uma plataforma econômica e eficiente que permite que eles gastem mais tempo criando aplicativos e menos tempo se concentrando no gerenciamento de cluster e infraestrutura.

Quem é a Databricks?
Os criadores do Apache Spark, Delta Lake e MLflow fundaram a Databricks.

Mais de 2.000 empresas globais usam a plataforma Databricks no ciclo de vida de aprendizado de máquina e big data.

Databricks Vision – Acelere a inovação unificando a ciência de dados, a engenharia de dados e a empresa.
Em essência, é o motor Spark.
A API DataFrames fornece uma abstração sobre Resilient Distributed Datasets (RDD), enquanto melhora o desempenho de 5 a 20 vezes em relação ao RDD tradicional com seu otimizador Catalyst.
O Spark ML fornece algoritmos de aprendizado de máquina altamente otimizados e de alta qualidade para processamento de big data.
A Graph Processing API nos fornece uma API facilmente acessível para modelar relacionamentos de pares entre pessoas, objetos ou nós em uma rede.
As APIs de streaming nos dão tolerância a falhas de ponta a ponta, com semântica única e a possibilidade de latência abaixo de milissegundos.
E tudo funciona perfeitamente em conjunto.

Azure Databricks
Solução Databricks - Plataforma de Big Data Analytics

O que o Databricks oferece que o Spark de código aberto não oferece?
Databricks Workspace: colaboração e ciência de dados interativos
Fluxos de trabalho do Databricks: trabalhos de produção e automação de fluxo de trabalho
Ambiente de execução do Databricks
Databricks I/O (DBIO): camada de acesso a dados otimizada
Databricks Serverless - Plataforma de autoajuste totalmente gerenciada
Databricks Enterprise Security (DBES): segurança e conformidade de ponta a ponta
O que é Apache Spark?
O Spark é um mecanismo de processamento unificado que pode analisar big data com SQL, aprendizado de máquina, processamento de gráficos ou análise de fluxo em tempo real:

Como um mecanismo de computação, o Azure Databricks está no centro da plataforma de software baseada no Azure, fornecendo integração nativa com o Azure Active Directory (Azure AD) e outros serviços do Azure.

Scala, Python, Java, R e SQL
Além de poder ser executado em vários ambientes, o Apache Spark torna a plataforma ainda mais acessível ao oferecer suporte a vários idiomas:
Scala: linguagem principal do Apache Spark
Python: mais conhecido como PySpark
R: SparkR (R em Spark)
Java
SQL: mais próximo da compatibilidade com ANSI SQL 2003
Agora todas as 99 consultas TPC-DS são executadas
Novo analisador compatível com padrões (com boas mensagens de erro)
Subconsultas (correlacionadas e não correlacionadas)
Estatísticas agregadas aproximadas
Com a API DataFrames, as diferenças de desempenho entre linguagens são quase inexistentes (especialmente para Scala, Java e Python).
Próxima unidade: Criar um espaço de trabalho e cluster do Azure Databricks


Criar um espaço de trabalho e cluster do Azure Databricks
Preenchidas
100 EXP
10 minutos
Ao falar sobre o espaço de trabalho do Azure Databricks, estamos falando de duas coisas diferentes. A primeira referência é o ambiente lógico do Azure Databricks onde os clusters são criados, os dados são armazenados (usando DBFS) e os recursos do servidor são hospedados. A segunda referência é a mais usada no contexto do Azure Databricks. Esta é a pasta raiz especial para todos os recursos do Databricks em sua organização, incluindo notebooks, bibliotecas e painéis, conforme mostrado abaixo:

Implantar um espaço de trabalho do Azure Databricks
Abra o Portal do Azure.
Clique em Criar um recurso no canto superior esquerdo.
Procure por "Databricks"
Selecione Azure Databricks .
Na página Azure Databricks, selecione Criar .
Forneça os valores necessários para criar o espaço de trabalho Azure Databricks:
Assinatura - escolha a assinatura do Azure na qual implantar o workspace.
Grupo de recursos - use Criar novo e digite um nome para o novo grupo de recursos.
Local - Selecione um local próximo para a implantação. Para obter a lista de regiões com suporte do Azure Databricks, confira Produtos disponíveis por região .
Nome do espaço de trabalho – forneça um nome exclusivo para o espaço de trabalho.
Plano tarifário : Avaliação (Premium - DBU 14 dias grátis) . Você deve selecionar esta opção ao criar o espaço de trabalho, caso contrário, você será cobrado. A área de trabalho será suspensa automaticamente após 14 dias. Quando a avaliação gratuita terminar, você poderá atualizar seu workspace para Premium , mas será cobrado pelo uso.
Selecione Revisar + Criar .
Selecione Criar .
Demora alguns minutos para criar o espaço de trabalho. Durante a criação do espaço de trabalho, o ícone Enviando Implantação para Azure Databricks aparece no lado direito do portal. Talvez seja necessário rolar para a direita do painel para ver o ícone. Há também uma barra de progresso exibida na parte superior da tela. Você pode verificar o progresso em qualquer um.

O que é um aglomerado?
Os notebooks são apoiados por clusters ou computadores em rede que colaboram para processar os dados. O primeiro passo é criar um cluster.

Criar um cluster
Quando terminar de criar seu espaço de trabalho Azure Databricks, selecione o link para acessar o recurso.

Selecione Iniciar área de trabalho para abrir a área de trabalho do Databricks em uma nova guia.

No menu esquerdo da área de trabalho do Databricks, selecione Processar .

Selecione Criar cluster para adicionar um novo cluster.

Digite um nome para o cluster. Use seu nome ou iniciais para diferenciar facilmente seu cluster dos de seus colegas de trabalho.

Selecione o modo de cluster : nó único .

Selecione a versão do tempo de execução do Databricks : Runtime: 7.3 LTS (Scala 2.12, Spark 3.0.1) .

Em Opções do piloto automático , deixe a caixa de seleção marcada e, na caixa de texto, digite.

Selecione o Tipo de Nó : Standard_DS3_v2 .

Selecione Criar cluster .

Próxima unidade: Noções básicas sobre notebooks Azure Databricks

Descrição dos blocos de anotações do Azure Databricks
Preenchidas
100 EXP
10 minutos
Com seu espaço de trabalho Databricks criado, é hora de criar seu primeiro notebook. Para executar o notebook, você precisará anexar o cluster criado na unidade anterior.

O que é um notebook Apache Spark?
Um notebook é uma coleção de células. Essas células são usadas para executar código, renderizar texto formatado ou exibir exibições gráficas.

Criar um caderno
No portal do Azure, clique no menu Todos os recursos na navegação à esquerda e selecione o espaço de trabalho Databricks criado na última unidade.

Selecione Iniciar área de trabalho para abrir a área de trabalho do Databricks em uma nova guia.

No menu à esquerda do espaço de trabalho do Databricks, selecione Home .

Clique com o botão direito do mouse na pasta pai.

Selecione Criar .

Selecione Caderno .


Nomeie o notebook como Primeiro Notebook.

Defina o idioma para Python .

Selecione o cluster ao qual este notebook deve ser associado.

 Observação

Essa opção só é exibida se houver algum cluster em execução no momento. Você ainda pode criar o notebook e anexá-lo a um cluster posteriormente.

Selecione Criar .

Agora que você criou o notebook, ele será usado para executar o código.

Associação e desassociação do notebook
Para usar o notebook para executar o código, você deve associá-lo a um cluster. Você também pode desanexar o notebook de um cluster e anexá-lo a outro com base nos requisitos da sua organização.

Se o notebook estiver associado a um cluster, você poderá:

Desanexar o notebook do cluster
Reinicie o cluster
Anexar a outro cluster
Abra a interface do Spark
Visualize os arquivos de log do driver.
Próxima Unidade: Exercício: Trabalhando com Notebooks


Exercício: Trabalhar com Notebooks
Preenchidas
100 EXP
10 minutos
Você pode usar notebooks Apache Spark para:

Ler e processar grandes arquivos e conjuntos de dados
Consulte, navegue e visualize conjuntos de dados
Junte-se a diferentes conjuntos de dados encontrados no Data Lake
Treinar e avaliar modelos de aprendizado de máquina
Processar fluxos de dados ao vivo
Realizar análise de gráficos de grandes conjuntos de dados e redes sociais
Para obter mais informações sobre o uso de notebooks, clone o arquivo de laboratório onde os notebooks de exemplo são fornecidos. Esses notebooks ajudarão você a entender como usá-los para tarefas diárias.

Clonando o arquivo Databricks
No portal do Azure, vá para o Azure Databricks Deployed Workspace e selecione Launch Workspace .

No painel esquerdo, selecione Workspace > Users e o nome de usuário (a entrada com o ícone de casa).

No painel exibido, selecione a seta ao lado do nome e selecione Importar .

Clique em Importar .
Selecione a pasta 01-Introduction-to-Azure-Databricks que aparece.
Para concluir este laboratório, use o conjunto de notebooks nesta pasta.
Conclusão do seguinte caderno
01-The-Databricks-Environment - Este notebook mostra o básico de um notebook Databricks.
Próxima Unidade: Verificação de Conhecimento

Resumo
Preenchidas
100 EXP
4 minutos
Neste módulo, você aprendeu o básico sobre como usar o espaço de trabalho Databricks e os notebooks Apache Spark. Os notebooks permitem que você trabalhe interativamente com diferentes tipos de dados. Você pode usar notebooks para processar grandes arquivos de dados, consultar, ler e gravar dados de diferentes fontes, treinar modelos de aprendizado de máquina e processar fluxos de dados ao vivo.

Agora que você terminou este módulo, você deve saber como fazer o seguinte:

Criar um cluster
Anexar um notebook a um cluster
Executar código Python, Scala, SQL ou R em uma célula de notebook
Desanexar um notebook de um cluster
Adapte o Azure Databricks ao ecossistema do Azure
Trabalhar com DBFS para apresentar o armazenamento Blob como um sistema de arquivos
Limpeza
Se você planeja concluir outros módulos do Azure Databricks, não exclua a instância do Azure Databricks ainda. Você pode usar o mesmo ambiente para o restante dos módulos.

Excluir a instância do Azure Databricks
Vá para o portal do Azure.
Navegue até o grupo de recursos que contém a instância do Azure Databricks.
Selecione Excluir grupo de recursos .
Digite o nome do grupo de recursos na caixa de texto de confirmação.
Selecione Excluir .

Processamento de dados em larga escala com Azure Data Lake Storage Gen2
1h 5min remanescente
caminho de aprendizagem
0 de 3 módulos concluídos
Principiante
engenheiro de dados
Azure
Portal do Azure
Armazenar
Máquinas virtuais
Neste caminho de aprendizado, você verá como o Azure Data Lake Storage ajuda a tornar as soluções de análise de big data mais eficientes e como é fácil configurar. Você também explorará como ele se encaixa em arquiteturas comuns, bem como os diferentes métodos de carregamento de dados no data warehouse. Por fim, você examinará a ampla variedade de recursos de segurança que garantirão a segurança dos dados.

Descrição do Azure Data Lake Storage Gen2
Preenchidas
100 EXP
4 minutos
O termo "data lake" define um repositório de dados que é armazenado em seu formato natural, normalmente como blobs ou arquivos. O Azure Data Lake Storage é uma solução de data lake completa, escalável e econômica para análise de big data criada no Azure.

O Azure Data Lake Storage combina um sistema de arquivos com uma plataforma de armazenamento para ajudar a identificar rapidamente insights nos dados. O Azure Data Lake Storage Gen2 se baseia nos recursos do armazenamento de BLOBs do Azure para ser otimizado especificamente para cargas de trabalho de análise. Essa integração facilita o desempenho analítico, os recursos de gerenciamento do ciclo de vida de dados e a classificação em camadas do armazenamento de Blobs e os recursos de alta disponibilidade, segurança e durabilidade do Armazenamento do Azure.

A variedade e o volume de dados gerados e analisados hoje estão aumentando. As empresas têm uma variedade de fontes de dados, de sites a sistemas de ponto de venda (POS) e, mais recentemente, de redes sociais a dispositivos de Internet das Coisas (IoT). Cada fonte fornece um aspecto essencial dos dados que precisam ser extraídos, analisados e potencialmente manipulados.

Vantagem
O Data Lake Storage Gen2 foi projetado para lidar com essa variedade e volume de dados em escala de exabytes, enquanto gerencia centenas de gigabytes de saída. Isso permite que o Data Lake Storage Gen2 seja usado como base para soluções em tempo real e em lote. Aqui está uma lista de benefícios adicionais que o Data Lake Storage Gen2 oferece:

Acesso compatível com Hadoop
Uma vantagem do Data Lake Storage Gen2 é que ele permite que você trate os dados como se estivessem armazenados em um Hadoop Distributed File System. Com esse recurso, os dados podem ser armazenados em um local e acessados por meio de tecnologias de computação, incluindo Azure Databricks, Azure HDInsight e Azure Synapse Analytics sem mover dados entre ambientes.

Segurança
O Data Lake Storage Gen2 oferece suporte a listas de controle de acesso (ACLs) e permissões POSIX (Portable Operating System Interface). Você pode definir permissões para dados armazenados em um data lake em um diretório ou nível de arquivo. Essa segurança pode ser configurada usando tecnologias como Hive e Spark ou utilitários como Azure Storage Explorer. Todos os dados armazenados são criptografados em repouso usando chaves gerenciadas pelo cliente ou gerenciadas pela Microsoft.

Desempenho
O Azure Data Lake Storage organiza os dados armazenados em uma hierarquia de diretórios e subdiretórios como um sistema de arquivos para facilitar a navegação. Como resultado, o processamento de dados requer menos recursos de computação, reduzindo tempo e custo.

redundância de dados
O Data Lake Storage Gen2 aproveita os modelos de replicação do Azure Blob que fornecem redundância de dados em um único data center com armazenamento localmente redundante (LRS) ou em uma região secundária usando a opção de armazenamento com redundância geográfica (GRS). Esse recurso garante que os dados estejam sempre disponíveis e protegidos em caso de catástrofe.

Próxima unidade: Exercício: Criar uma conta de armazenamento do Azure usando o portal
Continuar

Exercício: Criar uma conta de armazenamento do Azure usando o portal
Preenchidas
100 EXP
5 minutos
O Azure Data Lake Storage Gen2 é fácil de configurar. Requer uma conta de armazenamento do Azure do tipo StorageV2 (uso geral v2) com namespace hierárquico habilitado. Vejamos um exemplo de como configurar uma conta do Data Lake Storage no portal do Azure.

Entrar no Portal do Azure

Selecione Criar um recurso e, na caixa de texto "Pesquisar no Marketplace", digite Conta de armazenamento e clique em Conta de armazenamento .

Na tela Conta de armazenamento , clique em Criar .

Em seguida, na janela Criar conta de armazenamento , na guia Básico , na seção Detalhes do projeto, certifique-se de que a assinatura e o grupo de recursos apropriados estejam selecionados. Na seção Instância, defina um nome para a conta de armazenamento . Defina a região para Central US. Na lista de botões de opção Performance , selecione Standard e defina Redundancy to Locally Redundant Storage (LRS) .

Selecione a guia Avançado , na seção Data Lake Storage Gen2, clique na caixa de seleção ao lado de Habilitar namespace hierárquico , conforme mostrado abaixo.

Selecione a guia Revisar + Criar e clique em Criar .

Agora, essa nova conta de armazenamento do Azure está configurada para hospedar dados de uma instância do Azure Data Lake. Depois que a conta for implantada, você encontrará as opções relacionadas ao Azure Data Lake na página Visão geral.

Selecione a guia Revisar + Criar e clique em Criar .

Agora, essa nova conta de armazenamento do Azure está configurada para hospedar dados de uma instância do Azure Data Lake. Depois que a conta for implantada, você encontrará as opções relacionadas ao Azure Data Lake na página Visão geral.

Comparação entre o Azure Data Lake Storage e o Armazenamento de Blobs do Azure
Preenchidas
100 EXP
5 minutos
Grandes quantidades de dados não estruturados ("objetos") podem ser armazenadas no armazenamento de Blobs do Azure em uma única hierarquia, também conhecida como namespace simples. Esses dados podem ser acessados via HTTP ou HTTPs. O Azure Data Lake Storage Gen2 é baseado no armazenamento de Blobs e otimiza a E/S de dados de alto volume usando os namespaces hierárquicos que foram habilitados no exercício anterior.

Os namespaces hierárquicos organizam os dados do blob em diretórios e armazenam metadados sobre cada diretório e os arquivos que ele contém. Essa estrutura permite que operações como exclusões e renomeações de diretórios sejam executadas em uma única operação atômica. Em contraste, os namespaces simples requerem várias operações proporcionais ao número de objetos na estrutura. Os namespaces hierárquicos mantêm os dados organizados, resultando em melhor armazenamento e desempenho de recuperação para um caso de uso de análise e reduz o custo da análise.

Armazenamento de Blobs do Azure vs. Armazenamento do Azure Data Lake
Se você quiser armazenar dados sem analisá-los , defina a opção de namespace hierárquico como desabilitada para configurar a conta de armazenamento como uma conta de armazenamento de BLOBs do Azure. Você também pode usar o armazenamento Blob para arquivar dados usados com pouca frequência ou para armazenar recursos do site, como imagens e mídia.

Se você for realizar análise de dados, configure sua conta de armazenamento como uma conta do Azure Data Lake Storage Gen2. Para fazer isso, defina a opção Hierarchical Namespace como Enabled . Como o Azure Data Lake Storage Gen2 é criado com base na plataforma de armazenamento do Azure, os aplicativos podem usar as APIs de Blob ou as APIs do sistema de arquivos do Azure Data Lake Storage Gen2 para acessar dados.

Próxima unidade: Entendendo os estágios do processamento de big data usando o Azure Data Lake Storage

Descrição dos estágios de processamento de big data usando o Azure Data Lake Storage
Preenchidas
100 EXP
3 minutos
O Azure Data Lake Storage Gen2 desempenha um papel crítico em uma ampla variedade de arquiteturas de big data. Essas arquiteturas podem levar à criação de:

Um armazenamento de dados moderno.
Análise avançada com big data.
Uma solução de análise em tempo real.
Existem quatro fases de processamento de soluções de big data que são comuns a todas as arquiteturas:

Ingestão – A fase de ingestão identifica a tecnologia e os processos usados ​​para adquirir os dados de origem. Esses dados podem vir de arquivos, logs e outros tipos de dados não estruturados que devem ser colocados no Data Lake Storage. A tecnologia utilizada irá variar dependendo da frequência com que os dados são transferidos. Por exemplo, para mover lotes de dados, o Azure Data Factory pode ser a tecnologia mais apropriada. Para ingestão de dados em tempo real, Apache Kafka para HDInsight ou Stream Analytics podem ser tecnologias apropriadas.
Armazenamento – A fase de armazenamento identifica onde os dados ingeridos devem ser colocados. Nesse caso, o Azure Data Lake Storage Gen2 é usado.
Preparar e treinar – A fase Preparar e treinar identifica as tecnologias usadas para realizar a preparação de dados e treinamento de modelo e pontuação para soluções de ciência de dados. As tecnologias comuns usadas nesta fase são Azure Databricks, Azure HDInsight ou Azure Machine Learning Services.
Modelagem e entrega – Por fim, a fase de modelagem e entrega envolve as tecnologias que vão apresentar os dados aos usuários. Eles podem incluir ferramentas de visualização como Power BI ou outros armazenamentos de dados como Azure Synapse Analytics, Azure Cosmos DB, Banco de Dados SQL do Azure ou Azure Analysis Services. Muitas vezes, uma combinação dessas tecnologias será usada, dependendo dos requisitos de negócios.
Próxima unidade: Examinando os usos do Azure Data Lake Storage Gen2

Examinando os usos do Azure Data Lake Storage Gen2
Preenchidas
100 EXP
5 minutos
Serão examinados três casos de uso para empregar uma instância do Azure Data Lake Storage.

Construindo um data warehouse moderno
Imagine que você seja um consultor de engenharia de dados da Contoso. No passado, a empresa construiu uma solução de business intelligence local que usava o Microsoft SQL Server Database Engine, SQL Server Integration Services, SQL Server Analysis Services e SQL Server Reporting Services para fornecer relatórios históricos. Eles tentaram usar o componente de mineração de dados do Analysis Services para criar uma solução de análise preditiva para prever o comportamento de compra do cliente. Embora esse método tenha funcionado bem com um baixo volume de dados, ele não pôde ser dimensionado após a coleta de mais de um gigabyte de dados. Além disso, os dados JSON gerados por um aplicativo de terceiros não podiam ser processados quando um cliente usava o módulo de feedback do aplicativo de PDV.

A Contoso recorreu a você para ajudá-los a criar uma arquitetura que possa ser dimensionada com as necessidades de dados para criar um modelo preditivo e manipular os dados JSON para que possam ser integrados à solução de business intelligence. Você decide recomendar a seguinte arquitetura:


A arquitetura usa o Azure Data Lake Storage como o centro da solução para armazenamento de dados moderno. O Integration Services é substituído pelo Azure Data Factory para ingerir dados de um aplicativo corporativo em um Data Lake. Esta é a fonte do modelo de previsão incorporado ao Azure Databricks. O PolyBase é usado para transferir dados históricos em um formato relacional de big data que é mantido no Azure Synapse Analytics, onde os resultados do modelo treinado do Databricks também são armazenados. O Azure Analysis Services fornece capacidade de armazenamento em cache para o SQL Data Warehouse para atender a muitos usuários e apresentar os dados por meio de relatórios do Power BI.

Análise avançada para Big Data
Neste segundo caso de uso, o Azure Data Lake Storage desempenha um papel importante no fornecimento de um armazenamento de dados em grande escala. A AdventureWorks precisa de suas habilidades; é um vendedor global de bicicletas e componentes de ciclismo através de uma cadeia de distribuidores e na Internet. Quando os clientes navegam no catálogo de produtos em seus sites e adicionam os itens ao carrinho de compras, um mecanismo de recomendação integrado ao Azure Databricks recomenda outros produtos. Eles precisam garantir que os resultados do mecanismo de recomendação possam ser dimensionados globalmente. As recomendações são baseadas em arquivos de log da Web armazenados em servidores Web e transferidos de hora em hora para o modelo Azure Databricks. O tempo de resposta de uma recomendação deve ser inferior a 1 ms.

Nesta solução, o Azure Data Factory transfere terabytes de logs da Web de um servidor Web para o Azure Data Lake a cada hora. Esses dados são fornecidos como recursos para o modelo preditivo do Azure Databricks, que é treinado e pontuado. Os resultados são distribuídos globalmente usando o Azure Cosmos DB, que é usado pelo aplicativo em tempo real (o site AdventureWorks) para fornecer recomendações aos clientes à medida que eles adicionam produtos ao carrinho de compras online.

Para completar essa arquitetura, o PolyBase é usado no Data Lake para transferir dados descritivos para a instância do SQL Data Warehouse para geração de relatórios. O Azure Analysis Services fornece funcionalidade de cache para SQL Data Warehouse para atender a muitos usuários e exibir os dados por meio de relatórios do Power BI.

Soluções de análise em tempo real
Para executar soluções de análise em tempo real, a fase de ingestão da arquitetura é alterada para soluções de processamento de big data. Nesta arquitetura, observe a introdução do Apache Kafka para Azure HDInsight para ingerir dados de streaming de um dispositivo da Internet das Coisas (IoT), embora isso possa ser substituído pelo Hub IoT do Azure e Azure Stream Analytics. A chave é que os dados sejam armazenados no Data Lake Storage Gen2 para fornecer serviço a outras partes da solução.

Neste caso de uso, você é um engenheiro de dados que trabalha para a Trey Research, uma organização que colabora com uma empresa de transporte para monitorar a frota de veículos pesados de mercadorias que circulam na Europa. Cada um desses veículos é equipado com hardware de sensor que relata continuamente dados métricos sobre os níveis de temperatura, velocidade, óleo e fluido de freio do veículo. Quando o motor é desligado, o sensor gera um arquivo com informações resumidas sobre a viagem, incluindo quilometragem e altitude. Uma viagem é o período entre ligar e desligar o motor do veículo pesado.

Tanto os dados em tempo real quanto os dados em lote são processados em um modelo de aprendizado de máquina para prever um cronograma de manutenção para cada um dos veículos. Esses dados estão disponíveis para o aplicativo correspondente que as empresas de garagem podem usar se um veículo pesado quebrar em qualquer lugar da Europa. Além disso, relatórios históricos sobre o veículo devem ser apresentados visualmente aos usuários. Como resultado, a seguinte arquitetura é proposta:

Nessa arquitetura, há dois fluxos de ingestão. O Azure Data Factory ingere arquivos de resumo que são gerados quando o mecanismo do HGV está desligado. O Apache Kafka fornece o mecanismo de ingestão em tempo real para dados de telemetria. Os dois fluxos de dados são mantidos no Azure Data Lake Storage para uso futuro, mas também são enviados para outras tecnologias para atender às necessidades de negócios. Os dados de streaming e em lote são fornecidos ao modelo preditivo do Azure Databricks e os resultados são publicados no Azure Cosmos DB para uso por outras oficinas de automóveis. O PolyBase ingere dados do Azure Data Lake Storage para o SQL Data Warehouse, onde o Azure Analysis Services cria os relatórios HGV usando o Power BI.

Próxima Unidade: Resumo


Implemente uma solução de streaming de dados com o Azure Stream Analytics
2h18min.
caminho de aprendizagem
0 de 3 módulos concluídos
Principiante
engenheiro de dados
Azure
Portal do Azure
Armazenar
Análise de fluxo
Saiba mais sobre os conceitos de streaming de dados e processamento de eventos e como eles se aplicam ao Azure Stream Analytics. Em seguida, você configurará um trabalho do Stream Analytics para transmitir dados e aprenderá a gerenciar e monitorar um trabalho em execução.

Introdução
Preenchidas
100 EXP
5 minutos
Hoje, aplicativos conectados, dispositivos e sensores da Internet das Coisas (IoT) e uma variedade de outras fontes geram grandes quantidades de dados em tempo real. A proliferação dessas fontes de dados de streaming tornou a capacidade de consumir e tomar decisões informadas a partir desses dados em tempo quase real uma necessidade operacional para muitas organizações.

Para citar alguns exemplos, as lojas online analisam dados de cliques em tempo real para fornecer recomendações de produtos aos consumidores enquanto navegam pelo site. As instalações de fabricação usam dados de telemetria de sensores IoT para monitorar remotamente ativos de alto valor. E as transações com cartão de crédito dos sistemas de ponto de venda são examinadas em tempo real para detectar e prevenir atividades potencialmente fraudulentas.

O Azure Stream Analytics integra perfeitamente sua arquitetura de aplicativo em tempo real com um mecanismo de análise de fluxo para transformar dados de fluxo em insights acionáveis. O uso do Azure Stream Analytics permite uma análise de dados poderosa e em tempo real, independentemente do volume.

Neste módulo, você aprenderá como funciona a análise em tempo real de dados de fluxo, em princípio. Você também aprenderá como usar o Azure Stream Analytics para integrar dados de streaming em seus fluxos de trabalho de análise em tempo real.

Objetivos de aprendizado
Neste módulo, você aprenderá o seguinte:

Definição de fluxos de dados
Descrição do processamento de eventos
Saiba mais sobre o processamento de eventos com o Azure Stream Analytics
Próxima Unidade: Definindo Fluxos de Dados

Definição de fluxos de dados
Preenchidas
100 EXP
5 minutos
No contexto da análise, os fluxos de dados são dados sobre a ocorrência de atividades específicas que são emitidas por aplicativos, dispositivos IoT, sensores ou outras fontes conhecidas como produtores de dados. Esses fluxos de dados gerados perpetuamente geralmente contêm informações temporárias e adicionais sobre eventos. A proliferação de dispositivos e aplicativos conectados levou a um crescimento exponencial no número de fontes de dados de fluxo nos últimos anos.

Os fluxos de dados são usados principalmente para entender melhor as mudanças ao longo do tempo. Por exemplo, uma organização pode realizar uma análise de sentimento em tweets para ver se uma campanha publicitária gera comentários mais positivos sobre a empresa ou seus produtos.

Análise de fluxos de dados
Hoje, os fluxos de dados são onipresentes. A análise de um fluxo de dados geralmente é feita para medir como o estado de um evento muda ao longo do tempo ou para capturar informações sobre uma área de interesse. A intenção é:

Analise os dados continuamente para detectar problemas e compreendê-los ou respondê-los.
Compreender o comportamento de um componente ou sistema sob diferentes condições para gerar melhorias nesse componente ou sistema.
Acione ações ou alertas específicos quando determinados limites forem atingidos.
Ao analisar fluxos de dados e extrair insights acionáveis, as empresas podem aproveitar o conhecimento latente para melhorar a eficiência, impulsionar a inovação e responder a irregularidades. Exemplos de casos de uso que analisam fluxos de dados incluem:

tendências do mercado de ações
Empresas de serviços que monitoram dados de tubulações de água, transmissões elétricas e sistemas de distribuição
Dados de monitoramento de condição de componentes mecânicos nas indústrias automobilística e automotiva
Dados de monitoramento de equipamentos industriais e de fabricação
Dados de sensores em transporte, como gerenciamento de tráfego e pedágios em rodovias
Monitoramento de dados da saúde de um paciente no setor de saúde
Dados de satélite na indústria espacial
Detecção de fraudes nos setores bancário e financeiro
Análise de sentimento em postagens de mídia social
Abordagens para processamento de fluxo de dados
Existem duas abordagens para o processamento de fluxo de dados: ao vivo e sob demanda.

A abordagem mais comumente adotada para processar fluxos de dados é analisar continuamente novos dados à medida que chegam de um produtor de eventos, como os Hubs de Eventos do Azure. Essa abordagem "ao vivo" requer mais poder de processamento para realizar cálculos, mas oferece a capacidade de obter informações quase em tempo real. Usando um serviço como o Azure Stream Analytics, você pode executar cálculos e agregações em dados de entrada usando análise temporal. Os resultados dessas consultas podem ser enviados para um painel do Power BI para visualização e análise em tempo real.

O diagrama a seguir mostra uma solução de processamento de fluxo "ao vivo" de ponta a ponta que usa Hubs de Eventos para ingestão de dados de fluxo, Azure Stream Analytics para transformar os dados e Power BI para visualizar e analisar os dados.

A abordagem "sob demanda" para processar dados de fluxo envolve manter todos os dados de entrada em um armazenamento de dados, como Azure Data Lake Storage (ADLS) Gen2 . Esse método permite coletar dados de fluxo ao longo do tempo e armazená-los como dados estáticos. Você pode processar os dados estáticos em lotes quando for conveniente ou durante os períodos em que os custos de processamento forem menores.

O diagrama a seguir mostra uma solução de processamento de fluxo de dados sob demanda. Os Hubs de Eventos do Azure e os dados de fluxo do Hub IoT são gravados como blobs no Azure Data Lake Storage (ADLS) Gen2. Os dados estáticos são então processados ​​com o Azure Stream Analytics e enviados para um painel do Power BI para visualização e análise.

Próxima unidade: Descrição do processamento de eventos

Descrição do processamento de eventos
Preenchidas
100 EXP
5 minutos
O processamento de eventos refere-se ao consumo e análise de um fluxo de dados contínuo para extrair conhecimento latente e derivar informações acionáveis dos eventos que ocorrem dentro do fluxo. Os pipelines de processamento de eventos fornecem uma solução de ponta a ponta para ingestão, transformação e análise de fluxo de dados e são compostos de três componentes distintos:

Um produtor de eventos , que gera um fluxo de dados de eventos.
Um processador de eventos responsável por ingerir e transformar dados de eventos de fluxos.
Um consumidor de eventos que exibe ou consome dados de eventos e executa ações neles.

Produtor de eventos
Um produtor de eventos é um aplicativo, um dispositivo ou sensor conectado ou qualquer outro serviço que emite continuamente um fluxo de dados de eventos. Os eventos podem ser qualquer ação recorrente, como batimentos cardíacos, um carro passando por um pedágio ou um sensor do motor informando os valores de temperatura de um carro. Cada evento deve incluir informações temporais, como um carimbo de data/hora, indicando quando ocorreu.

Os Hubs de Eventos do Azure e o Hub IoT são frequentemente usados ​​como produtores de eventos no Azure. Esses serviços podem manipular eventos de entrada em grande escala e produzir um fluxo de eventos destinado a serviços de processamento de eventos downstream, como o Azure Stream Analytics.

processador de eventos
Um processador de eventos é um mecanismo projetado para consumir e transformar fluxos de dados de eventos. Os processadores de eventos exigem a capacidade de consultar facilmente as fatias de tempo. Realizar cálculos ou agregações ao longo do tempo, como contar o número de vezes que um evento ocorre durante um determinado período, é um caso de uso comum. Dependendo do espaço do problema, os processadores de eventos processam um evento de entrada por vez (por exemplo, um monitor de frequência cardíaca) ou vários simultaneamente (por exemplo, o Azure Stream Analytics processando dados dos sensores dos dispositivos).

O Azure Stream Analytics é a maneira mais rápida de colocar o processamento de eventos em funcionamento no Azure. Com o Stream Analytics, você pode ingerir dados de fluxo dos Hubs de Eventos do Azure ou do Hub IoT e executar consultas de análise em tempo real nos fluxos.

O Stream Analytics oferece suporte a vários tipos de funções de janela para realizar cálculos temporais em fluxos de dados, fornecendo uma maneira de agregar eventos em vários intervalos de tempo com base em definições de janela específicas. Ele também fornece a capacidade de usar os recursos do Azure Machine Learning para torná-lo uma ferramenta robusta para analisar fluxos de dados.

consumidor de eventos
Um consumidor de eventos é um aplicativo que usa a saída de um processador de eventos. Os consumidores de eventos podem ser usados ​​para exibir os dados ou executar uma ação específica com base nas informações, como gerar alertas quando os limites especificados são atingidos ou enviar dados para outro mecanismo de processamento de eventos.

Para visualizar e analisar dados, o Power BI é o consumidor de eventos recomendado. Ele fornece uma plataforma para criar visualizações vinculadas complexas e analisar dados agregados quase em tempo real. O Azure Stream Analytics pode enviar resultados diretamente para o Power BI, permitindo que os painéis sejam atualizados continuamente à medida que os fluxos de dados são processados.

Como criar pipelines de processamento de eventos
Os pipelines de processamento de eventos normalmente encadeiam vários serviços para criar um pipeline de análise quase em tempo real. O diagrama a seguir mostra um exemplo de pipeline de processamento de eventos com base em Hubs de Eventos, Azure Stream Analytics e Power BI.

Nesse pipeline, os Event Hubs ingerem dados de fluxos de sensores em uma fábrica conectada e, juntos, atuam como produtores de eventos. O processador de eventos é o Azure Stream Analytics, que recebe dados dos Hubs de Eventos e os transforma e agrega. O Power BI, o consumidor de eventos, recebe a saída do Stream Analytics e exibe visualizações avançadas usadas para analisar dados de eventos.

Próxima unidade: Processamento de eventos com Azure Stream Analytics

Processamento de eventos com Azure Stream Analytics
Preenchidas
100 EXP
5 minutos
O Azure Stream Analytics é um mecanismo de processamento de eventos de plataforma como serviço (PaaS). Torna possível transformar e analisar grandes volumes de dados de fluxos provenientes dos Hubs de Eventos do Azure e do Hub IoT e dados estáticos do Armazenamento do Azure. Com o Stream Analytics, você pode escrever consultas e agregações complexas baseadas em tempo em dados gerados por sensores, dispositivos ou aplicativos conectados. O Stream Analytics processa os dados em tempo real, graças aos quais são obtidas informações valiosas para a tomada de decisões em tempo real. Um pipeline de processamento de eventos típico baseado em Stream Analytics consiste nos quatro componentes a seguir:

Produtor de Eventos – Este é qualquer aplicativo, sistema ou sensor que produz continuamente dados do evento de interesse. Alguns exemplos são sensores que rastreiam o fluxo de água por uma tubulação pública e um aplicativo como o Twitter que gera tweets com uma única hashtag.
Sistema de ingestão de eventos - recebe dados de um produtor de eventos e os transmite para um mecanismo de análise. Os Hubs de Eventos do Azure, o Hub IoT do Azure ou o Armazenamento de Blobs do Azure podem atuar como o sistema de ingestão.
Mecanismo de análise de fluxo – a plataforma de computação que processa, agrega e transforma fluxos de dados recebidos. O Azure Stream Analytics fornece o Stream Analytics Query Language (SAQL), um subconjunto do Transact-SQL adaptado para realizar cálculos em dados de streaming. O mecanismo suporta funções de janela que são essenciais para processamento de fluxo e são implementadas usando SAQL.
Event Consumer - Este é um destino para a saída do mecanismo de análise de fluxo. A saída pode ser armazenada em uma plataforma de armazenamento de dados, como Azure Data Lake Storage Gen2, Azure Cosmos DB, Banco de Dados SQL do Azure ou Armazenamento de Blobs do Azure. Ou você pode consumir a saída quase em tempo real usando os painéis do Power BI.

Aspectos operacionais
O Stream Analytics garante o processamento de eventos e pelo menos a entrega de um evento, para que os dados nunca sejam perdidos. Além disso, possui recursos de recuperação integrados, no caso de um erro ao entregar um evento. O Stream Analytics também fornece pontos de verificação integrados para manter o status do trabalho e produzir resultados repetíveis.

Como o Azure Stream Analytics é um serviço PaaS, ele é totalmente gerenciado e altamente confiável. Sua integração integrada com várias fontes e destinos, juntamente com um modelo de programação flexível, melhora a produtividade do desenvolvedor. Além disso, como o mecanismo Stream Analytics permite o processamento na memória, ele oferece desempenho superior. Todos esses fatores contribuem para um baixo custo total de propriedade (TCO) do Azure Stream Analytics.

Próxima Unidade: Teste de Conhecimento

Resumo
Preenchidas
100 EXP
5 minutos
O Azure Stream Analytics é um serviço de PaaS que se integra a aplicativos e à Internet das Coisas (IoT) para obter insights com dados de streaming ou dados estáticos contidos em um armazenamento de blobs. O processo de consumir fluxos de dados, analisá-los e derivar informações acionáveis deles é chamado de processamento de eventos. Requer um produtor de eventos, um processador de eventos e um consumidor de eventos. O Azure Stream Analytics fornece processamento de eventos para seu fluxo, que é totalmente gerenciado e extremamente confiável.

Neste módulo, você aprendeu como funciona a análise em tempo real de dados de fluxo, em princípio. Você também descobriu como pode usar o Azure Stream Analytics para integrar dados de streaming em seus fluxos de trabalho de análise em tempo real.

Objetivos de aprendizado
Neste módulo, você aprendeu a:

Entenda os fluxos de dados
Entenda o processamento de eventos
Compreender as informações sobre o processamento de eventos com o Azure Stream Analytics


Proteja sua conta de armazenamento do Azure
5 minutos restantes
Módulo
7 de 8 unidades concluídas
Principiante
Administrador
Engenheiro de dados
Engenheiro de segurança
Solução de arquitetura
Azure
Armazenar
Saiba como o Armazenamento do Azure fornece segurança em várias camadas para proteger seus dados. Descubra como usar chaves de acesso, proteger redes e usar a Proteção Avançada contra Ameaças para monitorar seu sistema de forma proativa.

Objetivos de aprendizado
Neste módulo você irá:

Explore os recursos de segurança de classe empresarial do Azure Data Lake.
Entenda as chaves da conta de armazenamento.
Entenda as assinaturas de acesso compartilhado.
Entenda a criptografia em nível de transporte com HTTPS.
Entenda a Proteção Avançada contra Ameaças.
Controle o acesso à rede.

Armazenar dados de aplicativos com o armazenamento de BLOBs do Azure
50 minutos
Módulo
8 Unidades
Principiante
Desenvolvedor
Azure
Armazenar
Crie um aplicativo que armazene arquivos de usuário com armazenamento de Blobs do Azure.

Objetivos de aprendizado
Neste módulo, você irá:

Organize seus dados com o Armazenamento de Blobs do Azure
Criar recursos de armazenamento para armazenar blobs
Armazenar e recuperar dados do Armazenamento de Blobs do Azure

Introdução ao armazenamento de blobs
Concluído
100 EXP
3 minutos
Os blobs oferecem armazenamento de objetos na nuvem e uma API que permite criar aplicativos para acessar os dados.

Suponha que você trabalhe em uma empresa de jogos de realidade aumentada. Seu jogo é executado em todas as plataformas móveis. Você deseja adicionar um novo recurso para permitir que os usuários gravem videoclipes de seus jogos e carreguem os clipes em seus servidores. Os usuários poderão assistir a clipes diretamente no jogo ou através do seu site. Você planeja registrar cada upload e visualização para uso em análises e rastreabilidade.

Você precisa de uma solução de armazenamento que possa lidar com milhares de uploads simultâneos, grandes quantidades de dados de vídeo e arquivos de log em constante crescimento. Você também precisa adicionar a funcionalidade de visualização a todos os seus aplicativos móveis e ao seu site, para que você deseje acesso à API de várias plataformas e idiomas.

Aqui, você verá como o Armazenamento de Blobs do Azure pode ser apropriado para este aplicativo.

Objetivos de aprendizado
Neste módulo, você irá:

Organize seus dados com o Armazenamento de Blobs do Azure
Criar recursos de armazenamento para armazenar blobs
Armazenar e recuperar dados do Armazenamento de Blobs do Azure

O que são bolhas?
Concluído
100 EXP
3 minutos
Blobs são arquivos para a nuvem . Os aplicativos funcionam com blobs da mesma maneira que trabalham com arquivos em um disco; leitura e escrita de dados. No entanto, ao contrário de um arquivo local, você pode acessar blobs de qualquer lugar com uma conexão com a Internet.

O Armazenamento de Blobs do Azure não é estruturado , o que significa que não há restrições sobre os tipos de dados que ele pode conter. Por exemplo, um blob pode conter um documento PDF, uma imagem JPG, um arquivo JSON, conteúdo de vídeo e muito mais. Os blobs não estão limitados a formatos de arquivo comuns; um blob pode conter gigabytes de dados binários transmitidos de um instrumento científico, uma mensagem criptografada para outro aplicativo ou dados em um formato personalizado para um aplicativo que você está desenvolvendo.

Os blobs não são eficientes para dados estruturados que precisam ser consultados com frequência. Eles têm maior latência do que a memória e os discos locais e não têm os recursos de indexação que tornam os bancos de dados eficientes na execução de consultas. No entanto, os blobs são frequentemente usados em combinação com bancos de dados para armazenar dados não consultáveis. Por exemplo, um aplicativo com um banco de dados de perfis de usuário pode armazenar imagens de perfil em blobs. Cada registro de usuário no banco de dados inclui o nome ou URL do blob que contém a imagem do usuário.

Os blobs são usados para armazenamento de dados de várias maneiras em todos os tipos de aplicativos e arquiteturas:

Aplicativos que precisam transmitir grandes quantidades de dados usando um sistema de mensagens que suporta apenas mensagens pequenas. Esses aplicativos podem armazenar dados em blobs e enviar os URLs de blob em mensagens.
O Blob Storage pode ser usado como um sistema de arquivos para armazenar e compartilhar documentos e outros dados pessoais.
Ativos da Web estáticos, como imagens, podem ser armazenados em blobs e disponibilizados para download público como se fossem arquivos em um servidor da Web.
Muitos componentes do Azure usam blobs nos bastidores. Por exemplo, o Azure Cloud Shell armazena seus arquivos e configuração em blobs e as Máquinas Virtuais do Azure usam blobs para armazenamento em disco rígido.
Alguns aplicativos criam, atualizam e excluem constantemente blobs como parte de seu trabalho. Outros usarão um pequeno conjunto de blobs e raramente os alterarão.

Contas de armazenamento, contêineres e metadados
No Armazenamento de Blobs, cada blob vive dentro de um contêiner de blob . Você pode armazenar um número ilimitado de blobs em um contêiner e um número ilimitado de contêineres em uma conta de armazenamento. Os contêineres são "planos"; eles só podem armazenar blobs, não outros contêineres.

Blobs e contêineres dão suporte a metadados na forma de pares de strings de nome-valor. Seus aplicativos podem usar metadados para o que você quiser: uma descrição legível por humanos do conteúdo de um blob a ser exibido pelo aplicativo, uma string que seu aplicativo usa para determinar como processar os dados do blob e assim por diante.

 Dica

O Armazenamento de Blobs não fornece nenhum mecanismo para pesquisar ou classificar blobs por metadados. Para obter informações sobre como usar a Pesquisa Cognitiva do Azure, consulte a seção Leitura adicional no final deste módulo.

API de armazenamento de blobs e bibliotecas de cliente
A API de armazenamento de blob é baseada em REST e suportada por bibliotecas de cliente em muitas linguagens populares. Ele permite escrever aplicativos que criam e excluem blobs e contêineres, carregam e baixam dados de blob e listam os blobs em um contêiner.

Próxima unidade: Projetar uma estratégia de organização de armazenamento
Continuar

Projete uma estratégia de organização de armazenamento
200 EXP
7 minutos
Ao projetar um aplicativo que precisa armazenar dados, é importante pensar em como o aplicativo organizará os dados entre contas de armazenamento, contêineres e blobs.

Contas de armazenamento
Uma única conta de armazenamento é flexível o suficiente para organizar seus blobs. No entanto, você deve usar contas de armazenamento adicionais conforme necessário para separar logicamente os custos e controlar o acesso aos dados.

Recipientes e bolhas
A natureza do seu aplicativo e os dados que ele armazena devem orientar sua estratégia para nomear e organizar contêineres e blobs.

Os aplicativos que usam blobs como parte de um esquema de armazenamento que inclui um banco de dados geralmente não precisam depender muito de organização, nomenclatura ou metadados para indicar qualquer coisa sobre seus dados. Esses aplicativos geralmente usam identificadores como GUIDs como nomes de blob e fazem referência a esses identificadores em registros de banco de dados. O aplicativo usa o banco de dados para determinar onde os blobs são armazenados e o tipo de dados que eles contêm.

Outros aplicativos podem usar o Armazenamento de Blobs do Azure mais como um sistema de arquivos pessoal, em que os nomes de contêiner e blob são usados para indicar significado e estrutura. Os nomes de blob nesses tipos de aplicativos geralmente se parecem com nomes de arquivos tradicionais e incluem extensões de nome de arquivo como .jpgpara indicar que tipo de dados eles contêm. Eles usarão diretórios virtuais (veja abaixo) para organizar blobs e usarão frequentemente tags de metadados para armazenar informações sobre blobs e contêineres.

Há algumas coisas importantes a serem consideradas ao decidir como organizar e armazenar blobs e contêineres.

Limitações de nomenclatura
Os nomes de contêiner e blob devem estar em conformidade com um conjunto de regras, incluindo limitações de comprimento e restrições de caracteres. Para obter informações mais específicas sobre regras de nomenclatura, consulte a seção Leituras Adicionais no final deste módulo.

Acesso público e contêineres como limites de segurança
Por padrão, todos os blobs exigem autenticação para acessar. No entanto, você pode configurar contêineres individuais para permitir o download público de seus blobs sem autenticação. O download público oferece suporte a muitos casos de uso, como hospedagem de ativos de sites estáticos e compartilhamento de arquivos. Isso funciona porque o download do conteúdo do blob funciona da mesma maneira que a leitura de qualquer outro tipo de dados na web: basta apontar um navegador ou qualquer coisa que possa fazer uma solicitação GET na URL do blob.

Habilitar o acesso público é importante para a escalabilidade porque os dados baixados diretamente do Armazenamento de Blobs não geram nenhum tráfego em seu aplicativo do lado do servidor. Mesmo que você não aproveite imediatamente o acesso público ou use um banco de dados para controlar o acesso a dados por meio de seu aplicativo, planeje usar contêineres separados para os dados que deseja disponibilizar publicamente.

 Cuidado

Qualquer pessoa que conheça seus URLs de armazenamento pode baixar blobs em um contêiner configurado para acesso público sem nenhum tipo de autenticação ou auditoria. Nunca coloque dados de blob em um contêiner público que você não pretenda compartilhar publicamente.

Além do acesso público, o Azure tem um recurso de assinatura de acesso compartilhado que permite o controle de permissões refinado em contêineres. O controle de acesso de precisão permite cenários que melhoram ainda mais a escalabilidade, portanto, é útil pensar nos contêineres como limites de segurança em geral.

Prefixos de nome de blob (diretórios virtuais)
Tecnicamente, os contêineres são "planos" e não suportam nenhum tipo de aninhamento ou hierarquia. Mas se você der aos seus blobs nomes hierárquicos que se pareçam com caminhos de arquivo (como finance/budgets/2017/q1.xls), a operação de listagem da API pode filtrar os resultados para prefixos específicos. Isso permite que você navegue na lista como se fosse um sistema hierárquico de arquivos e pastas.

Esse recurso costuma ser chamado de diretórios virtuais , porque algumas ferramentas e bibliotecas de cliente o usam para visualizar e navegar no Blob Storage como se fosse um sistema de arquivos. Cada navegação de pasta aciona uma chamada separada para listar os blobs nessa pasta.

Usar nomes que são como nomes de arquivo para blobs é uma técnica comum para organizar e navegar em dados de blob complexos.

Tipos de blob
Existem três tipos diferentes de blobs nos quais você pode armazenar dados:

Os blobs de blocos são compostos por blocos de tamanhos diferentes que podem ser carregados independentemente e em paralelo. A gravação em um blob de bloco envolve o upload de dados para blocos e o commit deles no blob.
Os blobs de acréscimo são blobs de blocos especializados que dão suporte apenas à anexação de novos dados (não atualizando ou excluindo dados existentes), mas são muito eficientes nisso. Os blobs de acréscimo são ótimos para cenários como armazenamento de logs ou gravação de dados transmitidos.
Os blobs de páginas são projetados para cenários que envolvem leituras e gravações de acesso aleatório. Os blobs de páginas são usados para armazenar os arquivos de disco rígido virtual (VHD) usados pelas Máquinas Virtuais do Azure, mas são ótimos para qualquer cenário que envolva acesso aleatório.
Os blobs de blocos são a melhor escolha para a maioria dos cenários que não exigem especificamente blobs de acréscimo ou de página. Sua estrutura baseada em blocos suporta uploads e downloads muito rápidos e acesso eficiente a partes individuais de um blob. O processo de gerenciamento e confirmação de blocos é tratado automaticamente pela maioria das bibliotecas cliente, e algumas também lidam com uploads e downloads paralelos para maximizar o desempenho.

Ingestão em escala de petabytes com Azure Data Factory ou Azure Synapse Pipeline
43 minutos
Módulo
9 unidades

 4,6 ( 870 )

Avalie
Principiante
Engenheiro de dados
Cientista de dados
Fábrica de dados
Análise de sinapse
Neste módulo, você aprenderá os vários métodos que podem ser usados para ingerir dados entre vários armazenamentos de dados usando o Azure Data Factory.

Objetivos de aprendizado
Introdução
Listar os métodos de ingestão de data factory
Descrever conectores de fábrica de dados
Exercício: Use a atividade de cópia do data factory
Exercício: Gerenciar o tempo de execução de integração auto-hospedado
Exercício: configurar o tempo de execução de integração do Azure
Entenda as considerações de segurança de ingestão de dados
Verificação de conhecimento
Resumo

Introdução
Concluído
100 EXP
2 minutos
A primeira etapa do trabalho de integração de dados é ingerir dados de origem de qualquer número de sistemas em um destino ou um armazenamento de dados intermediário. O Azure Data Factory fornece diferentes métodos de ingestão que podem se conectar a quase uma centena de conectores de dados diferentes. O método escolhido dependerá em grande parte do seu conjunto de habilidades atual ou preferência por trabalhar com uma tecnologia para ingerir dados.

Independentemente do método escolhido, você deve configurar a infraestrutura apropriada para dar suporte ao método de ingestão de dados por meio de tempos de execução de integração. Além disso, você também deve garantir que a ingestão de dados seja segura em cada armazenamento de dados e enquanto estiver em trânsito

Objetivos de aprendizado
Neste módulo, você irá:

Descrever os métodos de ingestão de dados
Entenda os conectores disponíveis
Use a atividade de cópia
Gerenciar um tempo de execução de integração auto-hospedado
Configurar o tempo de execução de integração do Azure
Descrever as considerações de segurança de ingestão de dados
Próxima unidade: listar os métodos de ingestão de data factory

Listar os métodos de ingestão de data factory
Concluído
100 EXP
4 minutos
O Azure Data Factory pode acomodar organizações que estão embarcando em projetos de integração de dados a partir de diferentes pontos de partida. É raro que um projeto de migração de dados seja um projeto de campo verde. Normalmente, muitos fluxos de trabalho de integração de dados devem considerar pipelines existentes que foram criados em projetos anteriores, com diferentes dependências e usando diferentes tecnologias. Para esse fim, existem vários métodos de ingestão que podem ser usados para extrair dados de uma variedade de fontes.

Ingerindo dados usando a atividade de cópia
Use esse método para criar pipelines de ingestão de dados sem código que não exigem nenhuma transformação durante a extração dos dados. A atividade de cópia tem suporte para mais de 100 conectores nativos. Esse método pode atender a projetos de campo verde que possuem um método simples de extração para um armazenamento de dados intermediário. Um exemplo de ingestão de dados usando a atividade de cópia pode incluir a extração de dados de vários sistemas de banco de dados de origem e a saída dos dados para arquivos em um armazenamento de data lake. O benefício desse método de ingestão é que eles são simples de criar, mas não são capazes de lidar com transformações sofisticadas ou lógica de negócios.

Como ingerir dados usando recursos de computação
O Azure Data Factory pode chamar recursos de computação para processar dados por um serviço de plataforma de dados que pode ser mais adequado para o trabalho. Um ótimo exemplo disso é que o Azure Data Factory pode criar um pipeline para uma plataforma de dados analíticos, como pools do Spark em uma instância do Azure Synapse Analytics para realizar um cálculo complexo, que gera novos dados. Esses dados são então ingeridos de volta no pipeline para processamento posterior. Há uma ampla gama de recursos de computação e as atividades associadas que eles podem executar, conforme mostrado na tabela a seguir:

Ambiente de computação	Atividades
Cluster HDInsight sob demanda ou seu próprio cluster HDInsight	Hive, Pig, Spark, MapReduce, Hadoop Streaming
Lote do Azure	Atividades personalizadas
Máquina do Azure Machine Learning Studio	Atividades de aprendizagem: Execução em lote e recurso de atualização
Aprendizado de máquina do Azure	Pipeline de Execução do Azure Machine Learning
Azure Data Lake Analytics	Data Lake Analytics U-SQL
Azure SQL, Azure SQL Data Warehouse, SQL Server	Procedimento armazenado
Azure Databricks	Notebook, Jar, Python
Função do Azure	Atividade do Azure Function
Ingerir dados usando pacotes SSIS
Muitas organizações têm décadas de investimento em desenvolvimento em pacotes SQL Server Integration Services (SSIS) que contêm lógica de ingestão e transformação de armazenamentos de dados locais e na nuvem. O Azure Data Factory fornece a capacidade de elevar e deslocar a carga de trabalho SSIS existente, criando um Azure-SSIS Integration Runtime para executar pacotes SSIS nativamente e permitirá que você implante e gerencie seus pacotes SSIS existentes com pouca ou nenhuma alteração usando ferramentas familiares, como SQL Server Data Tools (SSDT) e SQL Server Management Studio (SSMS), assim como usar o SSIS no local.

Próxima unidade: Descrever os conectores da fábrica de dados

Descrever conectores de fábrica de dados
Concluído
100 EXP
3 minutos
Conectores são objetos do Azure Data Factory que permitem que seus serviços vinculados e conjuntos de dados se conectem a uma ampla variedade de fontes de dados e coletores. Isso pode incluir conexões com recursos do Azure e conectores de terceiros, como Amazon S3 ou nuvem do Google. Há quase 100 conectores disponíveis e eles funcionam com as atividades Copiar, Fluxo de Dados, Pesquisar, Obter Metadados e Excluir que podem ser encontradas no Azure Data Factory.

Os formatos de arquivo suportados incluem:

Formato Avro
Formato binário
Formato de texto delimitado
Formato JSON
Formato ORC
Formato parquet
Há muitos armazenamentos de dados para listar, mas a tabela a seguir lista as categorias de armazenamentos de dados e dois exemplos dos tipos de conectores existentes

Categoria	Exemplo de armazenamento de dados
Azure	Azure Data Lake Store, Azure Synapse Analytics
Bancos de dados	Netezza, Greenplum
Armazenamentos NoSQL	Cassandra, MongoDB
Arquivo	FTP, armazenamento em nuvem do Google
Protocolos genéricos	REST, ODBC
Serviços e aplicativos	Dinâmica, Força de Vendas
A lista de conectores está em constante evolução. Você pode manter-se atualizado com a lista mais recente e o suporte à atividade consultando a página de visão geral dos conectores

Próxima unidade: Exercício - Use a atividade de cópia do data factory

Formatos de arquivo suportados
Os seguintes formatos de arquivo são suportados. Consulte cada artigo para configurações baseadas em formato.

Formato Avro
Formato binário
Formato de modelo de dados comum
Formato de texto delimitado
Formato delta
Formato Excel
Formato JSON
Formato ORC
Formato parquet
Formato XML

Exercício - Gerenciar o tempo de execução de integração auto-hospedado
Concluído
100 EXP
5 minutos
No Data Factory, uma atividade define a ação a ser executada. Um serviço vinculado define um armazenamento de dados de destino ou um serviço de computação. Um tempo de execução de integração fornece a ponte entre a atividade e os serviços vinculados.

Tempo de execução de integração auto-hospedado
Um tempo de execução de integração auto-hospedado é capaz de:

Executando a atividade de cópia entre um armazenamento de dados na nuvem e um armazenamento de dados na rede privada.
Despachando as seguintes atividades de transformação em recursos de computação no local ou na Rede Virtual do Azure:
Atividade HDInsight Hive (BYOC-Traga seu próprio cluster)
Atividade HDInsight Pig (BYOC)
Atividade HDInsight MapReduce (BYOC)
Atividade do HDInsight Spark (BYOC)
Atividade de streaming HDInsight (BYOC)
Atividade de execução em lote de aprendizado de máquina
Atividades de recursos de atualização de aprendizado de máquina
Atividade de procedimento armazenado
Atividade U-SQL do Data Lake AnalyticsNo Data Factory, uma atividade define a ação a ser executada. Um serviço vinculado define um armazenamento de dados de destino ou um serviço de computação. Um tempo de execução de integração fornece a ponte entre a atividade e os serviços vinculados.

Tempo de execução de integração do Azure
Um tempo de execução de integração do Azure é capaz de:

Executando fluxos de dados no Azure
Executando a atividade de cópia entre armazenamentos de dados na nuvem
Despachando as seguintes atividades de transformação na rede pública : atividade Databricks Notebook/Jar/Python, atividade HDInsight Hive, atividade HDInsight Pig, atividade HDInsight MapReduce, atividade HDInsight Spark, atividade HDInsight Streaming, atividade Machine Learning Batch Execution, Machine Learning Update Resource Activities, Stored Atividade de procedimento, atividade U-SQL do Data Lake Analytics, atividade personalizada do .NET, atividade da Web, atividade de pesquisa e atividade de obtenção de metadados.
Você pode definir um determinado local de um Azure IR, caso em que a movimentação de dados ou o envio de atividade acontecerá nessa região específica. Se você optar por usar o IR do Azure de resolução automática, que é o padrão, o ADF fará o possível para detectar automaticamente seu coletor e armazenamento de dados de origem para escolher o melhor local na mesma região, se disponível, ou o mais próximo na mesma geografia para a atividade de cópia. Para qualquer outra coisa, ele usará o IR na região do Data Factory. O Azure Integration Runtime também tem suporte para redes virtuais.
Atividade personalizada (executada no Lote do Azure)
Atividade de pesquisa
Obter atividade de metadados.
O tempo de execução de integração auto-hospedado é registrado logicamente no Azure Data Factory e o recurso de computação usado para dar suporte à sua função conforme fornecido por você. Portanto, não há propriedade de localização explícita para IR auto-hospedado. Quando usado para realizar a movimentação de dados, o IR auto-hospedado extrai dados da origem e os grava no destino.

Criar um Integration Runtime auto-hospedado no Azure Data Factory
Na página Vamos começar da interface do usuário do Azure Data Factory, selecione Gerenciar no painel mais à esquerda e selecione Tempos de execução de integração. Selecione +Novo.

Próxima unidade: Exercício - Configurar o tempo de execução de integração do Azure


Entenda as considerações de segurança de ingestão de dados
Concluído
100 EXP
8 minutos
A integração de dados requer o manuseio seguro de dados em repouso e em trânsito. Antes da criação de uma solução de integração de dados, a fase de projeto deve considerar os requisitos de segurança. Isso fornecerá inicialmente cobertura dos sistemas de origem e como os dados deles são ingeridos. No entanto, há uma necessidade de uma abordagem holística, que também encapsule a segurança de armazenamentos de dados intermediários nas seguintes áreas

Rede
Há uma série de questões que você deve estar ciente sobre como configurar a segurança da rede. Você pode ter que trabalhar com o Administrador do Azure em sua organização para gerenciar algumas dessas áreas.

Use redes virtuais para proteger recursos do Azure
As redes virtuais permitem a comunicação segura entre os serviços do Azure ou com servidores que existem na rede local. Redes virtuais ou VNets são o bloco de construção fundamental para configurar redes privadas. Ele permite a comunicação segura entre recursos do Azure, serviços na Internet e com servidor em redes locais. O Azure Data Factory pode ingerir dados de um servidor local ou de uma máquina virtual hospedada no Azure. Para conseguir isso, um tempo de execução de integração auto-hospedado pode ser implantado em um servidor dentro de uma rede virtual. Para restringir o acesso, você deve configurar um Network Security Group (NSG) para permitir apenas acesso administrativo. Ao usar o tempo de execução de integração Azure-SSIS, você tem a opção de ingressar em uma rede virtual. Aceitar essa opção permite que o Azure Data Factory crie recursos de rede, um exemplo disso é que um Grupo de Segurança de Rede é criado automaticamente pelo Azure Data Factory e a porta 3389 está aberta a todo o tráfego por padrão. Bloqueie isso para garantir que apenas seus administradores tenham acesso.

Use serviços para detectar e evitar intrusões
Você pode negar a comunicação com endereços IP conhecidos habilitando o padrão de proteção de negação de serviço distribuído (DDoS) nas redes virtuais nas quais o tempo de execução de integração está hospedado. Além disso, você pode usar o Integrated Threat Intelligence da Central de Segurança do Azure para negar comunicações com endereços IP de Internet mal-intencionados ou não utilizados conhecidos. O Firewall do Azure com Threat Intelligence pode ser usado para controlar o acesso à rede. Se a detecção e/ou prevenção de intrusão com base na inspeção de carga útil for necessária, você poderá redirecionar o tráfego para um dispositivo de firewall por meio do túnel de força do Azure ExpressRoute ou para um Dispositivo Virtual de Rede que dê suporte a esse recurso

Simplifique o gerenciamento de regras de segurança usando tags de serviço de rede
As redes virtuais podem ser configuradas com etiquetas de serviço. As marcas de serviço permitem agrupar prefixos de endereço IP de um determinado serviço do Azure para fins administrativos. Usando as marcas de serviço, você pode criar regras de segurança de rede em grupos de segurança de rede com base nas marcas de serviço para reduzir as sobrecargas administrativas. Ao especificar o nome da etiqueta de serviço (por exemplo, DataFactoryManagement) no campo de origem ou destino apropriado de uma regra, você pode permitir ou negar tráfego de entrada para o serviço correspondente.

Identidade e controle de acesso
Gerenciar o acesso aos seus dados é uma área importante a ser considerada. Aqui estão algumas áreas para estar ciente.

Contas administrativas
As contas administrativas usadas para trabalhar e gerenciar o Azure Data Factory devem ser dedicadas, contas conhecidas que são monitoradas e gerenciadas regularmente para garantir que não sejam comprometidas. Para criar instâncias do Data Factory, a conta de usuário que você usa para entrar no Azure deve ser um membro da função de colaborador ou proprietário ou um administrador da assinatura do Azure. Para ambientes de alta segurança, considere o uso de máquinas dedicadas para acesso administrativo para quaisquer tarefas administrativas do ADF

Use o Active Directory para usar o logon único
Registre os principais de serviço no Azure Active Directory para aproveitar o gerenciamento de token para que seu serviço Azure Data Factory simplifique sua autenticação nos recursos do Azure. Um data factory pode ser associado a uma identidade gerenciada para recursos do Azure que representa o data factory específico. Você pode usar essa identidade gerenciada para autenticação do Banco de Dados SQL do Azure. A fábrica designada pode acessar e copiar dados de ou para seu banco de dados usando essa identidade.

Proteção de dados
Você pode ter que fazer considerações especiais para tipos específicos de dados, como dados médicos ou dados financeiros nas seguintes áreas.

Use o controle de acesso baseado em função (RBAC) para controlar o acesso aos recursos
Use o RBAC nas fontes de dados para controlar o acesso aos dados à entidade de serviço do Azure Data Factory.

Dados sensíveis
Há uma série de considerações que você deve levar em conta ao trabalhar com dados confidenciais, incluindo:

Manter uma lista dos armazenamentos de dados que contêm informações confidenciais
Isole os sistemas que armazenam ou processam informações confidenciais
Monitore e bloqueie a transferência não autorizada de informações confidenciais
Criptografe todas as informações confidenciais que estiverem em trânsito
Criptografe todas as informações confidenciais em repouso
Registro e monitoramento
Também é importante entender quem está acessando seus dados, e suas considerações de segurança devem envolver as seguintes áreas.

Configurar o gerenciamento de log de segurança central
Use o Azure Monitor para centralizar o armazenamento de logs de ingestão gerados pelo Azure Data Factory e consulte-os usando o Log Analytics. Além disso, configure uma estratégia para armazenar os logs a longo prazo nas contas de armazenamento do Azure para que você tenha os dados para estabelecer linhas de base para atividades de ingestão do ADF

Monitore e registre a configuração e o tráfego de pacotes de rede de redes virtuais, sub-redes e NICs
Habilite logs de fluxo de grupo de segurança de rede (NSG) para o NSG protegendo sua implantação do Integration Runtime e envie logs para uma conta de armazenamento do Azure para auditoria de tráfego. Você também pode enviar logs de fluxo NSG para um espaço de trabalho do Log Analytics e usar o Traffic Analytics para fornecer informações sobre o fluxo de tráfego em sua nuvem do Azure.

Ativar registro de auditoria
Você pode usar as configurações de diagnóstico do Azure Data Factory para configurar logs de diagnóstico para rastrear dados de execução de pipeline, que são retidos por 45 dias. Você pode salvar os logs de diagnóstico em contas de armazenamento do Azure para análise futura.

Ativar alertas sobre atividades
A configuração de diagnóstico para Azure Data Factory que envia logs para Log Analytics pode ter alertas configurados para um conjunto de condições pré-definidas que podem alertar um administrador sobre atividades

Siga o padrão de registro e monitoramento padrão em sua organização
Verifique os padrões de sua organização para registro e monitoramento e ajuste ao padrão, incluindo:

Registro de auditoria
Registros de segurança
Registro antimalware
Políticas de retenção de registros

Resumo
Concluído
100 EXP
3 minutos
A ingestão de dados pode operar em escala de petabytes usando um método de ingestão adequado ao seu conjunto de habilidades. O uso do método Copy Activity fornece o método mais simples para configurar seu mecanismo de ingestão. Independentemente do método usado, você deve estar ciente de que, para transferências de dados de servidores locais, é necessário um tempo de execução de integração auto-hospedado para permitir isso. Para execução do pacote SSIS, o tempo de execução de integração Azure-SSIS é mais apropriado. Também é importante garantir que os dados sejam ingeridos com segurança.

 Importante

Se você estiver trabalhando em sua própria assinatura enquanto executa esse conteúdo, é uma prática recomendada no final de um projeto identificar se ainda precisa ou não dos recursos que criou. Os recursos deixados em execução podem custar-lhe dinheiro. Você pode excluir recursos um por um ou apenas excluir o grupo de recursos para se livrar de todo o conjunto.



